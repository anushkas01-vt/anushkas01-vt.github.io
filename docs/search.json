[
  {
    "objectID": "posts/Regression/Regression.html",
    "href": "posts/Regression/Regression.html",
    "title": "Regression Analysis: Linear Regression and Non-Linear Regression",
    "section": "",
    "text": "Supervised learning, also known as supervised machine learning, is a subcategory of machine learning and artificial intelligence. It is defined by its use of labeled datasets to train algorithms that to classify data or predict outcomes accurately.\nMachine Learning Regression is a technique for investigating the relationship between independent variables or features and a dependent variable or outcome. It’s used as a method for predictive modelling in machine learning, in which an algorithm is used to predict continuous outcomes. You can read more about it here!\nIn this blog, we will discuss two types of regression problems, Linear Regression, and Non-Linear Regression. For each, we will compare a handful of machine learning models (linear and non-linear models) and present their results on evaluation metrics.\nLinear Regression This form of analysis estimates the coefficients of the linear equation, involving one or more independent variables that best predict the value of the dependent variable. Linear regression fits a straight line or surface that minimizes the discrepancies between predicted and actual output values.\nWe’ll make use of the Seoul Bike Sharing dataset which contains count of public bicycles rented per hour in the Seoul Bike Sharing System, with corresponding weather data and holiday information. The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information. A sample of the dataset can be seen below. The aim is to predict the bike count required at each hour for the stable supply of rental bikes.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndf_bike = pd.read_csv(\"SeoulBikeData.csv\")\ndf_bike.head(5)\n\n\n\n\n\n\n\n\n\nDate\nRented Bike Count\nHour\nTemperature(C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nSeasons\nHoliday\nFunctioning Day\n\n\n\n\n0\n01/12/2017\n254\n0\n-5.2\n37\n2.2\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n1\n01/12/2017\n204\n1\n-5.5\n38\n0.8\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n2\n01/12/2017\n173\n2\n-6.0\n39\n1.0\n2000\n-17.7\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n3\n01/12/2017\n107\n3\n-6.2\n40\n0.9\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n4\n01/12/2017\n78\n4\n-6.0\n36\n2.3\n2000\n-18.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n\n\n\n\n\nIt is important to check the dataset for any missing values before it is used for model training and testing.\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 14 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Date                      8760 non-null   object \n 1   Rented Bike Count         8760 non-null   int64  \n 2   Hour                      8760 non-null   int64  \n 3   Temperature(C)            8760 non-null   float64\n 4   Humidity(%)               8760 non-null   int64  \n 5   Wind speed (m/s)          8760 non-null   float64\n 6   Visibility (10m)          8760 non-null   int64  \n 7   Dew point temperature(C)  8760 non-null   float64\n 8   Solar Radiation (MJ/m2)   8760 non-null   float64\n 9   Rainfall(mm)              8760 non-null   float64\n 10  Snowfall (cm)             8760 non-null   float64\n 11  Seasons                   8760 non-null   object \n 12  Holiday                   8760 non-null   object \n 13  Functioning Day           8760 non-null   object \ndtypes: float64(6), int64(4), object(4)\nmemory usage: 958.2+ KB\nNone\nDate                        0\nRented Bike Count           0\nHour                        0\nTemperature(C)              0\nHumidity(%)                 0\nWind speed (m/s)            0\nVisibility (10m)            0\nDew point temperature(C)    0\nSolar Radiation (MJ/m2)     0\nRainfall(mm)                0\nSnowfall (cm)               0\nSeasons                     0\nHoliday                     0\nFunctioning Day             0\ndtype: int64\nDate                        0\nRented Bike Count           0\nHour                        0\nTemperature(C)              0\nHumidity(%)                 0\nWind speed (m/s)            0\nVisibility (10m)            0\nDew point temperature(C)    0\nSolar Radiation (MJ/m2)     0\nRainfall(mm)                0\nSnowfall (cm)               0\nSeasons                     0\nHoliday                     0\nFunctioning Day             0\ndtype: int64\n\n\nThis dataset seems to have no missing values so we’re good!\nLet’s format the dataset to ease data processing down the line. Beginning with breaking down the ‘Date’ into ‘Day’, ‘Month’, and ‘Year’ columns in the dataset.\n\n# Can break the date into date, month, year columns and convert them into integers (from strings) for the purpose of correlation map\ndays = [int((df_bike['Date'].iloc[i])[0:2]) for i in range(len(df_bike))]\nmonth = [int((df_bike['Date'].iloc[i])[3:5]) for i in range(len(df_bike))]\nyear = [int((df_bike['Date'].iloc[i])[6:]) for i in range(len(df_bike))]\ndf_bike['Day'], df_bike['Month'], df_bike['Year'] = days, month, year\n\ndf_bike.head(5)\n\n\n\n\n\n\n\n\nDate\nRented Bike Count\nHour\nTemperature(C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nSeasons\nHoliday\nFunctioning Day\nDay\nMonth\nYear\n\n\n\n\n0\n01/12/2017\n254\n0\n-5.2\n37\n2.2\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n1\n12\n2017\n\n\n1\n01/12/2017\n204\n1\n-5.5\n38\n0.8\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n1\n12\n2017\n\n\n2\n01/12/2017\n173\n2\n-6.0\n39\n1.0\n2000\n-17.7\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n1\n12\n2017\n\n\n3\n01/12/2017\n107\n3\n-6.2\n40\n0.9\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n1\n12\n2017\n\n\n4\n01/12/2017\n78\n4\n-6.0\n36\n2.3\n2000\n-18.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n1\n12\n2017\n\n\n\n\n\n\n\nNext, we convert string values such as the values in the ‘Seasons’, ‘Functioning Day’, and ‘Holiday’ columns. We are able to do this by mapping the discrete set of string values to a discrete set of integer values.\n\ndf1_bike = df_bike.drop(columns = ['Date'])\n# map unique season to numbers, map holiday to binary, and functioning day to binary\nseasons = {}\nfor idx, i in enumerate(df_bike['Seasons'].drop_duplicates()):\n    seasons[i] = idx\nholiday = {\"No Holiday\": 0, \"Holiday\": 1}\nfunctioning = {\"Yes\": 0, \"No\": 1}\ndf1_bike.Holiday = [holiday[item] for item in df_bike.Holiday]\ndf1_bike.Seasons = [seasons[item] for item in df_bike.Seasons]\ndf1_bike['Functioning Day'] = [functioning[item] for item in df1_bike['Functioning Day'] ]\n\ndf1_bike.head(3)\n\n\n\n\n\n\n\n\nRented Bike Count\nHour\nTemperature(C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nSeasons\nHoliday\nFunctioning Day\nDay\nMonth\nYear\n\n\n\n\n0\n254\n0\n-5.2\n37\n2.2\n2000\n-17.6\n0.0\n0.0\n0.0\n0\n0\n0\n1\n12\n2017\n\n\n1\n204\n1\n-5.5\n38\n0.8\n2000\n-17.6\n0.0\n0.0\n0.0\n0\n0\n0\n1\n12\n2017\n\n\n2\n173\n2\n-6.0\n39\n1.0\n2000\n-17.7\n0.0\n0.0\n0.0\n0\n0\n0\n1\n12\n2017\n\n\n\n\n\n\n\nPlotting the correlation matrix to identify the relationship, and the strength of relationship between the features(variables) in the dataset and also understand how strongly they are correlated with the target variable which is the rented bike count.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = df1_bike.corr()\nsns.heatmap(corr,\n    cmap=sns.diverging_palette(220, 10, as_cmap=True),\n    vmin=-1.0, vmax=1.0,\n    square=True, ax=ax)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\nBelow, we can plot visualizations to see how each feature (variable) effects the target: Rented Bike Count.\n\n\nCode\nplt.rcParams[\"figure.autolayout\"] = True\nfig, ax = plt.subplots(2, 2, figsize=(10, 8));\n# hour vs bike count\nsns.barplot(data=df1_bike,x='Hour',y='Rented Bike Count',ax=ax[0][0], palette='viridis');\nax[0][0].set(title='Count of Rented bikes acording to Hour');\n\n# Functioning vs bike count\nsns.barplot(data=df1_bike,x='Functioning Day',y='Rented Bike Count',ax=ax[0][1], palette='inferno');\nax[0][1].set(title='Count of Rented bikes acording to Functioning Day');\nax[0][1].set_xticklabels(['Yes', 'No'])\n\n# season vs bike count\nsns.barplot(data=df1_bike,x='Seasons', y='Rented Bike Count',ax=ax[1][0], palette='plasma');\nax[1][0].set(title='Count of Rented bikes acording to Seasons');\nax[1][0].set_xticklabels(['Winter', 'Spring', 'Summer', 'Autumn'])\n\n# month vs bike count\nsns.barplot(data=df1_bike,x='Month',y='Rented Bike Count',ax=ax[1][1], palette='cividis');\nax[1][1].set(title='Count of Rented bikes acording to Month ');\n\nplt.show()\n\n\n\n\n\n\n\nCode\nfig,ax=plt.subplots(figsize=(10,8))\nsns.pointplot(data=df1_bike,x='Hour',y='Rented Bike Count',hue='Seasons',ax=ax);\nax.set(title='Count of Rented bikes acording to seasons and hour of the day');\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(10, 8));\n# temperature vs bike count\n# Convert temperature in groups of 5C and average the rented bike counts for that range (rounding to 5s)\ntemp_min, temp_max = round(min(df1_bike['Temperature(C)'])/5)*5, round(max(df1_bike['Temperature(C)'])/5)*5\ndict_temp = {}\nfor i in range(temp_min, temp_max, 5):\n    # Filter rows based on the temperature interval\n    filtered_df = df1_bike[(df1_bike['Temperature(C)'] &gt;= i) & (df1_bike['Temperature(C)'] &lt; i+5)]\n    dict_temp[i] = filtered_df['Rented Bike Count'].mean()\n# print(dict_temp)\n# print(temp_max, temp_min)\nsns.barplot(data=dict_temp,ax=ax, palette='plasma');\nax.set(title='Count of Rented bikes acording to Temperature');\n\n# plt.show()\n\n\n\n\n\nPrinting the regression plot (Dependent Features vs Target Varibale).\n\nfig,ax=plt.subplots(4, 4, figsize=(10,10)) # since we know there are 16 features\nfor idx, col in enumerate(df1_bike.columns):\n  sns.regplot(x=df1_bike[col],y=df1_bike['Rented Bike Count'],scatter_kws={\"color\": 'blue'}, line_kws={\"color\": \"black\"}, ax=ax[idx//4][idx%4])\n\n\n\n\nTo train and evaluate the machine learning models we need to split the dataset appropriately into the training and testing datasets. Hence, we perform an 80-20 train-test split here.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\ny = df1_bike['Rented Bike Count']\nX = (df1_bike.drop(columns = ['Rented Bike Count'])).to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nThe following are the models used for estimating the number of bikes rented given other data. The code implementations for each can be found in the scikit-learn library (linked for each model), and the model paramters used are default parameters. 1. Linear regression model Linear regression is a simple yet powerful algorithm that models the relationship between the input features and the target variable by fitting a linear equation to the observed data. The main algorithm involves finding the coefficients that minimize the sum of squared differences between the predicted and actual values. This is typically achieved using the Ordinary Least Squares (OLS) method, aiming to optimize the line’s parameters to best represent the data points.\n\nSupport Vector Machine (Regressor) In regression tasks, Support Vector Machines (SVM) aim to find the hyperplane that best represents the relationship between the input features and the target variable. The primary algorithm involves identifying the support vectors and determining the optimal hyperplane to maximize the margin while minimizing the error. SVM uses a loss function that penalizes deviations from the regression line, and the algorithm seeks to find the coefficients that minimize this loss.\nRidge Regression Ridge Regression is an extension of linear regression that introduces a regularization term to prevent overfitting. The main algorithm involves adding a penalty term to the linear regression objective function, which is proportional to the square of the L2 norm of the coefficients. This regularization term helps stabilize the model by shrinking the coefficients, particularly useful when dealing with multicollinearity.\nLasso Regression Lasso Regression, similar to Ridge Regression, introduces regularization to linear regression. The main algorithm incorporates a penalty term, but in this case, it is proportional to the absolute value of the L1 norm of the coefficients. Lasso regression is effective for feature selection as it tends to produce sparse coefficient vectors, driving some coefficients to exactly zero.\nGradient Boosting Regressor Gradient Boosting Regressor is an ensemble learning method that builds a series of decision trees sequentially. The main algorithm involves fitting a weak learner (usually a shallow decision tree) to the residuals of the previous trees. The predictions of individual trees are combined to improve overall accuracy. The algorithm minimizes a loss function by adjusting the weights of the weak learners.\nRandom Forest Regressor Random Forest Regressor is an ensemble learning method that constructs multiple decision trees during training. The main algorithm involves training each tree on a random subset of the training data and features. The predictions of individual trees are then averaged or aggregated to reduce overfitting and improve generalization. Random Forest leverages the diversity among trees for robust predictions.\nDecision Tree Regressor Decision Tree Regressor models the relationship between input features and the target variable by recursively splitting the data based on feature thresholds. The main algorithm involves selecting the best split at each node to minimize the variance of the target variable. Decision trees are constructed until a stopping criterion is met, creating a tree structure that facilitates predictive modeling.\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn import linear_model\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n\nmodel_names = ['Linear Regression Model',\n 'Support Vector Machine (Regression)',\n 'Ridge Regression',\n  'Lasso Regression',\n 'Gradient Boosting Regression',\n  'Random Forest',\n  'Decision Tree']\nmodels = [LinearRegression(),\n    SVR(), \n    linear_model.Ridge(), \n    linear_model.Lasso(),\n    GradientBoostingRegressor(),\n    RandomForestRegressor(),\n    DecisionTreeRegressor()]\n\nevaluation_metrics = ['Mean Squared Error (MSE)',\n 'Root MSE (RMSE)',\n  'Mean Absolute Error',\n  'R2 Score', \n  'Explained Variance Score']\n\nThe model is fit on the training data, and predicted for the testing data. Regression models are commonly evaluated on the following metrics: 1. Mean Squared Error (MSE): MSE calculates the average squared difference between the predicted and actual values, providing a measure of the model’s precision. $ = _{i=1}^{n} (y_i - _i)^2$\n\nRoot Mean Squared Error (RMSE): RMSE is the square root of MSE and represents the average magnitude of the residuals in the same units as the target variable. $ = $\nMean Absolute Error (MAE): MAE calculates the average absolute difference between the predicted and actual values, providing a measure of the model’s accuracy. $ = _{i=1}^{n} |y_i - _i|$\nR2 Score: R2 Score, or the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variables. $ R^2 = 1 - where is the sum of squared residuals, and is the total sum of squares.$\nExplained Variance Score: The Explained Variance Score measures the proportion by which the model’s variance is reduced compared to a simple mean baseline. $ = 1 - where where y is the actual values and is the predicted values$\n\n\ny_preds = [] # list of model predictions\nmodel_scores = [] # list of model scores based on the evaluation metrics defined\nfor model in models:\n    reg = model\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    y_preds.append(y_pred)\n\n    mse = mean_squared_error(y_test.values, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_test.values, y_pred)\n    r2 = r2_score(y_test.values, y_pred)\n    evs = explained_variance_score(y_test.values, y_pred)\n\n    model_scores.append([mse, rmse, mae, r2, evs])\n\nLet us visualize the outputs of the linear models ‘Linear Regression Model’,‘Support Vector Machine (Regression)’, ‘Ridge Regression’, and ‘Lasso Regression’. and evaluate the results.\n\n\nCode\nplt.rcParams[\"figure.figsize\"] = [10,7]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, axs = plt.subplots(1, 1)\naxs.axis('tight')\naxs.axis('off')\n\ntable1 = axs.table(cellText=model_scores[0:4],\n                      cellLoc = 'left',\n                      rowLabels = model_names[0:4],\n                      rowColours= [\"palegreen\"] * 10,\n                      colLabels=evaluation_metrics,\n                      colColours= [\"palegreen\"] * 10,\n                      loc='center')\n\n# Highlight cells with minimum value in each column\nfor col_idx, metric in enumerate(evaluation_metrics):\n    col_values = [row[col_idx] for row in model_scores[0:4]]\n    min_value_idx = col_values.index(min(col_values))\n\n    # Highlight the cell with minimum value in coral color\n    table1[min_value_idx + 1, col_idx].set_facecolor(\"coral\")\n        \ntable1.auto_set_font_size(False)\ntable1.set_fontsize(14)\ntable1.scale(1, 4)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nNow, let us compare the performance of the linear models against non linear models on Linear Regression.\n\n\nCode\nplt.rcParams[\"figure.figsize\"] = [10, 7]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, axs = plt.subplots(1, 1)\naxs.axis('tight')\naxs.axis('off')\ntable2 = axs.table(cellText=model_scores,\n                      cellLoc = 'left',\n                      rowLabels = model_names,\n                      rowColours= [\"palegreen\"] * 10,\n                      colLabels=evaluation_metrics,\n                      colColours= [\"palegreen\"] * 10,\n                      loc='center')\n\n# Highlight cells with minimum value in each column\nfor col_idx, metric in enumerate(evaluation_metrics):\n    col_values = [row[col_idx] for row in model_scores]\n    min_value_idx = col_values.index(min(col_values))\n\n    # Highlight the cell with minimum value in coral color\n    table2[min_value_idx + 1, col_idx].set_facecolor(\"coral\")\n        \ntable2.auto_set_font_size(False)\ntable2.set_fontsize(14)\ntable2.scale(1, 4)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nAs we can see, non-linear models, due to their complex structure and abilty to map and analyze/learn from complex data, perform better on this task. However, it is not always that non-linear models are better than linear models since we must keep in mind the computational expense and efficiency of a model for a task, as well as the bias-variance tradeoff which again, is dependent not only on the model but also on the dataset/application at hand.\nAnalysing the difference between the actual and predicted values for the regression task by each model on 3 randomly chosen data points.\n\n\nCode\n# printing how far the predicted value is to the actual value for a random row in X\nimport random\nfig, ax = plt.subplots(figsize=(10, 5));\n\nlength = len(model_names)\n\nfor i in range(3):\n    idx = random.randint(0,len(y_test)-1)\n    plt.plot(range(length), [(y_test.values)[idx]]*length, label='True Value');\n    plt.scatter(range(length), [y_preds[q][idx] for q in range(length)], label='Predicted Values');\n    for j in range(length):\n        plt.plot([j, j], [(y_test.values)[idx], y_preds[j][idx]], color='gray', linestyle='--', linewidth=0.8)\n    plt.xticks(range(length), model_names)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nNon Linear Regression Nonlinear regression is a statistical technique that helps describe nonlinear relationships in experimental data. Nonlinear regression models are generally assumed to be parametric, where the model is described as a nonlinear equation. Typically machine learning methods are used for non-parametric nonlinear regression. Parametric nonlinear regression models the dependent variable (also called the response) as a function of a combination of nonlinear parameters and one or more independent variables (called predictors). The model can be univariate (single response variable) or multivariate (multiple response variables).The parameters can take the form of an exponential, trigonometric, power, or any other nonlinear function.\nThe non-linear dataset models China’s GDP value for each year from 1960-2014. A sample of the dataset and the dataset visualization can be seen below.\n\n\nCode\ndf_gdp = pd.read_csv(\"China_GDP.csv\")\nprint(df_gdp.info())\nprint(df_gdp.isna().sum())\ndf_gdp.head(5)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 55 entries, 0 to 54\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Year    55 non-null     int64  \n 1   Value   55 non-null     float64\ndtypes: float64(1), int64(1)\nmemory usage: 1008.0 bytes\nNone\nYear     0\nValue    0\ndtype: int64\n\n\n\n\n\n\n\n\n\nYear\nValue\n\n\n\n\n0\n1960\n5.918412e+10\n\n\n1\n1961\n4.955705e+10\n\n\n2\n1962\n4.668518e+10\n\n\n3\n1963\n5.009730e+10\n\n\n4\n1964\n5.906225e+10\n\n\n\n\n\n\n\n\n# plot Year vs GDP_value\nsns.scatterplot(data=df_gdp, x = 'Value', y = 'Year');\nplt.show()\n\n\n\n\nWe try to apply a regression line plot for the data. We see that the regression line is not able to accurately capture a linear relationship due to the non-linear relationship between the variables.\n\n# Regression line plot\nsns.regplot(x=df_gdp['Value'],y=df_gdp['Year'],scatter_kws={\"color\": 'blue'}, line_kws={\"color\": \"black\"})\nplt.show()\n\n\n\n\nSplitting the dataset into 80-20 training-testing set to train and evaluate the aforementioned models.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\ny = df_gdp['Year']\nX = (df_gdp.drop(columns = ['Year'])).to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nWe can plot the training set points as well as the true test set points and predicted test set points for each model (Linear and Non-Linear) to visualize model accuracy and performance. Again, we see that the linear models struggle to accurately predict the target value for a non-linear dataset.\n\n\nCode\nfig, ax = plt.subplots(2,2, figsize=(10, 10));\nfor idx, model in enumerate(models[0:4]):\n    reg = model\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    # Plot the data points for training set\n    ax[idx//2][idx%2].scatter(X_train, y_train, marker='o', color='black', label='Train');\n    # Plot the data points for testing set (true)\n    ax[idx//2][idx%2].scatter(X_test, y_test, color='purple', marker='o', label='True');\n    # Plot the data points for testing set (predicted)\n    ax[idx//2][idx%2].scatter(X_test, y_pred, color='blue', marker='o', label='Predicted');\n    ax[idx//2][idx%2].set_title(model_names[idx])\n    ax[idx//2][idx%2].set_xlabel(\"GDP\")\n    ax[idx//2][idx%2].set_xlabel(\"Year\")\n    ax[idx//2][idx%2].legend()\nplt.title(\"True vs Predicted Performance of Linear Regression Models\")\nplt.show()\n\n\n\n\n\nHowever, the complex, non-linear models are able to capture and analyze the non-linearity and predict the target variable value more accurately.\n\n\nCode\ny_preds = [] # list of model predictions\nmodel_scores = [] # list of model scores based on the evaluation metrics defined\n\nfig, ax = plt.subplots(3, 1, figsize=(10, 10));\nfor idx, model in enumerate(models[4:]):\n    reg = model\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    # Plot the data points for training set\n    ax[idx].scatter(X_train, y_train, marker='o', color='black', label='Train');\n    # Plot the data points for testing set (true)\n    ax[idx].scatter(X_test, y_test, color='purple', marker='o', label='True');\n    # Plot the data points for testing set (predicted)\n    ax[idx].scatter(X_test, y_pred, color='blue', marker='o', label='Predicted');\n    ax[idx].set_title(model_names[4+idx])\n    ax[idx].set_xlabel(\"GDP\")\n    ax[idx].set_xlabel(\"Year\")\n    ax[idx].legend()\n    \n    y_preds.append(y_pred)\n\n    mse = mean_squared_error(y_test.values, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_test.values, y_pred)\n    r2 = r2_score(y_test.values, y_pred)\n    evs = explained_variance_score(y_test.values, y_pred)\n\n    model_scores.append([mse, rmse, mae, r2, evs])\n    \nplt.title(\"True vs Predicted Performance of Non-Linear Regression Models\")\nplt.show()\n\n\n\n\n\nWe chart the model performance for the non-linear models. The models used for non-linear regression are Random Forest Regressor, Decision Tree Regressor, and Gradient Boost Regressor.\n\n\nCode\nplt.rcParams[\"figure.figsize\"] = [10, 7]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, axs = plt.subplots(1, 1)\naxs.axis('tight')\naxs.axis('off')\n\ntable1 = axs.table(cellText=model_scores,\n                      cellLoc = 'left',\n                      rowLabels = model_names[4:],\n                      rowColours= [\"palegreen\"] * 10,\n                      colLabels=evaluation_metrics,\n                      colColours= [\"palegreen\"] * 10,\n                      loc='center')\n\n# Highlight cells with minimum value in each column\nfor col_idx, metric in enumerate(evaluation_metrics):\n    col_values = [row[col_idx] for row in model_scores]\n    min_value_idx = col_values.index(min(col_values))\n\n    # Highlight the cell with minimum value in coral color\n    table1[min_value_idx + 1, col_idx].set_facecolor(\"coral\")\n        \ntable1.auto_set_font_size(False)\ntable1.set_fontsize(14)\ntable1.scale(1, 4)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nOnce again, we analyze the difference between the actual and predicted values for the regression task by each model on 3 randomly chosen data points.\n\n\nCode\n# printing how far the predicted value is to the actual value for a random row in X\nimport random\nfig, ax = plt.subplots(figsize=(10, 5));\n\nlength = len(model_names[4:])\n\nfor i in range(3):\n    idx = random.randint(0,len(y_test)-1)\n    plt.plot(range(length), [(y_test.values)[idx]]*length, label='True Value');\n    plt.scatter(range(length), [y_preds[q][idx] for q in range(length)], label='Predicted Values');\n    for j in range(length):\n        plt.plot([j, j], [(y_test.values)[idx], y_preds[j][idx]], color='gray', linestyle='--', linewidth=0.8)\n    plt.xticks(range(length), model_names[4:])\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nFrom this blog, we get a glimpse into the performance and approach to applying the appropriate based on the type of dataset at hand."
  },
  {
    "objectID": "posts/Clustering/Clustering.html",
    "href": "posts/Clustering/Clustering.html",
    "title": "Clustering with K-Means",
    "section": "",
    "text": "In this blog, we explore the application of identifying the most dominant colours in an image using K-Means Clustering.\nUnsupervised Learning in artificial intelligence is a type of machine learning that learns from data without human supervision. Unlike supervised learning, unsupervised machine learning models are given unlabeled data and allowed to discover patterns and insights without any explicit guidance or instruction.\nClustering is an unsupervised machine learning technique. Clustering is the process of building groups of data points in a way that the data in the same group are more similar to one another than to those in other groups.\nClustering algorithms include agglomerative clustering, Gaussian mixtures for clustering, K-Means clustering, hierarchial clustering, DBSCAN, and much more. For the application of extracting dominant colours from an image we make use of the K-Means algorithm (distance-based) over some of the other popular density based clustering algorithms like DBSCAN (popularly used for anomaly detection) Since the similarity between the colours can be simply represented by the location in the 3D space. Many distance-based clustering algorithms, including K-means, are computationally efficient and can handle large datasets, making them suitable for processing images with a large number of pixels. The efficiency of these algorithms allows for faster clustering and analysis of image data.\nThe K-means algorithms is one of the most popular clustering algorithms. It can cluster groups of data given the number of clusters to form. It begins by selecting n cluster centroids (randomly or using the kmeans++ initialization algorithm) and assigning the data points to each cluster based on its Euclidean Distance from the cluster centroid. A point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid. K-Means finds the best centroids by alternating between (1) assigning data points to clusters based on the current centroids (2) chosing centroids (points which are the center of a cluster) based on the current assignment of data points to clusters. It continues this process for a number of iterations while trying to minimize the root mean square error each iteration (till the centroids dont change) and improve clustering.\nA great example of this can be seen in the figure below taken from this KMeans article.\n\n\n\nK-Means Algorithm Visualization Sample\n\n\nComing to extracting the most dominant colours in an image, let’s take the example of the following image. \nWe begin by analyzing the image by extracting the RGB (Red, Green, Blue) values that together make up the colour of the pixel. By making use of the cv2 library from OpenCV, we can essentially “read” the image, i.e. get it’s pixel value, and store the data in a Pandas dataframe for future use. An example of the RGB values for the sample image can be seen below.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport cv2\nimport numpy as np\n\nRGB_values = pd.DataFrame(columns=['R', 'G', 'B'])\nimage = cv2.imread(\"beach.jpg\")\n\n# get image shape\nnumPixels = image.shape\nprint(numPixels)\n\ny, x = numPixels[0], numPixels[1]\nfor i, j in zip(range(y), range(x)):\n    BGR_values = image[i, j]\n    RGB_values.loc[len(RGB_values)] = np.flip(BGR_values) # to get the (B, G, R) values in (R, G, B) format\n\nRGB_values.head(10)\n\n\n(1200, 1920, 3)\n\n\n\n\n\n\n\n\n\nR\nG\nB\n\n\n\n\n0\n161\n195\n240\n\n\n1\n161\n195\n240\n\n\n2\n160\n194\n239\n\n\n3\n159\n193\n238\n\n\n4\n159\n193\n238\n\n\n5\n158\n192\n237\n\n\n6\n157\n191\n237\n\n\n7\n157\n191\n237\n\n\n8\n156\n190\n236\n\n\n9\n156\n190\n236\n\n\n\n\n\n\n\nA lot of processing libraries, and machine learning algorithms require the data to be scaled or normalized. The colorsys library requires the RGB values to be normalized before being able to convert it into HSL values to generate the plot displaying the colours in the image in a list form! Additionally, since the K-Means clustering algorithm is a distance based algorithm, it is beneficial to rescale each feature dimension of the observation set by its standard deviation so that higher range of some features do not influence the algorithm by acting ‘weighted’. Each feature is normalized across all observations to give it unit variance.\n\n\nCode\n# plot the RGB values on a graph\nfrom mpl_toolkits import mplot3d\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport colorsys\n\n# normalize the RGB values\nRGB_values = RGB_values/255\n\nRGB_values.head(5)\n\nRGB_unique = RGB_values.drop_duplicates()\nRGB_unique = list(RGB_unique.to_numpy())\nRGB_unique.sort(key=lambda rgb: colorsys.rgb_to_hls(*rgb))\ncmap_RGB = matplotlib.colors.ListedColormap(RGB_unique, \"Colours in the image\")\ncmap_RGB\n\n\nColours in the image  underbad over \n\n\nWe can see below the (R, G, B) values plotted in a 3D space. This visualization is especially important to better analyze the K-Means clustering algorithm.\n\n\nCode\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(111, projection='3d')\n# Data for three-dimensional scattered points\nzdata = RGB_values['B']\nxdata = RGB_values['R']\nydata = RGB_values['G']\nax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='hsv');\n\nax.set_xlabel('R')\nax.set_ylabel('G')\nax.set_zlabel('B')\nax.set_title('RGB value of input image');\n\n\n\n\n\nSince K-Means clustering requires a defined number of clusters, for accurate clustering, it is important to have an optimum number of clusters, not too less, not too many. We make use of the KElbowVisualizer to find the optimal value for the number of clusters. From the documentation: “The KElbowVisualizer implements the “elbow” method to help data scientists select the optimal number of clusters by fitting the model with a range of values for . If the line chart resembles an arm, then the “elbow” (the point of inflection on the curve) is a good indication that the underlying model fits best at that point. In the visualizer “elbow” will be annotated with a dashed line. The elbow method runs k-means clustering on the dataset for a range of values for k (say from 1-10) and then for each value of k computes an average score for all clusters.” Read More!\nWe can see from the graph that for our data, 4 is the optimum number of clusters. So our final result would be the 4 most dominant colours in the image.\n\nimport sklearn.cluster\nfrom yellowbrick.cluster import KElbowVisualizer\nmodel = KElbowVisualizer(sklearn.cluster.KMeans(), k=10)\nmodel.fit(RGB_values.to_numpy());\nmodel.show();\nelbow_value = model.elbow_value_\n\n\n\n\nBelow is the algorithm for K-Means clustering with k-means++ initialization from scratch. The K-Means clustering algorithm can also be implemented by using the scikit learn library.\nWe begin by initializing values such as the dataset in the numpy format, setting the number of clusters, setting the maximum number of iterations, as well as choosing the first centroid at random.\nThen, we run the k-means++ algorithm to select the initial centroids. The k-means++ algorithm selects initial cluster centroids using sampling based on an empirical probability distribution of the points’ contribution to the overall inertia. This technique speeds up convergence.\n\nTake one centroid \\(c_1\\), chosen uniformly at random from the dataset.\nTake a new center \\(c_i\\), choosing an instance \\(\\mathbf{x}_i\\) with probability: \\(D(\\mathbf{x}_i)^2\\) / \\(\\sum\\limits_{j=1}^{m}{D(\\mathbf{x}_j)}^2\\) where \\(D(\\mathbf{x}_i)\\) is the distance between the instance \\(\\mathbf{x}_i\\) and the closest centroid that was already chosen. This probability distribution ensures that instances that are further away from already chosen centroids are much more likely be selected as centroids.\nRepeat the previous step until all \\(k\\) centroids have been chosen.\n\nAfter the centroids have been chosen, the K-means algorithm begins running continuously mapping each datapoint to each cluster and improving chosen centroid values until the maximum number of iterations or until there is no change in the centroids chosen.\n\nimport math\nimport random\n\nx = RGB_values.to_numpy()\ncentroid1 = x[random.randint(0,len(x))]\ncentroids = []\ncentroids.append([centroid1])\nnumClusters = elbow_value\niteration = 0\nmax_iter = 100 # can be passed in as a parameter\n\n'''\nk-means++ \n'''\n\n# pick cluster centroid with probability proportional to the centroid1\ndistance = [math.dist(centroid1, x[i])**2 for i in range(len(x))]\n\nfor i in range(1, numClusters):\n    # so above has just chosen the highest dist ones, but we still want random choice where the probability is depending on the distance\n    # also, normalize dists\n    # calculates probabilities\n    prob = distance/np.sum(distance)\n    # choose next centroid with probability proportional to distance squared\n    new_centroid = x[np.random.choice(range(len(x)), size=1, p=prob)]\n    centroids.append(new_centroid)\n    # update distances between newly chosen centroid and other points now\n    distance = [math.dist(new_centroid[0], x[i])**2 for i in range(len(x))]\n'''\nK-means algorithm\n'''\ncentroids = np.array(centroids)\nprev_centroids = np.zeros(centroids.shape)\nwhile np.not_equal(centroids, prev_centroids).any() and iteration &lt; max_iter:\n            # Sort each datapoint, assigning to nearest centroid\n            # sorted points is of size num_clusters, num_points in each cluster\n            sorted_points = [[] for _ in range(numClusters)]\n            for point in x:\n                dists = [math.dist(point, np.squeeze(i)) for i in centroids]\n                centroid_idx = np.argmin(dists)\n                sorted_points[centroid_idx].append(point)\n            # Push current centroids to previous, reassign centroids as mean of the points belonging to them\n            # new centroid is mean of the points in each cluster\n            prev_centroids = centroids[:]\n            centroids = [np.mean(cluster, axis=0) for cluster in sorted_points]\n            # make sure that none of the centroid values are nan\n            for points in range(len(centroids)):\n                if np.isnan(centroids[points]).any():\n                    centroids[points] = prev_centroids[points]\n            iteration += 1\n\nPlotting the clusters formed after K-Means clustering algorithm as shown below.\n\n\nCode\ncolors = []\n\nfor i in centroids:\n    r, g, b = i\n    colors.append((\n    r,\n    g,\n    b\n    ))\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(111, projection='3d')\n\n\n# plot the sorted clusters\n\nfor i in range(len(sorted_points)):\n    a, b, c = zip(*sorted_points[i])\n    ax.scatter(a, b, c, s = 40 , color = colors[i], marker = 'o', label = \"cluster \"+str(i))\n    label = \"centroid of cluster\" if i == len(sorted_points)-1 else \"\"\n    ax.scatter(colors[i][0], colors[i][1], colors[i][2], s = 100 , marker = 'x', color = [0,0,0], label = label)\n\nax.set_xlabel('R')\nax.set_ylabel('G')\nax.set_zlabel('B')\nax.legend()\nplt.show()\n\n\n\n\n\nWe can visualize the following to be the most dominant colours in the given image!\n\n\nCode\nplt.grid(False)\nplt.imshow([colors])\nplt.show()\n\n\n\n\n\n\n\nCode\ntotal_points = len(RGB_values.to_numpy())\nlabels = ['Colour' + str(i+1) for i in range(len(colors))]\nsizes = [len(sorted_points[i])/total_points for i in range(len(sorted_points))]\nfig, ax = plt.subplots()\nax.pie(sizes,\n       colors=colors, autopct='%1.1f%%', pctdistance=1.15);\n\n\n\n\n\nAnd we’re done!"
  },
  {
    "objectID": "posts/Anomaly Detection/Anomaly.html",
    "href": "posts/Anomaly Detection/Anomaly.html",
    "title": "Anomaly Detection with DBSCAN",
    "section": "",
    "text": "Unsupervised Learning in artificial intelligence is a type of machine learning that learns from data without human supervision. Unlike supervised learning, unsupervised machine learning models are given unlabeled data and allowed to discover patterns and insights without any explicit guidance or instruction.\nClustering is an unsupervised machine learning technique. Clustering is the process of building groups of data points in a way that the data in the same group are more similar to one another than to those in other groups. Clustering algorithms include agglomerative clustering, Gaussian mixtures for clustering, K-Means clustering, hierarchial clustering, DBSCAN, and much more.\nAnomaly Detection is a use case of the clustering algorithm to identify noise, exceptions, or outliers in the data which deviate significantly from standard behaviors or patterns. Density based clustering algorithms are especially useful in anomaly detection. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is one such algorithm which is popularly used in anomaly detection.\nUnlike methods that rely on distance thresholds (like in some distance-based clustering), DBSCAN automatically detects outliers without requiring a predefined distance threshold. It adapts to the local density of the data, making it robust to variations in the density of clusters. DBSCAN can scale to large datasets well and can handle clusters of arbitrary shapes, making it suitable for datasets where outliers might be located in regions with irregular shapes or non-uniform density.\nIn this blog, we analyze the ionosphere dataset from the UCI Machine Learning Repository to identify the “Bad” radars from the dataset.\n“This radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. See the paper for more details. The targets were free electrons in the ionosphere.”Good” radar returns are those showing evidence of some type of structure in the ionosphere. “Bad” returns are those that do not; their signals pass through the ionosphere.” As can be seen from the tabe, the data has already been normalized.\n\n\nCode\nfrom ucimlrepo import fetch_ucirepo \nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# fetch dataset \nionosphere = fetch_ucirepo(id=52) \n\n\n# data (as pandas dataframes) \nX = ionosphere.data.features \ny = ionosphere.data.targets \n\n# metadata \nprint(ionosphere.metadata) \n  \n# variable information \nprint(ionosphere.variables) \n\n\n{'uci_id': 52, 'name': 'Ionosphere', 'repository_url': 'https://archive.ics.uci.edu/dataset/52/ionosphere', 'data_url': 'https://archive.ics.uci.edu/static/public/52/data.csv', 'abstract': 'Classification of radar returns from the ionosphere', 'area': 'Physics and Chemistry', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 351, 'num_features': 34, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['Class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1989, 'last_updated': 'Sun Jan 01 1989', 'dataset_doi': '10.24432/C5W01B', 'creators': ['V. Sigillito', 'S. Wing', 'L. Hutton', 'K. Baker'], 'intro_paper': None, 'additional_info': {'summary': 'This radar data was collected by a system in Goose Bay, Labrador.  This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts.  See the paper for more details.  The targets were free electrons in the ionosphere. \"Good\" radar returns are those showing evidence of some type of structure in the ionosphere.  \"Bad\" returns are those that do not; their signals pass through the ionosphere.  \\r\\n\\r\\nReceived signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number.  There were 17 pulse numbers for the Goose Bay system.  Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '-- All 34 are continuous\\r\\n-- The 35th attribute is either \"good\" or \"bad\" according to the definition summarized above.  This is a binary classification task.\\r\\n', 'citation': None}}\n           name     role         type demographic description units  \\\n0    Attribute1  Feature   Continuous        None        None  None   \n1    Attribute2  Feature   Continuous        None        None  None   \n2    Attribute3  Feature   Continuous        None        None  None   \n3    Attribute4  Feature   Continuous        None        None  None   \n4    Attribute5  Feature   Continuous        None        None  None   \n5    Attribute6  Feature   Continuous        None        None  None   \n6    Attribute7  Feature   Continuous        None        None  None   \n7    Attribute8  Feature   Continuous        None        None  None   \n8    Attribute9  Feature   Continuous        None        None  None   \n9   Attribute10  Feature   Continuous        None        None  None   \n10  Attribute11  Feature   Continuous        None        None  None   \n11  Attribute12  Feature   Continuous        None        None  None   \n12  Attribute13  Feature   Continuous        None        None  None   \n13  Attribute14  Feature   Continuous        None        None  None   \n14  Attribute15  Feature   Continuous        None        None  None   \n15  Attribute16  Feature   Continuous        None        None  None   \n16  Attribute17  Feature   Continuous        None        None  None   \n17  Attribute18  Feature   Continuous        None        None  None   \n18  Attribute19  Feature   Continuous        None        None  None   \n19  Attribute20  Feature   Continuous        None        None  None   \n20  Attribute21  Feature   Continuous        None        None  None   \n21  Attribute22  Feature   Continuous        None        None  None   \n22  Attribute23  Feature   Continuous        None        None  None   \n23  Attribute24  Feature   Continuous        None        None  None   \n24  Attribute25  Feature   Continuous        None        None  None   \n25  Attribute26  Feature   Continuous        None        None  None   \n26  Attribute27  Feature   Continuous        None        None  None   \n27  Attribute28  Feature   Continuous        None        None  None   \n28  Attribute29  Feature   Continuous        None        None  None   \n29  Attribute30  Feature   Continuous        None        None  None   \n30  Attribute31  Feature   Continuous        None        None  None   \n31  Attribute32  Feature   Continuous        None        None  None   \n32  Attribute33  Feature   Continuous        None        None  None   \n33  Attribute34  Feature   Continuous        None        None  None   \n34        Class   Target  Categorical        None        None  None   \n\n   missing_values  \n0              no  \n1              no  \n2              no  \n3              no  \n4              no  \n5              no  \n6              no  \n7              no  \n8              no  \n9              no  \n10             no  \n11             no  \n12             no  \n13             no  \n14             no  \n15             no  \n16             no  \n17             no  \n18             no  \n19             no  \n20             no  \n21             no  \n22             no  \n23             no  \n24             no  \n25             no  \n26             no  \n27             no  \n28             no  \n29             no  \n30             no  \n31             no  \n32             no  \n33             no  \n34             no  \n\n\n\n\n\n\n\n\n\n\n\nAttribute1\nAttribute2\nAttribute3\nAttribute4\nAttribute5\nAttribute6\nAttribute7\nAttribute8\nAttribute9\nAttribute10\n...\nAttribute25\nAttribute26\nAttribute27\nAttribute28\nAttribute29\nAttribute30\nAttribute31\nAttribute32\nAttribute33\nAttribute34\n\n\n\n\n0\n1\n0\n0.99539\n-0.05889\n0.85243\n0.02306\n0.83398\n-0.37708\n1.00000\n0.03760\n...\n0.56811\n-0.51171\n0.41078\n-0.46168\n0.21266\n-0.34090\n0.42267\n-0.54487\n0.18641\n-0.45300\n\n\n1\n1\n0\n1.00000\n-0.18829\n0.93035\n-0.36156\n-0.10868\n-0.93597\n1.00000\n-0.04549\n...\n-0.20332\n-0.26569\n-0.20468\n-0.18401\n-0.19040\n-0.11593\n-0.16626\n-0.06288\n-0.13738\n-0.02447\n\n\n2\n1\n0\n1.00000\n-0.03365\n1.00000\n0.00485\n1.00000\n-0.12062\n0.88965\n0.01198\n...\n0.57528\n-0.40220\n0.58984\n-0.22145\n0.43100\n-0.17365\n0.60436\n-0.24180\n0.56045\n-0.38238\n\n\n3\n1\n0\n1.00000\n-0.45161\n1.00000\n1.00000\n0.71216\n-1.00000\n0.00000\n0.00000\n...\n1.00000\n0.90695\n0.51613\n1.00000\n1.00000\n-0.20099\n0.25682\n1.00000\n-0.32382\n1.00000\n\n\n4\n1\n0\n1.00000\n-0.02401\n0.94140\n0.06531\n0.92106\n-0.23255\n0.77152\n-0.16399\n...\n0.03286\n-0.65158\n0.13290\n-0.53206\n0.02431\n-0.62197\n-0.05707\n-0.59573\n-0.04608\n-0.65697\n\n\n\n\n5 rows × 34 columns\n\n\n\nSince the DBSCAN algorithm maps density based on a distance metric, the greater the number of dimensions, the harder it becomes for the algorithm to map the data points accurately. By applying Principal Component Analysis (PCA), we can reduce the number of dimensions. PCA transforms high-dimensional data into a lower-dimensional representation by identifying and emphasizing the principal components using statistical methods. By retaining only the most informative components, PCA simplifies data while preserving essential patterns.\nBy plotting a scree plot, we map the variance explained which helps determine the dimensions the final dataset can be reduced to without losing too much information. The “elbow” of the plot is usually considered the optimum value.\n\n\nCode\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#define PCA model to use\npca = PCA(n_components=len(X.columns))\n\n#fit PCA model to data\npca_fit = pca.fit(X)\n\n# scree plot\nPC_values = np.arange(pca.n_components_) + 1\nplt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Variance Explained')\nplt.show()\n\n\n\n\n\nWe can see from the above plot that we can reduce the 34 column dimension data by projecting it into a 3D space.\nThe DBSCAN algorithm is mainly based on a metric function (normally the Euclidean distance) and a radius, \\({epsilon}\\). Given a sample point, its boundary is checked for other samples. If it is surrounded by at least \\(m\\) minimum points, it becomes a core point. If the number of points are less than \\(m\\), the point is classified as a boundary point, and if there are no other data points around within \\({epsilon}\\) radius, it is considered a noise point.\n\n\n\nDBSCAN working\n\n\nIt is important to understand the optimum epsilon \\({epsilon}\\) value for the best model performance to ensure that it does not classify data points with slight deviations from the normal to be considered noise (very low \\({epsilon}\\)) and so that it does not include data points that are noise to be normal (very large \\({epsilon}\\)).\n\n\nCode\nfrom sklearn.neighbors import NearestNeighbors\nnbrs = NearestNeighbors(n_neighbors=2, metric='cosine').fit(X)\ndistances, indices = nbrs.kneighbors(X)\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\n\n# Plot the k-distance graph\nplt.plot(distances)\nplt.title('k-distance plot')\nplt.xlabel('Data Point Index')\nplt.ylabel('Distance to k-th nearest neighbor')\n\n\n# Find the optimal epsilon (knee point)\nknee_point_index = np.argmax(np.diff(distances))  # Find the index with the maximum difference in distances\nepsilon = distances[knee_point_index]\nplt.axvline(x=knee_point_index, color='r', linestyle='--', label=f'Optimal Epsilon = {epsilon:.2f}')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\n\nWe can identify the optimum epsilon value from the ‘knee point’ of this graph. You can read more about this here!\nIn the below code, we structure the data into 3D using PCA and initialize and fit the DBSCAN model on the transformed data with the optimum chosen epsilon value. The DBSCAN implementation is imported from the scikit-learn library.\n\n\nCode\nfrom sklearn.cluster import DBSCAN\nimport collections\n\nprincipalComponents = PCA(n_components=3).fit_transform(X)\n\nprincipalDf = pd.DataFrame(data = principalComponents)\n# initialize DBSCAN and fit data\ncluster = DBSCAN(eps=epsilon).fit(principalDf)\n\nprincipalDf.columns=['PCA1','PCA2','PCA3']\nprincipalDf\n\nprint(collections.Counter(cluster.labels_))\n\n\nCounter({0: 252, -1: 49, 1: 32, 2: 18})\n\n\nAfter fitting the data, the data points have been assigned clusters as can be seen in the above output. The datapoints assigned to cluster ‘-1’ are considered to be the outlier points.\nAs can be seen from the 3D plot below, the outliers are low-density points.\n\n\nCode\nimport seaborn as sns\nimport numpy as np\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import ListedColormap\n\n\n# axes instance\nfig = plt.figure(figsize=(6,6))\nax = Axes3D(fig, auto_add_to_figure=False)\nfig.add_axes(ax)\n\n# get colormap from seaborn\ncmap = plt.cm.get_cmap('viridis', 2) \nx = principalDf['PCA1']\ny = principalDf['PCA2']\nz = principalDf['PCA3']\n\n# plot\nax.scatter(x, y, z, s=40, c=np.array(cluster.labels_)&gt;-1, marker='o', cmap=cmap, alpha=1)\n\nax.set_xlabel('X Label')\nax.set_ylabel('Y Label')\nax.set_zlabel('Z Label')\n\nplt.show()\n\n\n\n\n\nWe can also use the sns pairplot to visualize the distribution plots capturing relationship between the datapoints in each dimension . We can also visualize the representation of the outliers vs normal datapoints.\n\n\nCode\nprincipalDf['labels'] = cluster.labels_ &gt; -1\nsns.pairplot(data=principalDf, hue='labels')\n\n\n\n\n\nAs we can see, DBSCAN has proven to be effective in separating outliers from the data and is effective in applications of cleaning datasets, fraud detection, outlier detection, etc."
  },
  {
    "objectID": "codeFiles/Regression/Regression_Analysis.html",
    "href": "codeFiles/Regression/Regression_Analysis.html",
    "title": "CS5805",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndf_bike = pd.read_csv(\"SeoulBikeData.csv\")\ndf_bike.head(5)\n\n\n\n\n\n\n\n\nDate\nRented Bike Count\nHour\nTemperature(C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nSeasons\nHoliday\nFunctioning Day\n\n\n\n\n0\n01/12/2017\n254\n0\n-5.2\n37\n2.2\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n1\n01/12/2017\n204\n1\n-5.5\n38\n0.8\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n2\n01/12/2017\n173\n2\n-6.0\n39\n1.0\n2000\n-17.7\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n3\n01/12/2017\n107\n3\n-6.2\n40\n0.9\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n4\n01/12/2017\n78\n4\n-6.0\n36\n2.3\n2000\n-18.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n\n\n\n\n\nhttps://stackoverflow.com/questions/29432629/plot-correlation-matrix-using-pandas\n\ndf_bike.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 14 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Date                      8760 non-null   object \n 1   Rented Bike Count         8760 non-null   int64  \n 2   Hour                      8760 non-null   int64  \n 3   Temperature(C)            8760 non-null   float64\n 4   Humidity(%)               8760 non-null   int64  \n 5   Wind speed (m/s)          8760 non-null   float64\n 6   Visibility (10m)          8760 non-null   int64  \n 7   Dew point temperature(C)  8760 non-null   float64\n 8   Solar Radiation (MJ/m2)   8760 non-null   float64\n 9   Rainfall(mm)              8760 non-null   float64\n 10  Snowfall (cm)             8760 non-null   float64\n 11  Seasons                   8760 non-null   object \n 12  Holiday                   8760 non-null   object \n 13  Functioning Day           8760 non-null   object \ndtypes: float64(6), int64(4), object(4)\nmemory usage: 958.2+ KB\n\n\n\n#check for count of missing values in each column.\ndf_bike.isna().sum()\ndf_bike.isnull().sum()\n\nDate                        0\nRented Bike Count           0\nHour                        0\nTemperature(C)              0\nHumidity(%)                 0\nWind speed (m/s)            0\nVisibility (10m)            0\nDew point temperature(C)    0\nSolar Radiation (MJ/m2)     0\nRainfall(mm)                0\nSnowfall (cm)               0\nSeasons                     0\nHoliday                     0\nFunctioning Day             0\ndtype: int64\n\n\n\n# Can break the date into date, month, year columns and convert them into integers (from strings) for the purpose of correlation map\n\ndays = [int((df_bike['Date'].iloc[i])[0:2]) for i in range(len(df_bike))]\nmonth = [int((df_bike['Date'].iloc[i])[3:5]) for i in range(len(df_bike))]\nyear = [int((df_bike['Date'].iloc[i])[6:]) for i in range(len(df_bike))]\ndf_bike['Day'], df_bike['Month'], df_bike['Year'] = days, month, year\n\n\ndf_bike.head(5)\n\n\n\n\n\n\n\n\nDate\nRented Bike Count\nHour\nTemperature(C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nSeasons\nHoliday\nFunctioning Day\nDay\nMonth\nYear\n\n\n\n\n0\n01/12/2017\n254\n0\n-5.2\n37\n2.2\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n1\n12\n2017\n\n\n1\n01/12/2017\n204\n1\n-5.5\n38\n0.8\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n1\n12\n2017\n\n\n2\n01/12/2017\n173\n2\n-6.0\n39\n1.0\n2000\n-17.7\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n1\n12\n2017\n\n\n3\n01/12/2017\n107\n3\n-6.2\n40\n0.9\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n1\n12\n2017\n\n\n4\n01/12/2017\n78\n4\n-6.0\n36\n2.3\n2000\n-18.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n1\n12\n2017\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf1_bike = df_bike.drop(columns = ['Date'])\n# map unique season to numbers, map holiday to binary, and functioning day to binary\nseasons = {}\nfor idx, i in enumerate(df_bike['Seasons'].drop_duplicates()):\n    seasons[i] = idx\nholiday = {\"No Holiday\": 0, \"Holiday\": 1}\nfunctioning = {\"Yes\": 0, \"No\": 1}\ndf1_bike.Holiday = [holiday[item] for item in df_bike.Holiday]\ndf1_bike.Seasons = [seasons[item] for item in df_bike.Seasons]\ndf1_bike['Functioning Day'] = [functioning[item] for item in df1_bike['Functioning Day'] ]\n\ndf1_bike.head(3)\n\n\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = df1_bike.corr()\nsns.heatmap(corr,\n    cmap=sns.diverging_palette(220, 10, as_cmap=True),\n    vmin=-1.0, vmax=1.0,\n    square=True, ax=ax)\n\n   Rented Bike Count  Hour  Temperature(C)  Humidity(%)  Wind speed (m/s)  \\\n0                254     0            -5.2           37               2.2   \n1                204     1            -5.5           38               0.8   \n2                173     2            -6.0           39               1.0   \n\n   Visibility (10m)  Dew point temperature(C)  Solar Radiation (MJ/m2)  \\\n0              2000                     -17.6                      0.0   \n1              2000                     -17.6                      0.0   \n2              2000                     -17.7                      0.0   \n\n   Rainfall(mm)  Snowfall (cm)  Seasons  Holiday  Functioning Day  Day  Month  \\\n0           0.0            0.0        0        0                0    1     12   \n1           0.0            0.0        0        0                0    1     12   \n2           0.0            0.0        0        0                0    1     12   \n\n   Year  \n0  2017  \n1  2017  \n2  2017  \n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nplt.rcParams[\"figure.autolayout\"] = True\nfig, ax = plt.subplots(2, 2, figsize=(18, 6));\n# hour vs bike count\nsns.barplot(data=df1_bike,x='Hour',y='Rented Bike Count',ax=ax[0][0], palette='viridis');\nax[0][0].set(title='Count of Rented bikes acording to Hour');\n\n# Functioning vs bike count\nsns.barplot(data=df1_bike,x='Functioning Day',y='Rented Bike Count',ax=ax[0][1], palette='inferno');\nax[0][1].set(title='Count of Rented bikes acording to Functioning Day');\nax[0][1].set_xticklabels(['Yes', 'No'])\n\n# season vs bike count\nsns.barplot(data=df1_bike,x='Seasons', y='Rented Bike Count',ax=ax[1][0], palette='plasma');\nax[1][0].set(title='Count of Rented bikes acording to Seasons');\nax[1][0].set_xticklabels(['Winter', 'Spring', 'Summer', 'Autumn'])\n\n# month vs bike count\nsns.barplot(data=df1_bike,x='Month',y='Rented Bike Count',ax=ax[1][1], palette='cividis');\nax[1][1].set(title='Count of Rented bikes acording to Month ');\n\nplt.show()\n\n\n\n\n\n\nfig,ax=plt.subplots(figsize=(20,8))\nsns.pointplot(data=df1_bike,x='Hour',y='Rented Bike Count',hue='Seasons',ax=ax);\nax.set(title='Count of Rented bikes acording to seasons and hour of the day');\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(30, 6));\n# temperature vs bike count\n# Convert temperature in groups of 5C and average the rented bike counts for that range (rounding to 5s)\ntemp_min, temp_max = round(min(df1_bike['Temperature(C)'])/5)*5, round(max(df1_bike['Temperature(C)'])/5)*5\ndict_temp = {}\nfor i in range(temp_min, temp_max, 5):\n    # Filter rows based on the temperature interval\n    filtered_df = df1_bike[(df1_bike['Temperature(C)'] &gt;= i) & (df1_bike['Temperature(C)'] &lt; i+5)]\n    dict_temp[i] = filtered_df['Rented Bike Count'].mean()\n# print(dict_temp)\n# print(temp_max, temp_min)\nsns.barplot(data=dict_temp,ax=ax, palette='plasma');\nax.set(title='Count of Rented bikes acording to Temperature');\n\n# plt.show()\n\n\n\n\n\n# printing the regression plot for all the numerical features\nfig,ax=plt.subplots(4, 4, figsize=(40,40)) # since we know there are 16 features\nfor idx, col in enumerate(df1_bike.columns):\n  sns.regplot(x=df1_bike[col],y=df1_bike['Rented Bike Count'],scatter_kws={\"color\": 'blue'}, line_kws={\"color\": \"black\"}, ax=ax[idx//4][idx%4])\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ny = df1_bike['Rented Bike Count']\nX = (df1_bike.drop(columns = ['Rented Bike Count'])).to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn import linear_model\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n\nmodel_names = ['Linear Regression Model',\n 'Support Vector Machine (Regression)',\n 'Ridge Regression',\n  'Lasso Regression',\n 'Gradient Boosting Regression',\n  'Random Forest',\n  'Decision Tree']\nmodels = [LinearRegression(),\n    SVR(), \n    linear_model.Ridge(), \n    linear_model.Lasso(),\n    GradientBoostingRegressor(),\n    RandomForestRegressor(),\n    DecisionTreeRegressor()]\n\nevaluation_metrics = ['Mean Squared Error (MSE)',\n 'Root MSE (RMSE)',\n  'Mean Absolute Error',\n  'R2 Score', \n  'Explained Variance Score']\n\n\n\ny_preds = [] # list of model predictions\nmodel_scores = [] # list of model scores based on the evaluation metrics defined\nfor model in models:\n    reg = model\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    y_preds.append(y_pred)\n\n    mse = mean_squared_error(y_test.values, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_test.values, y_pred)\n    r2 = r2_score(y_test.values, y_pred)\n    evs = explained_variance_score(y_test.values, y_pred)\n\n    model_scores.append([mse, rmse, mae, r2, evs])\n\n\nplt.rcParams[\"figure.figsize\"] = [30, 7]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, axs = plt.subplots(1, 1)\naxs.axis('tight')\naxs.axis('off')\n\ntable1 = axs.table(cellText=model_scores[0:4],\n                      cellLoc = 'left',\n                      rowLabels = model_names[0:4],\n                      rowColours= [\"palegreen\"] * 10,\n                      colLabels=evaluation_metrics,\n                      colColours= [\"palegreen\"] * 10,\n                      loc='center')\n\n# Highlight cells with minimum value in each column\nfor col_idx, metric in enumerate(evaluation_metrics):\n    col_values = [row[col_idx] for row in model_scores[0:4]]\n    min_value_idx = col_values.index(min(col_values))\n\n    # Highlight the cell with minimum value in coral color\n    table1[min_value_idx + 1, col_idx].set_facecolor(\"coral\")\n        \ntable1.auto_set_font_size(False)\ntable1.set_fontsize(14)\ntable1.scale(1, 4)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nplt.rcParams[\"figure.figsize\"] = [30, 7]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, axs = plt.subplots(1, 1)\naxs.axis('tight')\naxs.axis('off')\ntable2 = axs.table(cellText=model_scores,\n                      cellLoc = 'left',\n                      rowLabels = model_names,\n                      rowColours= [\"palegreen\"] * 10,\n                      colLabels=evaluation_metrics,\n                      colColours= [\"palegreen\"] * 10,\n                      loc='center')\n\n# Highlight cells with minimum value in each column\nfor col_idx, metric in enumerate(evaluation_metrics):\n    col_values = [row[col_idx] for row in model_scores]\n    min_value_idx = col_values.index(min(col_values))\n\n    # Highlight the cell with minimum value in coral color\n    table2[min_value_idx + 1, col_idx].set_facecolor(\"coral\")\n        \ntable2.auto_set_font_size(False)\ntable2.set_fontsize(14)\ntable2.scale(1, 4)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n# printing how far the predicted value is to the actual value for a random row in X\nimport random\nfig, ax = plt.subplots(figsize=(30, 5));\n\nlength = len(model_names)\n\nfor i in range(3):\n    idx = random.randint(0,len(y_test)-1)\n    plt.plot(range(length), [(y_test.values)[idx]]*length, label='True Value');\n    plt.scatter(range(length), [y_preds[q][idx] for q in range(length)], label='Predicted Values');\n    for j in range(length):\n        plt.plot([j, j], [(y_test.values)[idx], y_preds[j][idx]], color='gray', linestyle='--', linewidth=0.8)\n    plt.xticks(range(length), model_names)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ndf_gdp = pd.read_csv(\"China_GDP.csv\")\ndf_gdp.head(5)\n\n\nprint(df_gdp.info())\nprint(df_gdp.isna().sum())\n\n\n# plot Year vs GDP_value\nsns.scatterplot(data=df_gdp, x = 'Value', y = 'Year');\nplt.show()\n\n\nsns.regplot(x=df_gdp['Value'],y=df_gdp['Year'],scatter_kws={\"color\": 'blue'}, line_kws={\"color\": \"black\"})\n\n\nfrom sklearn.model_selection import train_test_split\ny = df_gdp['Year']\nX = (df_gdp.drop(columns = ['Year'])).to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nfig, ax = plt.subplots(2,2, figsize=(10, 10));\nfor idx, model in enumerate(models[0:4]):\n    reg = model\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    # Plot the data points for training set\n    ax[idx//2][idx%2].scatter(X_train, y_train, marker='o', color='black', label='Train');\n    # Plot the data points for testing set (true)\n    ax[idx//2][idx%2].scatter(X_test, y_test, color='purple', marker='o', label='True');\n    # Plot the data points for testing set (predicted)\n    ax[idx//2][idx%2].scatter(X_test, y_pred, color='blue', marker='o', label='Predicted');\n    ax[idx//2][idx%2].set_title(model_names[idx])\n    ax[idx//2][idx%2].set_xlabel(\"GDP\")\n    ax[idx//2][idx%2].set_xlabel(\"Year\")\n    ax[idx//2][idx%2].legend()\nplt.title(\"True vs Predicted Performance of Linear Regression Models\")\nplt.show()\n\n\n\ny_preds = [] # list of model predictions\nmodel_scores = [] # list of model scores based on the evaluation metrics defined\n\nfig, ax = plt.subplots(3, 1, figsize=(10, 10));\nfor idx, model in enumerate(models[4:]):\n    reg = model\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    # Plot the data points for training set\n    ax[idx].scatter(X_train, y_train, marker='o', color='black', label='Train');\n    # Plot the data points for testing set (true)\n    ax[idx].scatter(X_test, y_test, color='purple', marker='o', label='True');\n    # Plot the data points for testing set (predicted)\n    ax[idx].scatter(X_test, y_pred, color='blue', marker='o', label='Predicted');\n    ax[idx].set_title(model_names[4+idx])\n    ax[idx].set_xlabel(\"GDP\")\n    ax[idx].set_xlabel(\"Year\")\n    ax[idx].legend()\n    \n    y_preds.append(y_pred)\n\n    mse = mean_squared_error(y_test.values, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_test.values, y_pred)\n    r2 = r2_score(y_test.values, y_pred)\n    evs = explained_variance_score(y_test.values, y_pred)\n\n    model_scores.append([mse, rmse, mae, r2, evs])\n    \nplt.title(\"True vs Predicted Performance of Non-Linear Regression Models\")\nplt.show()\n\n\nplt.rcParams[\"figure.figsize\"] = [30, 7]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, axs = plt.subplots(1, 1)\naxs.axis('tight')\naxs.axis('off')\n\ntable1 = axs.table(cellText=model_scores,\n                      cellLoc = 'left',\n                      rowLabels = model_names[4:],\n                      rowColours= [\"palegreen\"] * 10,\n                      colLabels=evaluation_metrics,\n                      colColours= [\"palegreen\"] * 10,\n                      loc='center')\n\n# Highlight cells with minimum value in each column\nfor col_idx, metric in enumerate(evaluation_metrics):\n    col_values = [row[col_idx] for row in model_scores]\n    min_value_idx = col_values.index(min(col_values))\n\n    # Highlight the cell with minimum value in coral color\n    table1[min_value_idx + 1, col_idx].set_facecolor(\"coral\")\n        \ntable1.auto_set_font_size(False)\ntable1.set_fontsize(14)\ntable1.scale(1, 4)\nfig.tight_layout()\nplt.show()\n\n\n# printing how far the predicted value is to the actual value for a random row in X\nimport random\nfig, ax = plt.subplots(figsize=(30, 5));\n\nlength = len(model_names[4:])\n\nfor i in range(3):\n    idx = random.randint(0,len(y_test)-1)\n    plt.plot(range(length), [(y_test.values)[idx]]*length, label='True Value');\n    plt.scatter(range(length), [y_preds[q][idx] for q in range(length)], label='Predicted Values');\n    for j in range(length):\n        plt.plot([j, j], [(y_test.values)[idx], y_preds[j][idx]], color='gray', linestyle='--', linewidth=0.8)\n    plt.xticks(range(length), model_names[4:])\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "codeFiles/Clustering/Clustering_K-means.html",
    "href": "codeFiles/Clustering/Clustering_K-means.html",
    "title": "CS5805",
    "section": "",
    "text": "import pandas as pd\nimport cv2\nimport numpy as np\n\n\nRGB_values = pd.DataFrame(columns=['R', 'G', 'B'])\nimage = cv2.imread(\"beach.jpg\")\n\n# get image shape\nnumPixels = image.shape\nprint(numPixels)\n\ny, x = numPixels[0], numPixels[1]\nfor i, j in zip(range(y), range(x)):\n    BGR_values = image[i, j]\n    RGB_values.loc[len(RGB_values)] = np.flip(BGR_values)\n\nprint(RGB_values)\n\n(1200, 1920, 3)\n        R    G    B\n0     161  195  240\n1     161  195  240\n2     160  194  239\n3     159  193  238\n4     159  193  238\n...   ...  ...  ...\n1195   53   42   58\n1196   49   38   55\n1197   47   36   53\n1198   49   37   57\n1199   49   37   57\n\n[1200 rows x 3 columns]\n\n\n\n# plot the RGB values on a graph\nfrom mpl_toolkits import mplot3d\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nRGB_values = RGB_values/255\n\n\nRGB_values.head(5)\n\n\n\n\n\n\n\n\nR\nG\nB\n\n\n\n\n0\n0.631373\n0.764706\n0.941176\n\n\n1\n0.631373\n0.764706\n0.941176\n\n\n2\n0.627451\n0.760784\n0.937255\n\n\n3\n0.623529\n0.756863\n0.933333\n\n\n4\n0.623529\n0.756863\n0.933333\n\n\n\n\n\n\n\n\nimport colorsys\nRGB_unique = RGB_values.drop_duplicates()\nRGB_unique = list(RGB_unique.to_numpy())\nRGB_unique.sort(key=lambda rgb: colorsys.rgb_to_hls(*rgb))\ncmap_RGB = matplotlib.colors.ListedColormap(RGB_unique, \"Colours in the image\")\ncmap_RGB\n\nColours in the image  underbad over \n\n\n\nfig = plt.figure(figsize = (15,15))\nax = fig.add_subplot(111, projection='3d')\n# Data for three-dimensional scattered points\nzdata = RGB_values['B']\nxdata = RGB_values['R']\nydata = RGB_values['G']\nax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='hsv');\n\nax.set_xlabel('R')\nax.set_ylabel('G')\nax.set_zlabel('B')\nax.set_title('RGB value of input image');\n\n\n\n\n\nfrom sklearn.cluster import KMeans\n\n\nfrom yellowbrick.cluster import KElbowVisualizer\nmodel = KElbowVisualizer(KMeans(), k=(2,10))\nmodel.fit(RGB_values.to_numpy());\nmodel.show();\nelbow_value = model.elbow_value_\n\nc:\\Users\\Anu2001\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\Anu2001\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\Anu2001\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\Anu2001\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\Anu2001\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\Anu2001\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\Anu2001\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\Anu2001\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n# using the library\nnum_clusters = elbow_value\nkmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=\"auto\")\ny_clusters = kmeans.fit_predict(RGB_values.to_numpy())\n\n\nimport math\nimport random\n\nx = RGB_values.to_numpy()\ncentroid1 = x[random.randint(0,len(x))]\ncentroids = []\ncentroids.append([centroid1])\nnumClusters = elbow_value\n\n# pick cluster centroid with probability proportional to the centroid1\ndistance = [math.dist(centroid1, x[i])**2 for i in range(len(x))]\n\nfor i in range(1, numClusters):\n    # so above has just chosen the highest dist ones, but we still want random choice where the probability is depending on the distance\n    # also, normalize dists\n    # calculates probabilities\n    prob = distance/np.sum(distance)\n    # choose next centroid with probability proportional to distance squared\n    new_centroid = x[np.random.choice(range(len(x)), size=1, p=prob)]\n    centroids.append(new_centroid)\n    # update distances between newly chosen centroid and other points now\n    distance = [math.dist(new_centroid[0], x[i])**2 for i in range(len(x))]\n\n\ncentroids = np.array(centroids)\nsorted_points = [[] for _ in range(4)]\nfor point in x:\n    dists = [math.dist(point, np.squeeze(i)) for i in centroids]\n    centroid_idx = np.argmin(dists)\n    sorted_points[centroid_idx].append(point)\n    \n# new centroid is mean of the points in each cluster\nprev_centroids = centroids[:]\n\n# sorted points is of size num_clusters, num_points in each cluster\ncentroids = [np.average(i) for i in sorted_points]\n\n\n\n\n\ncentroids = [np.mean(cluster, axis=0) for cluster in sorted_points]\n\n# make sure that none of the centroid values are nan\nfor points in range(len(centroids)):\n    if np.isnan(centroids[points]).any():\n        centroids[points] = prev_centroids[points]\n\n\ncolors = []\n\nfor i in centroids:\n    r, g, b = i\n    colors.append((\n    r,\n    g,\n    b\n    ))\n\n\n\nfig = plt.figure(figsize = (15,15))\nax = fig.add_subplot(111, projection='3d')\n\n\n# plot the sorted clusters\n\nfor i in range(len(sorted_points)):\n    a, b, c = zip(*sorted_points[i])\n    ax.scatter(a, b, c, s = 40 , color = colors[i], marker = 'o', label = \"cluster \"+str(i))\n    label = \"centroid of cluster\" if i == len(sorted_points)-1 else \"\"\n    ax.scatter(colors[i][0], colors[i][1], colors[i][2], s = 100 , marker = 'x', color = [0,0,0], label = label)\n\nax.set_xlabel('R')\nax.set_ylabel('G')\nax.set_zlabel('B')\nax.legend()\nplt.show()\n\n\n\n\n\nplt.grid(False)\nplt.imshow([colors])\nplt.show()\n\n\n\n\n\ntotal_points = len(RGB_values.to_numpy())\nlabels = ['Colour' + str(i+1) for i in range(len(colors))]\nsizes = [len(sorted_points[i])/total_points for i in range(len(sorted_points))]\nfig, ax = plt.subplots()\nax.pie(sizes,\n       colors=colors, autopct='%1.1f%%', pctdistance=1.15);"
  },
  {
    "objectID": "codeFiles/AnomalyDetection/AnomalyDetection.html",
    "href": "codeFiles/AnomalyDetection/AnomalyDetection.html",
    "title": "CS5805",
    "section": "",
    "text": "from ucimlrepo import fetch_ucirepo \nimport pandas as pd\n  \n# fetch dataset \nionosphere = fetch_ucirepo(id=52) \n  \n# data (as pandas dataframes) \nX = ionosphere.data.features \ny = ionosphere.data.targets \n  \n# metadata \nprint(ionosphere.metadata) \n  \n# variable information \nprint(ionosphere.variables) \n\n{'uci_id': 52, 'name': 'Ionosphere', 'repository_url': 'https://archive.ics.uci.edu/dataset/52/ionosphere', 'data_url': 'https://archive.ics.uci.edu/static/public/52/data.csv', 'abstract': 'Classification of radar returns from the ionosphere', 'area': 'Physics and Chemistry', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 351, 'num_features': 34, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['Class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1989, 'last_updated': 'Sun Jan 01 1989', 'dataset_doi': '10.24432/C5W01B', 'creators': ['V. Sigillito', 'S. Wing', 'L. Hutton', 'K. Baker'], 'intro_paper': None, 'additional_info': {'summary': 'This radar data was collected by a system in Goose Bay, Labrador.  This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts.  See the paper for more details.  The targets were free electrons in the ionosphere. \"Good\" radar returns are those showing evidence of some type of structure in the ionosphere.  \"Bad\" returns are those that do not; their signals pass through the ionosphere.  \\r\\n\\r\\nReceived signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number.  There were 17 pulse numbers for the Goose Bay system.  Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '-- All 34 are continuous\\r\\n-- The 35th attribute is either \"good\" or \"bad\" according to the definition summarized above.  This is a binary classification task.\\r\\n', 'citation': None}}\n           name     role         type demographic description units  \\\n0    Attribute1  Feature   Continuous        None        None  None   \n1    Attribute2  Feature   Continuous        None        None  None   \n2    Attribute3  Feature   Continuous        None        None  None   \n3    Attribute4  Feature   Continuous        None        None  None   \n4    Attribute5  Feature   Continuous        None        None  None   \n5    Attribute6  Feature   Continuous        None        None  None   \n6    Attribute7  Feature   Continuous        None        None  None   \n7    Attribute8  Feature   Continuous        None        None  None   \n8    Attribute9  Feature   Continuous        None        None  None   \n9   Attribute10  Feature   Continuous        None        None  None   \n10  Attribute11  Feature   Continuous        None        None  None   \n11  Attribute12  Feature   Continuous        None        None  None   \n12  Attribute13  Feature   Continuous        None        None  None   \n13  Attribute14  Feature   Continuous        None        None  None   \n14  Attribute15  Feature   Continuous        None        None  None   \n15  Attribute16  Feature   Continuous        None        None  None   \n16  Attribute17  Feature   Continuous        None        None  None   \n17  Attribute18  Feature   Continuous        None        None  None   \n18  Attribute19  Feature   Continuous        None        None  None   \n19  Attribute20  Feature   Continuous        None        None  None   \n20  Attribute21  Feature   Continuous        None        None  None   \n21  Attribute22  Feature   Continuous        None        None  None   \n22  Attribute23  Feature   Continuous        None        None  None   \n23  Attribute24  Feature   Continuous        None        None  None   \n24  Attribute25  Feature   Continuous        None        None  None   \n25  Attribute26  Feature   Continuous        None        None  None   \n26  Attribute27  Feature   Continuous        None        None  None   \n27  Attribute28  Feature   Continuous        None        None  None   \n28  Attribute29  Feature   Continuous        None        None  None   \n29  Attribute30  Feature   Continuous        None        None  None   \n30  Attribute31  Feature   Continuous        None        None  None   \n31  Attribute32  Feature   Continuous        None        None  None   \n32  Attribute33  Feature   Continuous        None        None  None   \n33  Attribute34  Feature   Continuous        None        None  None   \n34        Class   Target  Categorical        None        None  None   \n\n   missing_values  \n0              no  \n1              no  \n2              no  \n3              no  \n4              no  \n5              no  \n6              no  \n7              no  \n8              no  \n9              no  \n10             no  \n11             no  \n12             no  \n13             no  \n14             no  \n15             no  \n16             no  \n17             no  \n18             no  \n19             no  \n20             no  \n21             no  \n22             no  \n23             no  \n24             no  \n25             no  \n26             no  \n27             no  \n28             no  \n29             no  \n30             no  \n31             no  \n32             no  \n33             no  \n34             no  \n\n\n\nprint(len(X.columns))\n\n34\n\n\n\nX.head(5) # already normalized between [-1,1] as seen\n\n\n\n\n\n\n\n\nAttribute1\nAttribute2\nAttribute3\nAttribute4\nAttribute5\nAttribute6\nAttribute7\nAttribute8\nAttribute9\nAttribute10\n...\nAttribute25\nAttribute26\nAttribute27\nAttribute28\nAttribute29\nAttribute30\nAttribute31\nAttribute32\nAttribute33\nAttribute34\n\n\n\n\n0\n1\n0\n0.99539\n-0.05889\n0.85243\n0.02306\n0.83398\n-0.37708\n1.00000\n0.03760\n...\n0.56811\n-0.51171\n0.41078\n-0.46168\n0.21266\n-0.34090\n0.42267\n-0.54487\n0.18641\n-0.45300\n\n\n1\n1\n0\n1.00000\n-0.18829\n0.93035\n-0.36156\n-0.10868\n-0.93597\n1.00000\n-0.04549\n...\n-0.20332\n-0.26569\n-0.20468\n-0.18401\n-0.19040\n-0.11593\n-0.16626\n-0.06288\n-0.13738\n-0.02447\n\n\n2\n1\n0\n1.00000\n-0.03365\n1.00000\n0.00485\n1.00000\n-0.12062\n0.88965\n0.01198\n...\n0.57528\n-0.40220\n0.58984\n-0.22145\n0.43100\n-0.17365\n0.60436\n-0.24180\n0.56045\n-0.38238\n\n\n3\n1\n0\n1.00000\n-0.45161\n1.00000\n1.00000\n0.71216\n-1.00000\n0.00000\n0.00000\n...\n1.00000\n0.90695\n0.51613\n1.00000\n1.00000\n-0.20099\n0.25682\n1.00000\n-0.32382\n1.00000\n\n\n4\n1\n0\n1.00000\n-0.02401\n0.94140\n0.06531\n0.92106\n-0.23255\n0.77152\n-0.16399\n...\n0.03286\n-0.65158\n0.13290\n-0.53206\n0.02431\n-0.62197\n-0.05707\n-0.59573\n-0.04608\n-0.65697\n\n\n\n\n5 rows × 34 columns\n\n\n\n\nfrom sklearn.decomposition import PCA\n\n#define PCA model to use\npca = PCA(n_components=len(X.columns))\n\n#fit PCA model to data\npca_fit = pca.fit(X)\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nPC_values = np.arange(pca.n_components_) + 1\nplt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Variance Explained')\nplt.show()\n\nprint(pca.explained_variance_ratio_)\n\n\n\n\n[3.13442567e-01 1.22715916e-01 7.47531588e-02 6.93713827e-02\n 4.87451555e-02 3.67628972e-02 3.00194104e-02 2.87417744e-02\n 2.70258301e-02 2.25412044e-02 2.06320031e-02 1.83866023e-02\n 1.74260667e-02 1.64547193e-02 1.44493400e-02 1.39720397e-02\n 1.28249614e-02 1.18505458e-02 1.13257238e-02 1.08961135e-02\n 1.00825974e-02 9.50662790e-03 7.79732167e-03 7.23070868e-03\n 6.96611126e-03 6.21700612e-03 5.76370177e-03 5.19643141e-03\n 4.86322151e-03 4.29335970e-03 4.07318815e-03 3.36833665e-03\n 2.30397592e-03 6.18156468e-32]\n\n\n\nfrom sklearn.neighbors import NearestNeighbors\nnbrs = NearestNeighbors(n_neighbors=2, metric='cosine').fit(X)\ndistances, indices = nbrs.kneighbors(X)\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\n\n# Plot the k-distance graph\nplt.plot(distances)\nplt.title('k-distance plot')\nplt.xlabel('Data Point Index')\nplt.ylabel('Distance to k-th nearest neighbor')\n\n\n# Find the optimal epsilon (knee point)\nknee_point_index = np.argmax(np.diff(distances))  # Find the index with the maximum difference in distances\nepsilon = distances[knee_point_index]\nplt.axvline(x=knee_point_index, color='r', linestyle='--', label=f'Optimal Epsilon = {epsilon:.2f}')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\nfrom sklearn.cluster import DBSCAN\n\nprincipalComponents = PCA(n_components=3).fit_transform(X)\n\nprincipalDf = pd.DataFrame(data = principalComponents)\ncluster = DBSCAN(eps=epsilon).fit(principalDf)\n\n\nprincipalDf.columns=['PCA1','PCA2','PCA3']\nprincipalDf\n\n\n\n\n\n\n\n\nPCA1\nPCA2\nPCA3\n\n\n\n\n0\n-0.859333\n-0.961407\n-0.586082\n\n\n1\n0.765524\n-1.062714\n-1.397339\n\n\n2\n-1.116817\n-0.392256\n0.007988\n\n\n3\n0.981679\n0.590445\n0.478505\n\n\n4\n0.132848\n-0.788272\n-0.714253\n\n\n...\n...\n...\n...\n\n\n346\n-1.812768\n-0.085462\n0.222245\n\n\n347\n-2.047168\n0.064722\n0.374502\n\n\n348\n-2.009686\n0.007021\n0.345604\n\n\n349\n-1.878506\n-0.258263\n0.279973\n\n\n350\n-1.548783\n-0.187526\n0.217571\n\n\n\n\n351 rows × 3 columns\n\n\n\n\nimport collections\nprint(collections.Counter(cluster.labels_))\n\nCounter({0: 251, -1: 50, 1: 32, 2: 18})\n\n\n\n\n# clustering = DBSCAN().fit(project2D)\nimport seaborn as sns\nimport numpy as np\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import ListedColormap\n\n\n# axes instance\nfig = plt.figure(figsize=(6,6))\nax = Axes3D(fig, auto_add_to_figure=False)\nfig.add_axes(ax)\n\n# get colormap from seaborn\ncmap = plt.cm.get_cmap('viridis', 2) \nx = principalDf['PCA1']\ny = principalDf['PCA2']\nz = principalDf['PCA3']\n\n# plot\nax.scatter(x, y, z, s=40, c=np.array(cluster.labels_)&gt;-1, marker='o', cmap=cmap, alpha=1)\n\nax.set_xlabel('X Label')\nax.set_ylabel('Y Label')\nax.set_zlabel('Z Label')\n\nplt.show()\n# legend\n#plt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\n\nC:\\Users\\Anu2001\\AppData\\Local\\Temp\\ipykernel_2712\\188791831.py:16: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  cmap = plt.cm.get_cmap('viridis', 2)\n\n\n\n\n\n\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    bool_labels = dbscan.labels_&gt;-1\n    core_mask = np.zeros_like(bool_labels, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = bool_labels == 0\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    \n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n\n    # axes instance\n    fig = plt.figure(figsize=(6,6))\n    ax = Axes3D(fig, auto_add_to_figure=False)\n    fig.add_axes(ax)\n    ax.scatter(cores[:, 0], cores[:, 1], cores[:, 2],\n                c=bool_labels[core_mask], marker='o', s=size)\n    ax.scatter(cores[:, 0], cores[:, 1], cores[:, 2], marker='o', s=20,\n                c=bool_labels[core_mask])\n    ax.scatter(anomalies[:, 0], anomalies[:, 1], anomalies[:, 2],\n                c=\"r\", marker=\"x\", s=100)\n    ax.scatter(non_cores[:, 0], non_cores[:, 1], non_cores[:, 2],\n                c=bool_labels[non_core_mask], marker=\"o\")\n\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n\ndbscan = DBSCAN(eps=epsilon)\ndbscan.fit(principalDf)\n\nplot_dbscan(dbscan, principalDf.to_numpy(), size=100)\n\n\n\n\n\n\nprincipalDf['labels'] = cluster.labels_ &gt; -1\n\n\nsns.pairplot(data=principalDf, hue='labels')\n\nc:\\Users\\Anu2001\\anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py:123: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the blog page designed for the CS5805 Machine Learning class. This blog covers topics such as Probability Theory, Anomaly Detection, Clustering, Classification, and Regression. This blog is a work in progress. All the code files (ipynb) and datasets used in this blog can be found here!.\nThanks for reading!"
  },
  {
    "objectID": "codeFiles/Classification/Classification.html",
    "href": "codeFiles/Classification/Classification.html",
    "title": "CS5805",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndf_diabetes = pd.read_csv(\"diabetes.csv\")\n\n\ndf_diabetes.head(10)\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n5\n5\n116\n74\n0\n0\n25.6\n0.201\n30\n0\n\n\n6\n3\n78\n50\n32\n88\n31.0\n0.248\n26\n1\n\n\n7\n10\n115\n0\n0\n0\n35.3\n0.134\n29\n0\n\n\n8\n2\n197\n70\n45\n543\n30.5\n0.158\n53\n1\n\n\n9\n8\n125\n96\n0\n0\n0.0\n0.232\n54\n1\n\n\n\n\n\n\n\n\nprint(df_diabetes.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\nNone\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nf, ax = plt.subplots(figsize=(10, 8))\n\ncorr = df_diabetes.corr()\nsns.heatmap(corr,\n    cmap=sns.diverging_palette(220, 10, as_cmap=True),\n    vmin=-1.0, vmax=1.0,\n    annot = True,\n    square=True, ax=ax)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# data with 0 value instead of null values\ndf_diabetes.drop(columns=['Pregnancies', 'Outcome']).isin([0, 0.0]).sum()\n\nGlucose                       5\nBloodPressure                35\nSkinThickness               227\nInsulin                     374\nBMI                          11\nDiabetesPedigreeFunction      0\nAge                           0\ndtype: int64\n\n\n\n# storing outcomes in dataframe y, and storing pregnancies in a separate list temporarily\n# instead of creating a copy of another dataframe\npregnancies = df_diabetes['Pregnancies']\ny = df_diabetes['Outcome']\ndf_diabetes = df_diabetes.drop(columns=['Pregnancies', 'Outcome'])\n# making the 0 missing values into Nan values for imputing\ndf_diabetes.replace(0, np.nan, inplace=True)\nprint(f\"Number of missing values = {np.isnan(df_diabetes.to_numpy()).sum()}\")\ndf_diabetes['Pregnancies'] = pregnancies\ncolumns = df_diabetes.columns\ndf_diabetes.head(5)\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\ny = y\nX = (df_diabetes).to_numpy()\n# 80-20 Train-Test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) \n\nscaling_x=StandardScaler()\nX_train=scaling_x.fit_transform(X_train)\nX_test_imputed=scaling_x.transform(X_test)\n\n\n# Imputing missing values using knn\n# knn imputation transform for the dataset\n\nfrom sklearn.impute import KNNImputer\n\n# print total missing\nprint('Missing: %d' % sum(np.isnan(X).flatten()))\n# define imputer\nimputer = KNNImputer(n_neighbors=5) # taking 5 neighbours\n# fit transform on the dataset for training and testing set\nX_train_imputed = imputer.fit_transform(X_train)\nX_test_imputed = imputer.transform(X_test)\n# print total missing\nX_trans = np.concatenate((X_train_imputed, X_test_imputed), axis=0)\nprint('Missing: %d' % sum(np.isnan(X_trans).flatten()))\n\nMissing: 652\nMissing: 0\n\n\n\ndf_diabetes_cleaned = pd.DataFrame(X_trans, columns = columns)\ndf_diabetes_cleaned.head(5)\n\n\n\n\n\n\n\n\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nPregnancies\n\n\n\n\n0\n0.478034\n-0.188555\n0.315640\n-0.421253\n0.708354\n-0.946901\n0.810205\n3.353608\n\n\n1\n-0.824024\n-0.648467\n0.695029\n-0.508571\n0.664997\n0.396130\n-0.695262\n-1.121017\n\n\n2\n-0.189688\n-0.188555\n-0.063750\n-0.615099\n-0.693521\n-0.793670\n-1.029810\n-0.822709\n\n\n3\n-0.523549\n-0.648467\n0.600182\n-0.342666\n-0.245499\n2.799765\n0.057471\n-0.524401\n\n\n4\n0.044015\n1.191181\n0.789876\n-0.047530\n0.433760\n-0.814702\n-0.360714\n-1.121017\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom pprint import pprint\nbest_preds = []\nmodel_names = []\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nmodel_names.append('Support Vector Machine')\n# Define the parameter grid\nparam_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': [0.001, 0.01, 0.1, 1]}\n\n# Create an SVM model\nsvm_model = SVC()\n\n# print(\"Current params:\")\n# pprint(svm_model.get_params())\n\nsvm_model.fit(X_train_imputed, y_train)\n\n# Instantiate GridSearchCV with cross-validation\ngrid_search_svm = GridSearchCV(svm_model, param_grid, cv=3, scoring='accuracy')\n\n# Fit the model to the data and perform hyperparameter tuning\ngrid_search_svm.fit(X_train_imputed, y_train)\n\n# Print the best hyperparameters\n# print(\"Best Hyperparameters:\")\n# pprint(grid_search_svm.best_params_)\n\n# Get the best model\nbest_model_svm = grid_search_svm.best_estimator_\n\ny_pred = svm_model.predict(X_test_imputed)\ny_pred_best = best_model_svm.predict(X_test_imputed)\nprint(\"SVM without hyperparameter tuning\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\nprint(f\"F1 score: {f1_score(y_test, y_pred)}\")\n\nprint(\"SVM with hyperparameter tuning\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_best)}\")\nprint(f\"F1 score: {f1_score(y_test, y_pred_best)}\")    \n\nbest_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])\n\ncm = confusion_matrix(y_test, y_pred_best, labels=best_model_svm.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Diabetic', 'Diabetic'])\ndisp.plot()\nplt.show()\n\nSVM without hyperparameter tuning\nAccuracy: 0.7878787878787878\nF1 score: 0.6620689655172414\nSVM with hyperparameter tuning\nAccuracy: 0.7835497835497836\nF1 score: 0.6527777777777778\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nmodel_names.append('Random Forest')\n\nrf = RandomForestClassifier()\n# print(\"Current params:\")\n# pprint(rf.get_params())\n\nrf.fit(X_train_imputed, y_train)\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Create the random grid\nrandom_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': max_depth,\n               'min_samples_split': [2, 5, 10],\n               'min_samples_leaf': [1, 2, 4],\n               'bootstrap': [True, False]}\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train_imputed, y_train)\n\n# Print the best hyperparameters\n# print(\"Best Hyperparameters:\")\n# pprint(rf_random.best_params_)\n\n# Get the best model\nbest_model_rf = rf_random.best_estimator_\n\ny_pred = rf.predict(X_test_imputed)\ny_pred_best = best_model_rf.predict(X_test_imputed)\nprint(\"RF without hyperparameter tuning\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\nprint(f\"F1 score: {f1_score(y_test, y_pred)}\")\n\nprint(\"RF with hyperparameter tuning\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_best)}\")\nprint(f\"F1 score: {f1_score(y_test, y_pred_best)}\")    \n\nbest_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])\n\ncm = confusion_matrix(y_test, y_pred_best, labels=best_model_rf.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Diabetic', 'Diabetic'])\ndisp.plot()\nplt.show()\n\nFitting 3 folds for each of 100 candidates, totalling 300 fits\nRF without hyperparameter tuning\nAccuracy: 0.8095238095238095\nF1 score: 0.7027027027027026\nRF with hyperparameter tuning\nAccuracy: 0.8095238095238095\nF1 score: 0.7105263157894737\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel_names.append('Decision Tree')\n\ndt = DecisionTreeClassifier()\n\nprint(\"Current params:\")\npprint(dt.get_params())\n\ndt.fit(X_train_imputed, y_train)\n\nparams = {\n    'max_depth': [None, 5, 10, 15],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': range(1, 5),\n    'max_features': ['auto', 'sqrt', 'log2', None],\n    'criterion': ['gini', 'entropy'],\n}\n\ngrid_search_dt = GridSearchCV(dt, params, cv=3, scoring='accuracy')\n\n# Fit the model to the data and perform hyperparameter tuning\ngrid_search_dt.fit(X_train_imputed, y_train)\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\")\npprint(grid_search_dt.best_params_)\n\n# Get the best model\nbest_model_dt = grid_search_dt.best_estimator_\n\ny_pred = dt.predict(X_test_imputed)\ny_pred_best = best_model_dt.predict(X_test_imputed)\n\n\nprint(\"DT without hyperparameter tuning\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\nprint(f\"F1 score: {f1_score(y_test, y_pred)}\")\nprint()\nprint(\"DT with hyperparameter tuning\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_best)}\")\nprint(f\"F1 score: {f1_score(y_test, y_pred_best)}\")    \nprint()\n\nbest_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])\n\ncm = confusion_matrix(y_test, y_pred_best, labels=best_model_dt.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Diabetic', 'Diabetic'])\n\ndisp.plot()\nplt.show()\n\nDT without hyperparameter tuning\nAccuracy: 0.7489177489177489\nF1 score: 0.6741573033707865\nDT with hyperparameter tuning\nAccuracy: 0.696969696969697\nF1 score: 0.513888888888889\n\n\n\n\n\n\nfrom xgboost import XGBClassifier\nfrom skopt import BayesSearchCV\n\nmodel_names.append('XGBoost')\n\n# Create an XGBoost classifier\nxgb = XGBClassifier()\n\nprint(\"Current params:\")\npprint(xgb.get_params())\n\nxgb.fit(X_train_imputed, y_train)\n\n# Define the parameter search space\nparam_space = {\n    'max_depth': (3, 10),\n    'learning_rate': (0.01, 1.0, 'log-uniform'),\n    'n_estimators': (50, 200),\n    'min_child_weight': (1, 10),\n    'subsample': (0.1, 1.0, 'uniform'),\n    'gamma': (0.0, 1.0, 'uniform'),\n    'colsample_bytree': (0.1, 1.0, 'uniform'),\n}\n\n# Instantiate BayesSearchCV\nbayes_search_xgb = BayesSearchCV(\n    xgb,\n    param_space,\n    cv=3,  # Number of cross-validation folds\n)\n\nnp.int = np.int_\n# Fit the model to the training data and perform hyperparameter tuning\nbayes_search_xgb.fit(X_train_imputed, y_train)\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\")\npprint(bayes_search_xgb.best_params_)\n\n# Get the best model\nbest_model_xgb = bayes_search_xgb.best_estimator_\n\n\ny_pred = xgb.predict(X_test_imputed)\ny_pred_best = best_model_xgb.predict(X_test_imputed)\nprint(\"XGB without hyperparameter tuning\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\nprint(f\"F1 score: {f1_score(y_test, y_pred)}\")\nprint()\nprint(\"XGB with hyperparameter tuning\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_best)}\")\nprint(f\"F1 score: {f1_score(y_test, y_pred_best)}\")    \nprint()\n\nbest_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])\n\ncm = confusion_matrix(y_test, y_pred_best, labels=best_model_xgb.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Diabetic', 'Diabetic'])\ndisp.plot()\nplt.show()\n\nXGB without hyperparameter tuning\nAccuracy: 0.7316017316017316\nF1 score: 0.6219512195121951\nXGB with hyperparameter tuning\nAccuracy: 0.8138528138528138\nF1 score: 0.7295597484276729\n\n\n\n\n\n\n# tabulate their classification report\nevaluation_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\nplt.rcParams[\"figure.figsize\"] = [30, 7]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, axs = plt.subplots(1, 1)\naxs.axis('tight')\naxs.axis('off')\n\ntable1 = axs.table(cellText=best_preds,\n                      cellLoc = 'left',\n                      rowLabels = model_names,\n                      rowColours= [\"palegreen\"] * 10,\n                      colLabels=evaluation_metrics,\n                      colColours= [\"palegreen\"] * 10,\n                      loc='center')\n\n# Highlight cells with minimum value in each column\nfor col_idx, metric in enumerate(evaluation_metrics):\n    col_values = [row[col_idx] for row in best_preds]\n    max_value_idx = col_values.index(max(col_values))\n\n    # Highlight the cell with maximum value in coral color\n    table1[max_value_idx + 1, col_idx].set_facecolor(\"coral\")\n        \ntable1.auto_set_font_size(False)\ntable1.set_fontsize(14)\ntable1.scale(1, 4)\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "codeFiles/Probability_Theory/Probability_Theory.html",
    "href": "codeFiles/Probability_Theory/Probability_Theory.html",
    "title": "CS5805",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\n\nCoding Baum-Welch Algorithm from scratch\n\ndef forward(states, sequence, a, b, pi, key):\n    N = len(states)\n    T = len(sequence)\n    pi = pi[key] # prob of state i, since 2 states, let's half it be 0.5, 0.5 initially\n    i = key # holds the first state\n\n    # Pseudocount to handle zeros\n    pseudocount = 1e-100\n    # for all possible states, and the first actual state (alpha)\n    # i.e. alpha i for all i has been caluclated given yt\n    alpha = np.zeros((N, T))\n    alpha[:,0] = pi * b[:,int(sequence[0])] + pseudocount\n\n\n    # next, we have to do iterations to calculate alpha at different times t\n    # we need all alpha values since it is going to be summed up to calculate gamma\n    \n    for t in range(1, T):\n        for j in range(N):\n            alpha[j][t] = sum(alpha[i][t-1]*a[i][j]*b[j][int(sequence[t])] for i in range(N)) + pseudocount\n\n    return alpha\n\n\ndef backward(states, sequence, a, b):\n    N = len(states)\n    T = len(sequence)\n    beta = np.zeros((N, T))\n\n    # Pseudocount to handle zeros\n    pseudocount = 1e-100\n\n    # Initialization\n    beta[:, -1] = 1  # Set the last column to 1\n\n    # Recursion\n    for t in range(T - 2, -1, -1):\n        for i in range(N):\n            beta[i, t] = sum(a[i, j] * b[j, int(sequence[t + 1])] * beta[j, t + 1] for j in range(N)) + pseudocount\n\n    return beta\n\n\ndef train(a, b, pi, sequence, states, key, n_iterations = 100, tol=1e-6):\n    #Baum-Welch algorithm for HMM\n    # calculate gamma, xi, and then update a and b parameters\n    N = len(states)\n    T = len(sequence)\n    \n    # M is the number of possible observations i.e. number of columns\n    M = b.shape[1]\n\n    prev_log_likelihood = 0\n\n    for iteration in range(n_iterations):\n        alpha = forward(states, sequence, a, b, pi, key)\n        beta = backward(states, sequence, a, b)\n\n        print(f\"Alpha: {alpha}\")\n        print(f\"Beta:{beta}\")\n\n        # Pseudocount to handle zeros\n        pseudocount = 1e-100\n        gamma = alpha * beta\n        # print(gamma)\n        denominator = np.sum(gamma, axis=0, keepdims=True) # same for all i\n        gamma = gamma/denominator + pseudocount\n\n        print(f\"gamma:{gamma}\") \n\n        xi = np.zeros((N, N, T - 1))\n\n        for i in range(N):\n            for j in range(N):\n                for t in range(T - 1):\n                    numerator = alpha[i, t] * a[i, j] * b[j, int(sequence[t + 1])] * beta[j, t + 1]\n                    denominator = np.sum(alpha[k, t] * a[k, l] * b[l, int(sequence[t + 1])] * beta[l, t + 1] for k in range(N) for l in range(N))\n                    xi[i, j, t] = (numerator / denominator) + pseudocount\n\n        print(f\"Xi: {xi}\")\n\n\n        # update a and b\n        # M-step\n        '''\n        sequence == k creates a boolean array of the same length as sequence, where each element is True if the corresponding element in sequence is equal to k, and False otherwise.\n    mask = (sequence == k) assigns this boolean array to the variable mask.\n    In the context of the Baum-Welch algorithm or similar algorithms for Hidden Markov Models (HMMs), this kind of mask is often used to select specific observations in the computation of probabilities. For example, \n    it might be used to sum over only the observations that match a particular value, which is relevant when updating the emission matrix b.\n        '''\n        # a = (np.sum(xi, axis=2) + pseudocount)/ np.sum(gamma[:, :-1], axis=1, keepdims=True) \n        for i in range(N):  # N is the number of states\n            for j in range(N):  # N is the number of states\n                numerator = np.sum(xi[i, j, :])\n                denominator = np.sum(gamma[i, :])\n                a[i, j] = (numerator+pseudocount) / (denominator+pseudocount) \n\n\n        b = np.zeros((N, M))\n        # print(gamma.shape)\n        gamma_sum = np.sum(gamma, axis=1)\n        \n        obs = []\n        for i in sequence:\n            obs.append(int(i))\n        obs = np.array(obs)\n\n        for j in range(N):\n            for k in range(M):\n                mask = (obs==k) # for indicative function i.e. 1 if observed = yt, else 0\n                b[j, k] = (np.sum(gamma[j]*mask)+ pseudocount) / (np.sum(gamma[j]) + pseudocount) \n        \n\n        # Normalize rows to ensure each row sums to 1.0\n        a = a / np.sum(a, axis=1)[:, np.newaxis]\n        b = b / np.sum(b, axis=1)[:, np.newaxis]\n\n        print(f\"a = {a}, b = {b}\")\n\n        # Log Likelihood Calculation\n        log_likelihood = np.sum(np.log(np.sum(alpha, axis=0)))\n\n        # Convergence Check\n        if np.abs(log_likelihood - prev_log_likelihood) &lt; tol:\n            print(f\"Converged after {iteration + 1} iterations.\")\n            break\n\n        prev_log_likelihood = log_likelihood\n\n    return a, b, pi\n\n\ndef predict(sequence, states, a, b, pi):\n    # Makes use of the viterbi algorithm to predict best path\n    # Initialize Variables\n    T = len(sequence)\n    N = len(states)\n\n    # Pseudocount to handle zeros\n    pseudocount = 1e-100\n\n    viterbi_table = np.zeros((N, T)) # delta\n    backpointer = np.zeros((N, T)) # psi\n\n    # Initialization step, for t = 0\n    print(int(sequence[0]))\n    viterbi_table[:, 0] = pi * b[:, int(sequence[0])] + pseudocount\n\n    # Calculate Probabilities\n    for t in range(1, T):\n        for s in range(N):\n            \n            max_prob = max(viterbi_table[prev_s][t-1] * a[prev_s][s] for prev_s in range(N)) * b[s][int(sequence[t])] \n            viterbi_table[s][t] = max_prob + pseudocount\n            backpointer[s][t] = np.argmax([viterbi_table[prev_s][t-1] * a[prev_s][s]for prev_s in range(N)])\n\n    #Traceback and Find Best Path\n    best_path = []\n    last_state = np.argmax(viterbi_table[:, -1])\n\n    best_path.append(last_state)\n    best_prob = 1.0\n    for t in range(T-2, -1, -1):\n        last_state = last_state = np.argmax(viterbi_table[:, t])\n        best_prob *= (viterbi_table[last_state, t] + pseudocount)\n        best_path.append(last_state) # i.e. add to start of list\n\n        \n    return best_path\n\nAPPLICATION: DISHONEST CASINO EXAMPLE\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom hmmlearn import hmm\n\n\n\ngen_model = hmm.CategoricalHMM(n_components=2, n_iter=100, init_params = 'se')\n\ngen_model.startprob_ = np.array([1.0, 0.0])\n\ngen_model.transmat_ = np.array([[0.95, 0.05],\n                                [0.1, 0.9]])\n\ngen_model.emissionprob_ = \\\n    np.array([[1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6],\n              [1 / 10, 1 / 10, 1 / 10, 1 / 10, 1 / 10, 1 / 2]])\n\n# simulate the loaded dice rolls\nrolls, gen_states = gen_model.sample(30000)\n\n\nimport matplotlib.pyplot as plt \n# Import the MarkovChain class from markovchain.py\nfrom markovchain import MarkovChain\n\nP = gen_model.transmat_\nmc = MarkovChain(P, ['Fair', 'Loaded'])\nprint(\"Transition Model:\")\nmc.draw()\n\n\ndata = {'Fair': gen_model.emissionprob_[0], 'Unfair': gen_model.emissionprob_[1]}\ndf_emission = pd.DataFrame(data, index=['1', '2', '3', '4', '5', '6'])\n\nprint(\"Emission Matrix:\")\ndf_emission.head()\n\nTransition Model:\nEmission Matrix:\n\n\n\n\n\n\n\n\n\n\n\n\nFair\nUnfair\n\n\n\n\n1\n0.166667\n0.1\n\n\n2\n0.166667\n0.1\n\n\n3\n0.166667\n0.1\n\n\n4\n0.166667\n0.1\n\n\n5\n0.166667\n0.1\n\n\n\n\n\n\n\n\npd.DataFrame({'Roll': rolls.flatten()[0:10]+1, 'Coin_State': gen_states.flatten()[0:10]}).head()\n\n\n\n\n\n\n\n\nRoll\nCoin_State\n\n\n\n\n0\n6\n0\n\n\n1\n1\n0\n\n\n2\n2\n0\n\n\n3\n6\n1\n\n\n4\n1\n1\n\n\n\n\n\n\n\n\n# plotting the states of the first 500 coin flips\nfig, ax = plt.subplots()\nax.plot(gen_states[:500])\nax.set_title('States over time')\nax.set_xlabel('Time (# of rolls)')\nax.set_ylabel('State')\nfig.show()\n\nC:\\Users\\Anu2001\\AppData\\Local\\Temp\\ipykernel_10324\\3287461216.py:7: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\n\n# plot rolls for the fair and loaded states\nfig, ax = plt.subplots()\nax.hist(rolls[gen_states == 0], label='fair', alpha=0.5,\n        bins=np.arange(7) - 0.5, density=True)\nax.hist(rolls[gen_states == 1], label='loaded', alpha=0.5,\n        bins=np.arange(7) - 0.5, density=True)\nax.set_title('Roll probabilities by state')\nax.set_xlabel('Count')\nax.set_ylabel('Roll')\nax.legend()\nfig.show()\n\nC:\\Users\\Anu2001\\AppData\\Local\\Temp\\ipykernel_10324\\2992798780.py:11: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\n\n\n# split our data into training and validation sets (50/50 split)\nX_train = rolls[:rolls.shape[0] // 2]\nX_test = rolls[rolls.shape[0] // 2:]\ny_test = np.array(gen_states[gen_states.shape[0] // 2:])\ngen_model = gen_model.fit(X_train)\n\n# check base score (non-tuned model)\ngen_score = gen_model.score(X_test)\n\nprint(f'Generated score: {gen_score}')\n\n\n# use the Viterbi algorithm to predict the most likely sequence of states\n# given the model\n# states = best_model.predict(rolls)\n\nstates = gen_model.predict(X_test)\n# plot our recovered states compared to generated (aim 1)\nfig, ax = plt.subplots()\nax.plot(gen_states[:500], label='generated')\nax.plot(states[:500] + 1.5, label='recovered')\nax.set_yticks([])\nax.set_title('States compared to generated')\nax.set_xlabel('Time (# rolls)')\nax.set_xlabel('State')\nax.legend()\nfig.show()\n\n# %%\n# Let's check our learned transition probabilities and see if they match.\n\nprint(f'Transmission Matrix Generated:\\n{gen_model.transmat_.round(3)}\\n\\n')\n\n# %%\n# Finally, let's see if we can tell how the die is loaded.\n\nprint(f'Emission Matrix Generated:\\n{gen_model.emissionprob_.round(3)}\\n\\n')\n\nEven though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\nEven though the 'emissionprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'e'\nC:\\Users\\Anu2001\\AppData\\Local\\Temp\\ipykernel_10324\\2874475397.py:47: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\nGenerated score: -26140.512085616556\nBest score:      -26136.572745325924\nTransmission Matrix Generated:\n[[0.949 0.051]\n [0.135 0.865]]\n\nTransmission Matrix Recovered:\n[[0.903 0.097]\n [0.058 0.942]]\n\n\nEmission Matrix Generated:\n[[0.161 0.16  0.171 0.167 0.166 0.173]\n [0.095 0.111 0.085 0.083 0.093 0.533]]\n\nEmission Matrix Recovered:\n[[0.106 0.118 0.103 0.1   0.107 0.467]\n [0.165 0.164 0.175 0.171 0.17  0.155]]\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\nfrom sklearn.metrics import RocCurveDisplay\n\n# True states (hidden states)\ntrue_states = y_test\npredicted_states = states\n\n# Evaluate confusion matrix\nconf_matrix = confusion_matrix(true_states, predicted_states)\n# Display confusion matrix\nprint(\"Confusion Matrix:\")\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Fair', 'Loaded'])\ndisp.plot()\nplt.show()\n\n# Evaluate classification report\nclass_report = classification_report(true_states, predicted_states)\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(class_report)\n\nRocCurveDisplay.from_predictions(true_states, predicted_states)\n\nConfusion Matrix:\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.77      0.95      0.85      9900\n           1       0.83      0.45      0.59      5100\n\n    accuracy                           0.78     15000\n   macro avg       0.80      0.70      0.72     15000\nweighted avg       0.79      0.78      0.76     15000\n\n\n\n\n\n\n&lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x1404967d030&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS5805: Machine Learning",
    "section": "",
    "text": "Blog posts by: Anushka S\n\n\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection with DBSCAN\n\n\n\n\n\n\n\nClustering\n\n\nUnsupervised Learning\n\n\nMachine Learning\n\n\nDBSCAN\n\n\nAnomaly Detection\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nAnushka S\n\n\n\n\n\n\n  \n\n\n\n\nClassification: Predicting the onset of diabetes\n\n\n\n\n\n\n\nClassification\n\n\nMachine Learning\n\n\nSupervised Learning\n\n\nSVM\n\n\nRandom Forest\n\n\nDecision Tree\n\n\nXGBoost\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nAnushka S\n\n\n\n\n\n\n  \n\n\n\n\nClustering with K-Means\n\n\n\n\n\n\n\nClustering\n\n\nUnsupervised Learning\n\n\nMachine Learning\n\n\nK-Means\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nAnushka S\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory\n\n\n\n\n\n\n\nProbability Theory\n\n\nMachine Learning\n\n\nRandom Variables\n\n\nHidden Markov Models\n\n\nViterbi algorithm\n\n\nBaum Welch\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nAnushka S\n\n\n\n\n\n\n  \n\n\n\n\nRegression Analysis: Linear Regression and Non-Linear Regression\n\n\n\n\n\n\n\nRegression\n\n\nMachine Learning\n\n\nSupervised Learning\n\n\nSVM\n\n\nRandom Forest\n\n\nDecision Tree\n\n\nGradient Boost\n\n\nLinear\n\n\nNon-Linear\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nAnushka S\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Classification/Classification.html",
    "href": "posts/Classification/Classification.html",
    "title": "Classification: Predicting the onset of diabetes",
    "section": "",
    "text": "Supervised learning, also known as supervised machine learning, is a subcategory of machine learning and artificial intelligence. It is defined by its use of labeled datasets to train algorithms that to classify data or predict outcomes accurately.\nClassification uses machine learning algorithms that learn how to assign a class label to examples from the problem domain. The class labels are a set of discrete values. A model will use the training dataset and will calculate how to best map examples of input data to specific class labels. As such, the training dataset must be sufficiently representative of the problem and have many examples of each class label. Based on the set of class labels, classification can be binary classification (2 class labels) or multi-class classification (&gt;2 class labels). You can read more on classification here!.\nIn this blog, we will be dealing with binary classification on the Pima Indian Diabetes dataet from the UCI Machine Learning Repository. This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\nBelow, we can see a sample of the dataset chosen.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndf_diabetes = pd.read_csv(\"diabetes.csv\")\ndf_diabetes.head(10)\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n5\n5\n116\n74\n0\n0\n25.6\n0.201\n30\n0\n\n\n6\n3\n78\n50\n32\n88\n31.0\n0.248\n26\n1\n\n\n7\n10\n115\n0\n0\n0\n35.3\n0.134\n29\n0\n\n\n8\n2\n197\n70\n45\n543\n30.5\n0.158\n53\n1\n\n\n9\n8\n125\n96\n0\n0\n0.0\n0.232\n54\n1\n\n\n\n\n\n\n\nWe can plot the correlation between the features(columns) and in the chart below, we can see which features have a higher correlation with the tagret variable.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nf, ax = plt.subplots(figsize=(8, 8))\n\ncorr = df_diabetes.corr()\nsns.heatmap(corr,\n    cmap=sns.diverging_palette(220, 10, as_cmap=True),\n    vmin=-1.0, vmax=1.0,\n    annot = True,\n    square=True, ax=ax);\nplt.show()\n\n\n\n\n\nWe can view the info about the data. We see that there are no null values but there are columns having 0 values which are missing values. It is important to handle missing data and prepare it well before it is processed through the classification model.\n\n\nCode\nprint(df_diabetes.info())\nprint(df_diabetes.drop(columns=['Pregnancies', 'Outcome']).isin([0, 0.0]).sum())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\nNone\nGlucose                       5\nBloodPressure                35\nSkinThickness               227\nInsulin                     374\nBMI                          11\nDiabetesPedigreeFunction      0\nAge                           0\ndtype: int64\n\n\nWe convert the missing values into Nan values so that we can apply the K nearest neighbours imputation algorithm.\n\n\nCode\n# storing outcomes in dataframe y, and storing pregnancies in a separate list temporarily\n# instead of creating a copy of another dataframe\npregnancies = df_diabetes['Pregnancies']\ny = df_diabetes['Outcome']\ndf_diabetes = df_diabetes.drop(columns=['Pregnancies', 'Outcome'])\n# making the 0 missing values into Nan values for imputing\ndf_diabetes.replace(0, np.nan, inplace=True)\nprint(f\"Number of missing values = {np.isnan(df_diabetes.to_numpy()).sum()}\")\ndf_diabetes['Pregnancies'] = pregnancies\ncolumns = df_diabetes.columns\ndf_diabetes.head(5)\n\n\nNumber of missing values = 652\n\n\n\n\n\n\n\n\n\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nPregnancies\n\n\n\n\n0\n148.0\n72.0\n35.0\nNaN\n33.6\n0.627\n50\n6\n\n\n1\n85.0\n66.0\n29.0\nNaN\n26.6\n0.351\n31\n1\n\n\n2\n183.0\n64.0\nNaN\nNaN\n23.3\n0.672\n32\n8\n\n\n3\n89.0\n66.0\n23.0\n94.0\n28.1\n0.167\n21\n1\n\n\n4\n137.0\n40.0\n35.0\n168.0\n43.1\n2.288\n33\n0\n\n\n\n\n\n\n\nBefore imputing the data, we: 1. Split the data into train-test split 2. Scale the training data and the testing data separately.\nThe reason for splitting the data and then scaling it and then applying imputation is so that there is no data leakage between the train-test datasets. Since data leakage can make our model biased leading to incorrect results and inaccurate evaluation metric scores.\nThe training set and the test set are then imputed separately with the KNNImputer with 5 neighbours. Imputation for completing missing values using k-Nearest Neighbors. Each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\ny = y\nX = (df_diabetes).to_numpy()\n# 80-20 Train-Test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) \n\nscaling_x=StandardScaler()\nX_train=scaling_x.fit_transform(X_train)\nX_test=scaling_x.transform(X_test)\n\n# Imputing missing values using knn\n# knn imputation transform for the dataset\n\nfrom sklearn.impute import KNNImputer\n\n# print total missing\nprint('Missing: %d' % sum(np.isnan(X).flatten()))\n# define imputer\nimputer = KNNImputer(n_neighbors=5) # taking 5 neighbours\n# fit transform on the dataset for training and testing set\nX_train_imputed = imputer.fit_transform(X_train)\nX_test_imputed = imputer.transform(X_test)\n# print total missing\nX_trans = np.concatenate((X_train_imputed, X_test_imputed), axis=0)\nprint('Missing: %d' % sum(np.isnan(X_trans).flatten()))\n\nMissing: 652\nMissing: 0\n\n\nWe can see, all values have been normalized and there are no missing values in the dataset.\n\n\nCode\ndf_diabetes_cleaned = pd.DataFrame(X_trans, columns = columns)\ndf_diabetes_cleaned.head(5)\n\n\n\n\n\n\n\n\n\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nPregnancies\n\n\n\n\n0\n0.757757\n0.607781\n1.665478\n-0.227452\n0.831683\n0.529526\n0.567932\n1.516591\n\n\n1\n0.233976\n-0.808447\n0.698824\n0.513708\n1.313142\n-0.069689\n0.398450\n1.812018\n\n\n2\n-0.649904\n0.135705\n1.085486\n-0.433789\n0.729555\n-0.794249\n0.991638\n0.925736\n\n\n3\n-0.060651\n0.450422\n-0.905821\n1.088148\n-1.050384\n-0.167519\n2.601722\n1.221164\n\n\n4\n-0.060651\n0.293064\n0.795490\n-0.433789\n1.094297\n-0.760619\n-0.364222\n-0.551400\n\n\n\n\n\n\n\nWe can now begin classification and compare various popular classification models such as Support Vector Machines (SVM), Decision Trees (DT), Random Forest (RF), and XGBoost (XGB). We also explore some hyperparameter tuning parameters and compare all the models on their performance on this dataset. Hyperparameter tuning relies more on experimental results than theory, and thus the best method to determine the optimal settings is to try many different combinations evaluate the performance of each model.\nThe evaluation metrics are:\n\nAccuracy: Accuracy is a measure of overall correctness and is calculated as the ratio of correctly predicted instances to the total number of instances in the dataset. \\(Accuracy = \\frac{True Positive + True Negative}{True Positive + True Negative + False Positive + False Negative}\\)\nPrecision: Precision is the ratio of correctly predicted positive instances to the total predicted positive instances. It measures the accuracy of positive predictions. \\(Precision = \\frac{True Positive}{ True Positive + False Positive}\\)\nRecall: Recall is the ratio of correctly predicted positive instances to the total actual positive instances. It measures the model’s ability to capture all positive instances. \\(Recall (Sensitivity) = \\frac{True Positive}{ True Positive + False Negative}\\)\nF1 score: The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall, especially useful when the class distribution is imbalanced. \\(F1 = \\frac{2*Precision*Recall}{ Precision + Recall}\\)\n\nwhere TP and TN define the samples labeled correctly as the positive class or negative class, FP define the samples falsely labeled as positive and FN define the samples falsely labeled as negative.\n\n\nCode\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom pprint import pprint\nbest_preds = []\nmodel_names = []\n\n\nSupport Vector Machine Classification\nSupport Vector Machines (SVM) is a supervised machine learning algorithm used for classification and regression tasks. SVM works by finding a hyperplane in a high-dimensional space that best separates the data points of different classes. The “support vectors” are the data points closest to the decision boundary, and the margin is the distance between the support vectors and the decision boundary. SVM aims to maximize this margin, providing robust generalization to unseen data. SVM is considered a linear classifier but it can handle non-linear relationships through the use of kernel functions, allowing it to map input data into a higher-dimensional space. SVM is particularly effective in high-dimensional spaces and is widely used in various applications, including image classification and text categorization. In the code, we make use of the scikit-learn library for the SVM implementation. The parameter information can be found in the implementation page.\n\nfrom sklearn.svm import SVC\n\nmodel_names.append('Support Vector Machine')\n\n# Create an SVM model\nsvm_model = SVC()\n\nprint(\"Current params:\")\npprint(svm_model.get_params())\n\nsvm_model.fit(X_train_imputed, y_train)\n\ny_pred_best = svm_model.predict(X_test_imputed)\n\nbest_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])\n\nCurrent params:\n{'C': 1.0,\n 'break_ties': False,\n 'cache_size': 200,\n 'class_weight': None,\n 'coef0': 0.0,\n 'decision_function_shape': 'ovr',\n 'degree': 3,\n 'gamma': 'scale',\n 'kernel': 'rbf',\n 'max_iter': -1,\n 'probability': False,\n 'random_state': None,\n 'shrinking': True,\n 'tol': 0.001,\n 'verbose': False}\n\n\n\n\n\nSVM classifier\nAccuracy: 0.7987012987012987\nF1 score: 0.6666666666666666\n\n\n\n\n\n\n\nDecision Tree Classification\nDecision Trees are a non-linear, hierarchical model that partitions the input space into regions and assigns a class label or regression value to each region. The tree structure is built by recursively splitting the data based on the most informative features at each node. A common splitting technique is using the impurity measure to decide whether the branch must be split or not. Decision Trees are interpretable, easy to visualize, and capable of handling both categorical and numerical features. However, they are prone to overfitting, especially when deep trees are constructed. Techniques like pruning and limiting tree depth help mitigate overfitting and improve generalization. In the code, we make use of the scikit-learn library for the Decision Tree implementation. The parameter information can be found in the implementation page.\nGrid-Search Hyperparameter tuning\nGridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations of parameters we define. In the code, we make use of the scikit-learn library for the GridSearchCV implementation. The parameter information can be found in the implementation page.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel_names.append('Decision Tree')\n\ndt = DecisionTreeClassifier()\n\nprint(\"Current params:\")\npprint(dt.get_params())\n\ndt.fit(X_train_imputed, y_train)\n\nparams = {\n    'max_depth': [None, 5, 10, 15],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': range(1, 5),\n    'max_features': ['auto', 'sqrt', 'log2', None],\n    'criterion': ['gini', 'entropy'],\n}\n\ngrid_search_dt = GridSearchCV(dt, params, cv=3, scoring='accuracy')\n\n# Fit the model to the data and perform hyperparameter tuning\ngrid_search_dt.fit(X_train_imputed, y_train)\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\")\npprint(grid_search_dt.best_params_)\n\n# Get the best model\nbest_model_dt = grid_search_dt.best_estimator_\n\ny_pred = dt.predict(X_test_imputed)\ny_pred_best = best_model_dt.predict(X_test_imputed)\n\nbest_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])\n\nCurrent params:\n{'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'random_state': None,\n 'splitter': 'best'}\nBest Hyperparameters:\n{'criterion': 'gini',\n 'max_depth': 5,\n 'max_features': 'log2',\n 'min_samples_leaf': 4,\n 'min_samples_split': 5}\n\n\n\n\n\nDT without hyperparameter tuning\nAccuracy: 0.7012987012987013\nF1 score: 0.5740740740740741\n\nDT with hyperparameter tuning\nAccuracy: 0.7532467532467533\nF1 score: 0.5681818181818181\n\n\n\n\n\n\n\nRandom Forest Classification\nRandom Forest is an ensemble learning method that builds a multitude of decision trees during training and outputs the mode of the classes for classification tasks or the average prediction for regression tasks. Each tree in the forest is constructed using a random subset of the training data and a random subset of features. The randomness and diversity among trees help mitigate overfitting and improve overall model accuracy. Random Forest is known for its robustness, versatility, and effectiveness in handling high-dimensional data. In the code, we make use of the scikit-learn library for the Random Forest implementation. The parameter information can be found in the implementation page.\nRandomSearch Hyperparameter tuning\nRandomizedSearchCV, a method that, sample randomly from a distribution and evaluates the randomly chosen of parameters we define. In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter. In the code, we make use of the scikit-learn library for the RandomizedSearchCV implementation. The parameter information can be found in the implementation page.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nmodel_names.append('Random Forest')\n\nrf = RandomForestClassifier()\nprint(\"Current params:\")\npprint(rf.get_params())\n\nrf.fit(X_train_imputed, y_train)\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Create the random grid\nrandom_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': max_depth,\n               'min_samples_split': [2, 5, 10],\n               'min_samples_leaf': [1, 2, 4],\n               'bootstrap': [True, False]}\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train_imputed, y_train)\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\")\npprint(rf_random.best_params_)\n\n# Get the best model\nbest_model_rf = rf_random.best_estimator_\n\ny_pred = rf.predict(X_test_imputed)\ny_pred_best = best_model_rf.predict(X_test_imputed)\n\nbest_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])\n\nCurrent params:\n{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': 'sqrt',\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': None,\n 'verbose': 0,\n 'warm_start': False}\nFitting 3 folds for each of 100 candidates, totalling 300 fits\nBest Hyperparameters:\n{'bootstrap': False,\n 'max_depth': 110,\n 'max_features': 'sqrt',\n 'min_samples_leaf': 2,\n 'min_samples_split': 2,\n 'n_estimators': 600}\n\n\n\n\n\nRF without hyperparameter tuning\nAccuracy: 0.8051948051948052\nF1 score: 0.7058823529411765\n\nRF with hyperparameter tuning\nAccuracy: 0.8181818181818182\nF1 score: 0.7254901960784313\n\n\n\n\n\n\n\nXGBoost\nXGBoost, or Extreme Gradient Boosting, is a machine learning algorithm renowned for its efficiency and performance in predictive modeling tasks. It belongs to the ensemble learning family and is an extension of traditional gradient boosting methods. The core idea behind XGBoost is the sequential addition of weak learners, often decision trees, which are trained to correct errors made by preceding models. XGBoost introduces several key innovations, including regularization techniques to prevent overfitting, parallelized tree construction for faster training, and gradient-based optimization for rapid convergence. At its core, XGBoost structures its ensemble as a collection of decision trees, each contributing to the final prediction. So, gradient boosting is in the form of ensemble of weak prediction models. The algorithm assigns weights to the misclassified instances in each iteration, adjusting subsequent trees to focus on the previously misclassified samples. During training, XGBoost uses gradient-based optimization to efficiently navigate the solution space and arrive at an ensemble of trees that collectively delivers a robust and accurate prediction.\nIn the code, we make use of the XGBoost library for the XGBoost implementation. The parameter information can be found in the implementation page.\nBayesSearch Hyperparameter tuning\nBayesian optimization is a more sophisticated technique that uses Bayesian methods to model the underlying function that maps hyperparameters to the model performance. It tries to find the optimal set of hyperparameters by making smart guesses based on the previous results. Bayesian optimization is more efficient than grid or random search because it attempts to balance exploration and exploitation of the search space. It can also deal with the cases of large number of hyperparameters and large search space. However, it can be more difficult to implement than grid search or random search and may require more computational resources.\nIn the code, we make use of the skopt library for the BayesSearchCV implementation. The parameter information can be found in the implementation page.\n\nfrom xgboost import XGBClassifier\nfrom skopt import BayesSearchCV\n\nmodel_names.append('XGBoost')\n\n# Create an XGBoost classifier\nxgb = XGBClassifier()\n\nprint(\"Current params:\")\npprint(xgb.get_params())\n\nxgb.fit(X_train_imputed, y_train)\n\n# Define the parameter search space\nparam_space = {\n    'max_depth': (3, 10),\n    'learning_rate': (0.01, 1.0, 'log-uniform'),\n    'n_estimators': (50, 200),\n    'min_child_weight': (1, 10),\n    'subsample': (0.1, 1.0, 'uniform'),\n    'gamma': (0.0, 1.0, 'uniform'),\n    'colsample_bytree': (0.1, 1.0, 'uniform'),\n}\n\n# Instantiate BayesSearchCV\nbayes_search_xgb = BayesSearchCV(\n    xgb,\n    param_space,\n    cv=3,  # Number of cross-validation folds\n)\n\nnp.int = np.int_\n# Fit the model to the training data and perform hyperparameter tuning\nbayes_search_xgb.fit(X_train_imputed, y_train)\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\")\npprint(bayes_search_xgb.best_params_)\n\n# Get the best model\nbest_model_xgb = bayes_search_xgb.best_estimator_\n\n\ny_pred = xgb.predict(X_test_imputed)\ny_pred_best = best_model_xgb.predict(X_test_imputed)\n\nbest_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])\n\nCurrent params:\n{'base_score': None,\n 'booster': None,\n 'callbacks': None,\n 'colsample_bylevel': None,\n 'colsample_bynode': None,\n 'colsample_bytree': None,\n 'device': None,\n 'early_stopping_rounds': None,\n 'enable_categorical': False,\n 'eval_metric': None,\n 'feature_types': None,\n 'gamma': None,\n 'grow_policy': None,\n 'importance_type': None,\n 'interaction_constraints': None,\n 'learning_rate': None,\n 'max_bin': None,\n 'max_cat_threshold': None,\n 'max_cat_to_onehot': None,\n 'max_delta_step': None,\n 'max_depth': None,\n 'max_leaves': None,\n 'min_child_weight': None,\n 'missing': nan,\n 'monotone_constraints': None,\n 'multi_strategy': None,\n 'n_estimators': None,\n 'n_jobs': None,\n 'num_parallel_tree': None,\n 'objective': 'binary:logistic',\n 'random_state': None,\n 'reg_alpha': None,\n 'reg_lambda': None,\n 'sampling_method': None,\n 'scale_pos_weight': None,\n 'subsample': None,\n 'tree_method': None,\n 'validate_parameters': None,\n 'verbosity': None}\nBest Hyperparameters:\nOrderedDict([('colsample_bytree', 0.1),\n             ('gamma', 1.0),\n             ('learning_rate', 0.13052021185631646),\n             ('max_depth', 4),\n             ('min_child_weight', 10),\n             ('n_estimators', 66),\n             ('subsample', 1.0)])\n\n\n\n\n\nXGB without hyperparameter tuning\nAccuracy: 0.7727272727272727\nF1 score: 0.6728971962616823\n\nXGB with hyperparameter tuning\nAccuracy: 0.7922077922077922\nF1 score: 0.6862745098039216\n\n\n\n\n\n\n\nAnalyzing the results of all the chosen models, we get the table below:\n\n\nCode\n# tabulate their classification report\nevaluation_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\nplt.rcParams[\"figure.figsize\"] = [30, 7]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, axs = plt.subplots(1, 1)\naxs.axis('tight')\naxs.axis('off')\n\ntable1 = axs.table(cellText=best_preds,\n                      cellLoc = 'left',\n                      rowLabels = model_names,\n                      rowColours= [\"palegreen\"] * 10,\n                      colLabels=evaluation_metrics,\n                      colColours= [\"palegreen\"] * 10,\n                      loc='center')\n\n# Highlight cells with minimum value in each column\nfor col_idx, metric in enumerate(evaluation_metrics):\n    col_values = [row[col_idx] for row in best_preds]\n    max_value_idx = col_values.index(max(col_values))\n\n    # Highlight the cell with maximum value in coral color\n    table1[max_value_idx + 1, col_idx].set_facecolor(\"coral\")\n        \ntable1.auto_set_font_size(False)\ntable1.set_fontsize(14)\ntable1.scale(1, 4)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nThis blog only discusses a few classification algorithms and model tuning parameters. By applying the right model, tuning, and regularizing the model, we can aim to improve the accuracy of the model."
  },
  {
    "objectID": "posts/Probability/Probability.html",
    "href": "posts/Probability/Probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Hidden Markov Models (HMMs) are a powerful statistical tool used in various fields, including speech recognition, bioinformatics, and natural language processing. Grounded in probability theory, HMMs are a type of stochastic model that represents a system evolving over time with hidden states. The model assumes that the observed data result from a probabilistic process involving these hidden states, making it particularly effective in situations where the underlying dynamics are not directly observable but can be inferred through observed data and the probabilities governing state transitions and emissions. Probability theory forms the backbone of HMMs, allowing them to make predictions and decisions based on the likelihood of sequences of observations given the model’s parameters.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nThe hmmlearn library in Python performs Unsupervised learning and inference of Hidden Markov Models.\nLet us take the common Hidden Markov Model example of the ocassionally dishonest casino. In a casino, they use a fair die most of the time, but switch to the loaded die once in a while. The purpose of making use of the Hidden Markov Model is to identify instances when the dice roll is probabilistically from the fair die or the loaded die.\nThe probabilities for the various outcome variables have been adapted from this textbook.\n\n\n\nMarkov Model for dishonest casino\n\n\nThe emission probabilities are the probability of each outcome (die roll) at its current state. The transition probabilities define the probability of transitioning to a state (fair, loaded), and the start probabilities define the starting probability of being in each state.\nFrom the hmmlearn library, the CategoricalHMM model (derived from the MultinomialHMM model), uses the Baum-Welch algorithm for training hidden Markov models (HMMs). The Baum-Welch algorithm, also known as the Forward-Backward algorithm or the Expectation-Maximization (EM) algorithm for HMMs, is an iterative procedure for estimating the parameters of an HMM given a set of observed data.\nYou can read more on the training of the Hidden Markov Model and the probability theory behind identifying the sequence state part at the end of this blog.\nIn the block of code below, we initialize the probabilities, initialize the CategoricalHMM method which takes in n_components as the number of possible states, the number of iterations for training. The other parameters (such as init_param, algorithm, etc.) details can be found here.\nWe then use the sample() method to generate die roll and corresponding state samples.\n\nfrom hmmlearn import hmm\n\ngen_model = hmm.CategoricalHMM(n_components=2, n_iter=100, init_params = 'se')\n\ngen_model.startprob_ = np.array([1.0, 0.0])\n\ngen_model.transmat_ = np.array([[0.95, 0.05],\n                                [0.1, 0.9]])\n\ngen_model.emissionprob_ = \\\n    np.array([[1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6],\n              [1 / 10, 1 / 10, 1 / 10, 1 / 10, 1 / 10, 1 / 2]])\n\nrolls, gen_states = gen_model.sample(30000)\n\n\n\nTransition Model:\nEmission Matrix:\n\n\n\n\n\n\n\n\n\n\n\n\nFair\nUnfair\n\n\n\n\n1\n0.166667\n0.1\n\n\n2\n0.166667\n0.1\n\n\n3\n0.166667\n0.1\n\n\n4\n0.166667\n0.1\n\n\n5\n0.166667\n0.1\n\n\n\n\n\n\n\n\n\nSample of Dice Rolls generated\n\n\n\n\n\n\n\n\n\nRoll\nCoin_State\n\n\n\n\n0\n3\n0\n\n\n1\n2\n0\n\n\n2\n6\n0\n\n\n3\n6\n1\n\n\n4\n1\n0\n\n\n\n\n\n\n\nPlotting the states of the first 500 generated coin flips:\n\n\nCode\nfig, ax = plt.subplots()\nax.plot(gen_states[:500])\nax.set_title('States over time')\nax.set_xlabel('Time (# of rolls)')\nax.set_ylabel('State')\nfig.show()\n\n\n\n\n\nPlotting the rolls for the fair and loaded states\n\n\nCode\nfig, ax = plt.subplots()\nax.hist(rolls[gen_states == 0], label='fair', alpha=0.5,\n        bins=np.arange(7) - 0.5, density=True)\nax.hist(rolls[gen_states == 1], label='loaded', alpha=0.5,\n        bins=np.arange(7) - 0.5, density=True)\nax.set_title('Roll probabilities by state')\nax.set_xlabel('Count')\nax.set_ylabel('Roll')\nax.legend()\nfig.show()\n\n\n\n\n\nIn the code below, we are performing a 50%-50% train-test dataset split. We are then fitting the model on our train set and obtaining the score for the model which is simply the log probability under the model. Then, we make use of the predict method which implements the Viterbi algorithm to predict the best sequence of states for the given observations (dice rolls).\n\n# split our data into training and validation sets (50/50 split)\nX_train = rolls[:rolls.shape[0] // 2]\nX_test = rolls[rolls.shape[0] // 2:]\ny_test = np.array(gen_states[gen_states.shape[0] // 2:])\ngen_model = gen_model.fit(X_train)\n\n# check base score (non-tuned model)\ngen_score = gen_model.score(X_test)\n\nprint(f'Generated score: {gen_score}')\n\n# use the Viterbi algorithm to predict the most likely sequence of states\n# given the model\nstates = gen_model.predict(X_test)\n\nEven though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\nEven though the 'emissionprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'e'\n\n\nGenerated score: -26037.75132903366\n\n\nRecovered states vs Generated states:\n\n\nCode\nfig, ax = plt.subplots()\nax.plot(gen_states[:500], label='generated')\nax.plot(states[:500] + 1.5, label='recovered')\nax.set_yticks([])\nax.set_title('States compared to generated')\nax.set_xlabel('Time (# rolls)')\nax.set_xlabel('State')\nax.legend()\nfig.show()\n\n\n\n\n\nUpdated Markov Model probabilities after training the HMM on the dataset with the Baum-Welch algorithm.\n\n\nTransition Model:\nEmission Matrix:\n\n\n\n\n\n\n\n\n\n\n\n\nFair\nUnfair\n\n\n\n\n1\n0.111\n0.169\n\n\n2\n0.097\n0.168\n\n\n3\n0.102\n0.165\n\n\n4\n0.106\n0.167\n\n\n5\n0.112\n0.160\n\n\n\n\n\n\n\nResults of the model in the form of a confusion matrix to identify how many times the model predicted ‘Fair’ and ‘Loaded’ coin correctly given the dice roll.\nAs we can see from the results below, the accuracy of the model is not considered to be extremely good. This is because we are dealing with a truly probabilistic model, the results are based on the ‘likelihood’ parameter. Also, the model has been trained on sample data which may not mimic true data to the fullest.\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\nfrom sklearn.metrics import RocCurveDisplay\n\n# True states (hidden states)\ntrue_states = y_test\npredicted_states = states\n\n# Evaluate confusion matrix\nconf_matrix = confusion_matrix(true_states, predicted_states)\n# Display confusion matrix\nprint(\"Confusion Matrix:\")\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Fair', 'Loaded'])\ndisp.plot()\nplt.show()\n\n# Evaluate classification report\nclass_report = classification_report(true_states, predicted_states)\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(class_report)\n\nRocCurveDisplay.from_predictions(true_states, predicted_states)\n\n\nConfusion Matrix:\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.25      0.10      0.15      9847\n           1       0.20      0.42      0.27      5153\n\n    accuracy                           0.21     15000\n   macro avg       0.22      0.26      0.21     15000\nweighted avg       0.23      0.21      0.19     15000\n\n\n\n\n\n\n&lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x290e25c0ee0&gt;\n\n\n\n\n\n\n\nCode\ndef convert_to_numpy(string_val, type):\n    str_int = []\n    if type == \"rolls\":\n        for i in string_val:\n            str_int.append(np.array([int(i)-1]))\n    else:\n        dice = {'F':0, 'L':1}\n        for i in string_val:\n            str_int.append(np.array(dice[i]))\n    return str_int\n\ndef convert_to_string(string_die):\n    dice = {0: 'F', 1:'L'}\n    string = \"\"\n    for i in string_die:\n        string+=dice[i]\n    return string\n\n\nTesting the model on an example taken from the textbook: {width = 60%}\n\n\nCode\ntest_rolls1 = \"315116246446644245311321631164152133625144543631656626566666\"\n\ny_true1 = \"FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFLLLLLLLLLLLL\"\n\ntest_rolls2 = \"222555441666566563564324364131513465146353411126414626253356\"\ny_true2 = \"FFFFFFFFLLLLLLLLLLLLLFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFL\"\n\nX_test_1 = convert_to_numpy(test_rolls1, \"rolls\")\ny_test_1 = gen_model.predict(X_test_1)\nb = convert_to_string(y_test_1)\nprint(f\"Output:{test_rolls1} \\nDie:{y_true1} \\nViterbi:{b}\")\n\nX_test_2 = convert_to_numpy(test_rolls2, \"rolls\")\ny_test_2 = gen_model.predict(X_test_2)\nb = convert_to_string(y_test_2)\nprint(f\"Output:{test_rolls2} \\nDie:{y_true2} \\nViterbi:{b}\")\n\n\nOutput:315116246446644245311321631164152133625144543631656626566666 \nDie:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFLLLLLLLLLLLL \nViterbi:FFFFFFFFFFFFFLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLFFFFFFFFFFFFFFF\nOutput:222555441666566563564324364131513465146353411126414626253356 \nDie:FFFFFFFFLLLLLLLLLLLLLFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFL \nViterbi:FFFFFFFFFFFFFFFFFFFFLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL\n\n\nDetailing the probability theory behind the Hidden Markov Model The Baum-Welch algorithm, also known as the Forward-Backward algorithm, is a parameter estimation technique for Hidden Markov Models (HMMs). Named after Leonard Baum and Lloyd Welch, this algorithm is a form of the Expectation-Maximization (EM) algorithm. Its primary goal is to iteratively refine the parameters of an HMM based on observed data, making it a powerful tool for model training.\nThe algorithm consists of two main steps, the expectation step and the maximization step.\nThe parameters of a HMM are given by \\(\\theta=(A,B,\\pi)\\), where:\n\n\\(A\\) is the state transition matrix, which defines the probability of transitioning from one state to another.\n\\(B\\) is the emission matrix, which defines the probability of emitting a given observation from a given state.\n\\(\\pi\\) is the initial state distribution, which defines the probability of being in each state at the beginning of the sequence.\n\n\\(A=\\{a_{ij}\\}=P(X_{t}=j|X_{t-1}=i)\\) is the state transition matrix\n\\(\\pi=\\{\\pi_{i}\\}=P(X_{1}=i)\\) is the initial state distribution\n\\(B=\\{b_{j}(y_{t})\\}=P(Y_{t}=y_{t}|X_{t}=j)\\) is the emission matrix\nGiven observation sequences \\((Y=(Y_{1}=y_{1},Y_{2}=y_{2},...,Y_{T}=y_{T}))\\) the algorithm tries to find the parameters \\((\\theta)\\) that maximise the probability of the observation.\nThe algorithm starts by choosing some initial values for the HMM parameters \\(\\theta = (A, B, \\pi)\\). Then, it repeats the following steps until convergence:\n\nDetermine probable state paths. This involves calculating the probability of each possible state path, given the observed sequence of emissions.\nCount the expected number of transitions and emissions. This involves counting the number of times each state transition is taken and each emission is made, weighted by the probability of each state path.\nRe-estimate the HMM parameters. This involves using the expected number of transitions and emissions to update the HMM parameters \\(\\theta\\).\n\nThe forward-backward algorithm is used for finding probable paths.\nForward Procedure \\((\\alpha_{i}(t)=P(Y_{1}=y_{1},...,Y_{t}=y_{t},X_{t}=i|\\theta))\\) be the probability of seeing \\((y_{1},...,y_{t})\\) and being in state \\(i\\) at time \\(t\\). Found recursively using:\n\\(\\(\\alpha_{i}(1)=\\pi_{i}b_{i}(y_{1})\\)\\)\n\\(\\(\\alpha_{j}(t+1)=b_{j}(y_{t+1})\\sum_{i=1}^{N}\\alpha_{i}(t)a_{ij}\\)\\)\nBackward Procedure \\((\\beta_{i}(t)=P(Y_{t+1}=y_{t+1},...,Y_{T}=y_{T}|X_{t}=i,\\theta))\\) be the probability of ending partial sequence \\((y_{t+1},...,y_{T})\\) given starting state \\(i\\) at time \\(t\\).\n\\((\\beta_{i}(t))\\) is computed recursively as:\n\\(\\(\\beta_{i}(T)=1\\)\\) \\(\\(\\beta_{i}(t)=\\sum_{j=1}^{N}\\beta_{j}(t+1)a_{ij}b_{j}(y_{t+1}))\\)\n\n\nCode\ndef forward(states, sequence, a, b, pi, key):\n    N = len(states)\n    T = len(sequence)\n    pi = pi[key] # prob of state i, since 2 states, let's half it be 0.5, 0.5 initially\n    i = key # holds the first state\n\n    # Pseudocount to handle zeros\n    pseudocount = 1e-100\n    # for all possible states, and the first actual state (alpha)\n    # i.e. alpha i for all i has been caluclated given yt\n    alpha = np.zeros((N, T))\n    alpha[:,0] = pi * b[:,int(sequence[0])] + pseudocount\n\n\n    # next, we have to do iterations to calculate alpha at different times t\n    # we need all alpha values since it is going to be summed up to calculate gamma\n    \n    for t in range(1, T):\n        for j in range(N):\n            alpha[j][t] = sum(alpha[i][t-1]*a[i][j]*b[j][int(sequence[t])] for i in range(N)) + pseudocount\n\n    return alpha\n\n\n\n\nCode\ndef backward(states, sequence, a, b):\n    N = len(states)\n    T = len(sequence)\n    beta = np.zeros((N, T))\n\n    # Pseudocount to handle zeros\n    pseudocount = 1e-100\n\n    # Initialization\n    beta[:, -1] = 1  # Set the last column to 1\n\n    # Recursion\n    for t in range(T - 2, -1, -1):\n        for i in range(N):\n            beta[i, t] = sum(a[i, j] * b[j, int(sequence[t + 1])] * beta[j, t + 1] for j in range(N)) + pseudocount\n\n    return beta\n\n\nThe Expectation step: Calculate the probabilities of being in each state at each time step given the observed sequence using the Forward-Backward algorithm. These probabilities represent the likelihood of the system being in state \\(i\\) at time \\(t\\) given the entire observed sequence. Calculate the joint probabilities of transitioning from state \\(i\\) to state \\(j\\) at consecutive time steps given the observed sequence.\n\\(\\gamma_t(i) = \\frac{\\alpha_t(i) \\cdot \\beta_t(i)}{\\sum_{j=1}^{N} \\alpha_t(j) \\cdot \\beta_t(j)}\\)\n\\(\\xi_t(i, j) = \\frac{\\alpha_t(i) \\cdot a_{ij} \\cdot b_j(o_{t+1}) \\cdot \\beta_{t+1}(j)}{\\sum_{k=1}^{N} \\sum_{l=1}^{N} \\alpha_t(k) \\cdot a_{kl} \\cdot b_l(o_{t+1}) \\cdot \\beta_{t+1}(l)}\\)\nThe Maximization step: Update the model parameters, including the initial state probabilities, transition probabilities, and emission probabilities. The updated parameters are computed by normalizing the expected counts derived from the E-step.\n\\(\\hat{\\pi}_i = \\gamma_1(i)\\)\n\\(\\hat{a}_{ij} = \\frac{\\sum_{t=1}^{T-1} \\xi_t(i, j)}{\\sum_{t=1}^{T-1} \\gamma_t(i)}\\)\n\\(\\hat{b}_i(k) = \\frac{\\sum_{t=1}^{T} \\gamma_t(i) \\cdot \\delta_{o_t, k}}{\\sum_{t=1}^{T} \\gamma_t(i)}\\)\n\n\nCode\ndef train(a, b, pi, sequence, states, key, n_iterations = 100, tol=1e-6):\n    #Baum-Welch algorithm for HMM\n    # calculate gamma, xi, and then update a and b parameters\n    N = len(states)\n    T = len(sequence)\n    \n    # M is the number of possible observations i.e. number of columns\n    M = b.shape[1]\n\n    prev_log_likelihood = 0\n\n    for iteration in range(n_iterations):\n        alpha = forward(states, sequence, a, b, pi, key)\n        beta = backward(states, sequence, a, b)\n\n        print(f\"Alpha: {alpha}\")\n        print(f\"Beta:{beta}\")\n\n        # Pseudocount to handle zeros\n        pseudocount = 1e-100\n        gamma = alpha * beta\n        # print(gamma)\n        denominator = np.sum(gamma, axis=0, keepdims=True) # same for all i\n        gamma = gamma/denominator + pseudocount\n\n        print(f\"gamma:{gamma}\") \n\n        xi = np.zeros((N, N, T - 1))\n\n        for i in range(N):\n            for j in range(N):\n                for t in range(T - 1):\n                    numerator = alpha[i, t] * a[i, j] * b[j, int(sequence[t + 1])] * beta[j, t + 1]\n                    denominator = np.sum(alpha[k, t] * a[k, l] * b[l, int(sequence[t + 1])] * beta[l, t + 1] for k in range(N) for l in range(N))\n                    xi[i, j, t] = (numerator / denominator) + pseudocount\n\n        print(f\"Xi: {xi}\")\n\n\n        # update a and b\n        # M-step\n        '''\n        sequence == k creates a boolean array of the same length as sequence, where each element is True if the corresponding element in sequence is equal to k, and False otherwise.\n    mask = (sequence == k) assigns this boolean array to the variable mask.\n    In the context of the Baum-Welch algorithm or similar algorithms for Hidden Markov Models (HMMs), this kind of mask is often used to select specific observations in the computation of probabilities. For example, \n    it might be used to sum over only the observations that match a particular value, which is relevant when updating the emission matrix b.\n        '''\n        # a = (np.sum(xi, axis=2) + pseudocount)/ np.sum(gamma[:, :-1], axis=1, keepdims=True) \n        for i in range(N):  # N is the number of states\n            for j in range(N):  # N is the number of states\n                numerator = np.sum(xi[i, j, :])\n                denominator = np.sum(gamma[i, :])\n                a[i, j] = (numerator+pseudocount) / (denominator+pseudocount) \n\n\n        b = np.zeros((N, M))\n        # print(gamma.shape)\n        gamma_sum = np.sum(gamma, axis=1)\n        \n        obs = []\n        for i in sequence:\n            obs.append(int(i))\n        obs = np.array(obs)\n\n        for j in range(N):\n            for k in range(M):\n                mask = (obs==k) # for indicative function i.e. 1 if observed = yt, else 0\n                b[j, k] = (np.sum(gamma[j]*mask)+ pseudocount) / (np.sum(gamma[j]) + pseudocount) \n        \n\n        # Normalize rows to ensure each row sums to 1.0\n        a = a / np.sum(a, axis=1)[:, np.newaxis]\n        b = b / np.sum(b, axis=1)[:, np.newaxis]\n\n        print(f\"a = {a}, b = {b}\")\n\n        # Log Likelihood Calculation\n        log_likelihood = np.sum(np.log(np.sum(alpha, axis=0)))\n\n        # Convergence Check\n        if np.abs(log_likelihood - prev_log_likelihood) &lt; tol:\n            print(f\"Converged after {iteration + 1} iterations.\")\n            break\n\n        prev_log_likelihood = log_likelihood\n\n    return a, b, pi\n\n\nThe Viterbi algorithm is a dynamic programming algorithm used for decoding Hidden Markov Models (HMMs) and finding the most likely sequence of hidden states given an observed sequence. The algorithm efficiently determines the optimal state sequence by considering the probabilities of transitions and emissions.\nThe core idea behind the Viterbi algorithm is to iteratively compute the most likely path to each state at each time step, incorporating both the current observation and the previously calculated probabilities.\n\\[\nδ_i(t) = max_j δ_j(t - 1) a_ji b_i(Y_t)\n\\]\n\\[\nψ_i(t) = argmax_j δ_j(t - 1) a_ji\n\\]\n\n\nCode\ndef predict(sequence, states, a, b, pi):\n    # Makes use of the viterbi algorithm to predict best path\n    # Initialize Variables\n    T = len(sequence)\n    N = len(states)\n\n    # Pseudocount to handle zeros\n    pseudocount = 1e-100\n\n    viterbi_table = np.zeros((N, T)) # delta\n    backpointer = np.zeros((N, T)) # psi\n\n    # Initialization step, for t = 0\n    print(int(sequence[0]))\n    viterbi_table[:, 0] = pi * b[:, int(sequence[0])] + pseudocount\n\n    # Calculate Probabilities\n    for t in range(1, T):\n        for s in range(N):\n            \n            max_prob = max(viterbi_table[prev_s][t-1] * a[prev_s][s] for prev_s in range(N)) * b[s][int(sequence[t])] \n            viterbi_table[s][t] = max_prob + pseudocount\n            backpointer[s][t] = np.argmax([viterbi_table[prev_s][t-1] * a[prev_s][s]for prev_s in range(N)])\n\n    #Traceback and Find Best Path\n    best_path = []\n    last_state = np.argmax(viterbi_table[:, -1])\n\n    best_path.append(last_state)\n    best_prob = 1.0\n    for t in range(T-2, -1, -1):\n        last_state = last_state = np.argmax(viterbi_table[:, t])\n        best_prob *= (viterbi_table[last_state, t] + pseudocount)\n        best_path.append(last_state) # i.e. add to start of list\n\n        \n    return best_path\n\n\nHidden Markov Models are also used in many applications, one such interesting one is to identify cpgIslandsgiven a genomic sequence. You can read more about it here!."
  }
]