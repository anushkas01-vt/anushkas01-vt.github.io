{
  "hash": "3bd13d0ac26d50d4038fcfe305cda432",
  "result": {
    "markdown": "---\ntitle: \"Classification: Predicting the onset of diabetes\"\nauthor: \"Anushka S\"\ndate: \"2023-12-03\"\ncategories: [Classification, Machine Learning, Supervised Learning, SVM, Random Forest, Decision Tree, XGBoost]\n\n---\n\n[Supervised learning](https://www.ibm.com/topics/supervised-learning#:~:text=Supervised%20learning%2C%20also%20known%20as,data%20or%20predict%20outcomes%20accurately.), also known as supervised machine learning, is a subcategory of machine learning and artificial intelligence. It is defined by its use of labeled datasets to train algorithms that to classify data or predict outcomes accurately.\n\nClassification uses machine learning algorithms that learn how to assign a class label to examples from the problem domain. The class labels are a set of discrete values. A model will use the training dataset and will calculate how to best map examples of input data to specific class labels. As such, the training dataset must be sufficiently representative of the problem and have many examples of each class label. Based on the set of class labels, classification can be binary classification (2 class labels) or multi-class classification (>2 class labels). You can read more on classification [here!](https://machinelearningmastery.com/types-of-classification-in-machine-learning/).\n\nIn this blog, we will be dealing with binary classification on the Pima Indian Diabetes dataet from the [UCI Machine Learning Repository](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database). This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\nBelow, we can see a sample of the dataset chosen.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndf_diabetes = pd.read_csv(\"diabetes.csv\")\ndf_diabetes.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>116</td>\n      <td>74</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25.6</td>\n      <td>0.201</td>\n      <td>30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3</td>\n      <td>78</td>\n      <td>50</td>\n      <td>32</td>\n      <td>88</td>\n      <td>31.0</td>\n      <td>0.248</td>\n      <td>26</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>115</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>35.3</td>\n      <td>0.134</td>\n      <td>29</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>197</td>\n      <td>70</td>\n      <td>45</td>\n      <td>543</td>\n      <td>30.5</td>\n      <td>0.158</td>\n      <td>53</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8</td>\n      <td>125</td>\n      <td>96</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.232</td>\n      <td>54</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe can plot the correlation between the features(columns) and in the chart below, we can see which features have a higher correlation with the tagret variable.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nf, ax = plt.subplots(figsize=(8, 8))\n\ncorr = df_diabetes.corr()\nsns.heatmap(corr,\n    cmap=sns.diverging_palette(220, 10, as_cmap=True),\n    vmin=-1.0, vmax=1.0,\n    annot = True,\n    square=True, ax=ax);\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Classification_files/figure-html/cell-3-output-1.png){width=788 height=741}\n:::\n:::\n\n\nWe can view the info about the data. We see that there are no null values but there are columns having 0 values which are missing values. It is important to handle missing data and prepare it well before it is processed through the classification model.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nprint(df_diabetes.info())\nprint(df_diabetes.drop(columns=['Pregnancies', 'Outcome']).isin([0, 0.0]).sum())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\nNone\nGlucose                       5\nBloodPressure                35\nSkinThickness               227\nInsulin                     374\nBMI                          11\nDiabetesPedigreeFunction      0\nAge                           0\ndtype: int64\n```\n:::\n:::\n\n\nWe convert the missing values into Nan values so that we can apply the K nearest neighbours imputation algorithm.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# storing outcomes in dataframe y, and storing pregnancies in a separate list temporarily\n# instead of creating a copy of another dataframe\npregnancies = df_diabetes['Pregnancies']\ny = df_diabetes['Outcome']\ndf_diabetes = df_diabetes.drop(columns=['Pregnancies', 'Outcome'])\n# making the 0 missing values into Nan values for imputing\ndf_diabetes.replace(0, np.nan, inplace=True)\nprint(f\"Number of missing values = {np.isnan(df_diabetes.to_numpy()).sum()}\")\ndf_diabetes['Pregnancies'] = pregnancies\ncolumns = df_diabetes.columns\ndf_diabetes.head(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of missing values = 652\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Pregnancies</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>148.0</td>\n      <td>72.0</td>\n      <td>35.0</td>\n      <td>NaN</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>85.0</td>\n      <td>66.0</td>\n      <td>29.0</td>\n      <td>NaN</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>183.0</td>\n      <td>64.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>89.0</td>\n      <td>66.0</td>\n      <td>23.0</td>\n      <td>94.0</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>137.0</td>\n      <td>40.0</td>\n      <td>35.0</td>\n      <td>168.0</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBefore imputing the data, we:\n1. Split the data into train-test split\n2. Scale the training data and the testing data separately.\n\nThe reason for splitting the data and then scaling it and then applying imputation is so that there is no data leakage between the train-test datasets. Since data leakage can make our model biased leading to incorrect results and inaccurate evaluation metric scores.\n\nThe training set and the test set are then imputed separately with the [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) with 5 neighbours.\nImputation for completing missing values using k-Nearest Neighbors. Each sampleâ€™s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\ny = y\nX = (df_diabetes).to_numpy()\n# 80-20 Train-Test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) \n\nscaling_x=StandardScaler()\nX_train=scaling_x.fit_transform(X_train)\nX_test=scaling_x.transform(X_test)\n\n# Imputing missing values using knn\n# knn imputation transform for the dataset\n\nfrom sklearn.impute import KNNImputer\n\n# print total missing\nprint('Missing: %d' % sum(np.isnan(X).flatten()))\n# define imputer\nimputer = KNNImputer(n_neighbors=5) # taking 5 neighbours\n# fit transform on the dataset for training and testing set\nX_train_imputed = imputer.fit_transform(X_train)\nX_test_imputed = imputer.transform(X_test)\n# print total missing\nX_trans = np.concatenate((X_train_imputed, X_test_imputed), axis=0)\nprint('Missing: %d' % sum(np.isnan(X_trans).flatten()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMissing: 652\nMissing: 0\n```\n:::\n:::\n\n\nWe can see, all values have been normalized and there are no missing values in the dataset.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndf_diabetes_cleaned = pd.DataFrame(X_trans, columns = columns)\ndf_diabetes_cleaned.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Pregnancies</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.757757</td>\n      <td>0.607781</td>\n      <td>1.665478</td>\n      <td>-0.227452</td>\n      <td>0.831683</td>\n      <td>0.529526</td>\n      <td>0.567932</td>\n      <td>1.516591</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.233976</td>\n      <td>-0.808447</td>\n      <td>0.698824</td>\n      <td>0.513708</td>\n      <td>1.313142</td>\n      <td>-0.069689</td>\n      <td>0.398450</td>\n      <td>1.812018</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.649904</td>\n      <td>0.135705</td>\n      <td>1.085486</td>\n      <td>-0.433789</td>\n      <td>0.729555</td>\n      <td>-0.794249</td>\n      <td>0.991638</td>\n      <td>0.925736</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.060651</td>\n      <td>0.450422</td>\n      <td>-0.905821</td>\n      <td>1.088148</td>\n      <td>-1.050384</td>\n      <td>-0.167519</td>\n      <td>2.601722</td>\n      <td>1.221164</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.060651</td>\n      <td>0.293064</td>\n      <td>0.795490</td>\n      <td>-0.433789</td>\n      <td>1.094297</td>\n      <td>-0.760619</td>\n      <td>-0.364222</td>\n      <td>-0.551400</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe can now begin classification and compare various popular classification models such as Support Vector Machines (SVM), Decision Trees (DT), Random Forest (RF), and XGBoost (XGB). We also explore some hyperparameter tuning parameters and compare all the models on their performance on this dataset. Hyperparameter tuning relies more on experimental results than theory, and thus the best method to determine the optimal settings is to try many different combinations evaluate the performance of each model. \n\nThe evaluation metrics are:\n\n\n1. Accuracy: Accuracy is a measure of overall correctness and is calculated as the ratio of correctly predicted instances to the total number of instances in the dataset.\n$Accuracy = \\frac{True Positive + True Negative}{True Positive + True Negative + False Positive + False Negative}$\n\n2. Precision: Precision is the ratio of correctly predicted positive instances to the total predicted positive instances. It measures the accuracy of positive predictions.\n$Precision = \\frac{True Positive}{ True Positive + False Positive}$\n\n3. Recall:  Recall is the ratio of correctly predicted positive instances to the total actual positive instances. It measures the model's ability to capture all positive instances.\n$Recall (Sensitivity) = \\frac{True Positive}{ True Positive + False Negative}$\n\n4. F1 score: The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall, especially useful when the class distribution is imbalanced.\n$F1 = \\frac{2*Precision*Recall}{ Precision + Recall}$\n\nwhere TP and TN define the samples labeled correctly as the positive class or negative class, FP define the samples falsely labeled as positive and FN define the samples falsely labeled as negative.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom pprint import pprint\nbest_preds = []\nmodel_names = []\n```\n:::\n\n\n**Support Vector Machine Classification**\n\nSupport Vector Machines (SVM) is a supervised machine learning algorithm used for classification and regression tasks. SVM works by finding a hyperplane in a high-dimensional space that best separates the data points of different classes. The \"support vectors\" are the data points closest to the decision boundary, and the margin is the distance between the support vectors and the decision boundary. SVM aims to maximize this margin, providing robust generalization to unseen data. SVM is considered a linear classifier but it can handle non-linear relationships through the use of kernel functions, allowing it to map input data into a higher-dimensional space. SVM is particularly effective in high-dimensional spaces and is widely used in various applications, including image classification and text categorization. \nIn the code, we make use of the [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) library for the SVM implementation. The parameter information can be found in the implementation page.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.svm import SVC\n\nmodel_names.append('Support Vector Machine')\n\n# Create an SVM model\nsvm_model = SVC()\n\nprint(\"Current params:\")\npprint(svm_model.get_params())\n\nsvm_model.fit(X_train_imputed, y_train)\n\ny_pred_best = svm_model.predict(X_test_imputed)\n\nbest_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCurrent params:\n{'C': 1.0,\n 'break_ties': False,\n 'cache_size': 200,\n 'class_weight': None,\n 'coef0': 0.0,\n 'decision_function_shape': 'ovr',\n 'degree': 3,\n 'gamma': 'scale',\n 'kernel': 'rbf',\n 'max_iter': -1,\n 'probability': False,\n 'random_state': None,\n 'shrinking': True,\n 'tol': 0.001,\n 'verbose': False}\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSVM classifier\nAccuracy: 0.7987012987012987\nF1 score: 0.6666666666666666\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](Classification_files/figure-html/cell-11-output-1.png){width=574 height=429}\n:::\n:::\n\n\n**Decision Tree Classification**\n\nDecision Trees are a non-linear, hierarchical model that partitions the input space into regions and assigns a class label or regression value to each region. The tree structure is built by recursively splitting the data based on the most informative features at each node. A common splitting technique is using the [impurity measure](https://medium.com/@viswatejaster/measure-of-impurity-62bda86d8760) to decide whether the branch must be split or not. Decision Trees are interpretable, easy to visualize, and capable of handling both categorical and numerical features. However, they are prone to overfitting, especially when deep trees are constructed. Techniques like pruning and limiting tree depth help mitigate overfitting and improve generalization.\nIn the code, we make use of the [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) library for the Decision Tree implementation. The parameter information can be found in the implementation page.\n\n*Grid-Search Hyperparameter tuning*\n\nGridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations of parameters we define.\nIn the code, we make use of the [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) library for the GridSearchCV implementation. The parameter information can be found in the implementation page.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel_names.append('Decision Tree')\n\ndt = DecisionTreeClassifier()\n\nprint(\"Current params:\")\npprint(dt.get_params())\n\ndt.fit(X_train_imputed, y_train)\n\nparams = {\n    'max_depth': [None, 5, 10, 15],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': range(1, 5),\n    'max_features': ['auto', 'sqrt', 'log2', None],\n    'criterion': ['gini', 'entropy'],\n}\n\ngrid_search_dt = GridSearchCV(dt, params, cv=3, scoring='accuracy')\n\n# Fit the model to the data and perform hyperparameter tuning\ngrid_search_dt.fit(X_train_imputed, y_train)\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\")\npprint(grid_search_dt.best_params_)\n\n# Get the best model\nbest_model_dt = grid_search_dt.best_estimator_\n\ny_pred = dt.predict(X_test_imputed)\ny_pred_best = best_model_dt.predict(X_test_imputed)\n\nbest_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCurrent params:\n{'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'random_state': None,\n 'splitter': 'best'}\nBest Hyperparameters:\n{'criterion': 'gini',\n 'max_depth': 5,\n 'max_features': 'sqrt',\n 'min_samples_leaf': 2,\n 'min_samples_split': 10}\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nDT without hyperparameter tuning\nAccuracy: 0.7207792207792207\nF1 score: 0.6055045871559633\n\nDT with hyperparameter tuning\nAccuracy: 0.7402597402597403\nF1 score: 0.6363636363636364\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![](Classification_files/figure-html/cell-14-output-1.png){width=574 height=429}\n:::\n:::\n\n\n**Random Forest Classification**\n\nRandom Forest is an ensemble learning method that builds a multitude of decision trees during training and outputs the mode of the classes for classification tasks or the average prediction for regression tasks. Each tree in the forest is constructed using a random subset of the training data and a random subset of features. The randomness and diversity among trees help mitigate overfitting and improve overall model accuracy. Random Forest is known for its robustness, versatility, and effectiveness in handling high-dimensional data.\nIn the code, we make use of the [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) library for the Random Forest implementation. The parameter information can be found in the implementation page.\n\n\n*RandomSearch Hyperparameter tuning*\n\nRandomizedSearchCV, a method that, sample randomly from a distribution and evaluates the randomly chosen of parameters we define. In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.\nIn the code, we make use of the [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) library for the RandomizedSearchCV implementation. The parameter information can be found in the implementation page.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nmodel_names.append('Random Forest')\n\nrf = RandomForestClassifier()\nprint(\"Current params:\")\npprint(rf.get_params())\n\nrf.fit(X_train_imputed, y_train)\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Create the random grid\nrandom_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': max_depth,\n               'min_samples_split': [2, 5, 10],\n               'min_samples_leaf': [1, 2, 4],\n               'bootstrap': [True, False]}\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train_imputed, y_train)\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\")\npprint(rf_random.best_params_)\n\n# Get the best model\nbest_model_rf = rf_random.best_estimator_\n\ny_pred = rf.predict(X_test_imputed)\ny_pred_best = best_model_rf.predict(X_test_imputed)\n\nbest_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCurrent params:\n{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': 'sqrt',\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': None,\n 'verbose': 0,\n 'warm_start': False}\nFitting 3 folds for each of 100 candidates, totalling 300 fits\nBest Hyperparameters:\n{'bootstrap': False,\n 'max_depth': 30,\n 'max_features': 'sqrt',\n 'min_samples_leaf': 2,\n 'min_samples_split': 10,\n 'n_estimators': 800}\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRF without hyperparameter tuning\nAccuracy: 0.8311688311688312\nF1 score: 0.7400000000000001\n\nRF with hyperparameter tuning\nAccuracy: 0.8051948051948052\nF1 score: 0.7058823529411765\n```\n:::\n:::\n\n\n::: {.cell execution_count=16}\n\n::: {.cell-output .cell-output-display}\n![](Classification_files/figure-html/cell-17-output-1.png){width=574 height=429}\n:::\n:::\n\n\n**XGBoost**\n\nXGBoost, or Extreme Gradient Boosting, is a machine learning algorithm renowned for its efficiency and performance in predictive modeling tasks. It belongs to the ensemble learning family and is an extension of traditional gradient boosting methods. The core idea behind XGBoost is the sequential addition of weak learners, often decision trees, which are trained to correct errors made by preceding models. XGBoost introduces several key innovations, including regularization techniques to prevent overfitting, parallelized tree construction for faster training, and gradient-based optimization for rapid convergence.\nAt its core, XGBoost structures its ensemble as a collection of decision trees, each contributing to the final prediction. So, gradient boosting is in the form of ensemble of weak prediction models. The algorithm assigns weights to the misclassified instances in each iteration, adjusting subsequent trees to focus on the previously misclassified samples. During training, XGBoost uses gradient-based optimization to efficiently navigate the solution space and arrive at an ensemble of trees that collectively delivers a robust and accurate prediction. \n\nIn the code, we make use of the [XGBoost](https://xgboost.readthedocs.io/en/stable/parameter.html) library for the XGBoost implementation. The parameter information can be found in the implementation page.\n\n\n*BayesSearch Hyperparameter tuning*\n\nBayesian optimization is a more sophisticated technique that uses Bayesian methods to model the underlying function that maps hyperparameters to the model performance. It tries to find the optimal set of hyperparameters by making smart guesses based on the previous results. Bayesian optimization is more efficient than grid or random search because it attempts to balance exploration and exploitation of the search space. It can also deal with the cases of large number of hyperparameters and large search space. However, it can be more difficult to implement than grid search or random search and may require more computational resources.\n\nIn the code, we make use of the [skopt](https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html) library for the BayesSearchCV implementation. The parameter information can be found in the implementation page.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code code-fold=\"false\"}\nfrom xgboost import XGBClassifier\nfrom skopt import BayesSearchCV\n\nmodel_names.append('XGBoost')\n\n# Create an XGBoost classifier\nxgb = XGBClassifier()\n\nprint(\"Current params:\")\npprint(xgb.get_params())\n\nxgb.fit(X_train_imputed, y_train)\n\n# Define the parameter search space\nparam_space = {\n    'max_depth': (3, 10),\n    'learning_rate': (0.01, 1.0, 'log-uniform'),\n    'n_estimators': (50, 200),\n    'min_child_weight': (1, 10),\n    'subsample': (0.1, 1.0, 'uniform'),\n    'gamma': (0.0, 1.0, 'uniform'),\n    'colsample_bytree': (0.1, 1.0, 'uniform'),\n}\n\n# Instantiate BayesSearchCV\nbayes_search_xgb = BayesSearchCV(\n    xgb,\n    param_space,\n    cv=3,  # Number of cross-validation folds\n)\n\nnp.int = np.int_\n# Fit the model to the training data and perform hyperparameter tuning\nbayes_search_xgb.fit(X_train_imputed, y_train)\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\")\npprint(bayes_search_xgb.best_params_)\n\n# Get the best model\nbest_model_xgb = bayes_search_xgb.best_estimator_\n\n\ny_pred = xgb.predict(X_test_imputed)\ny_pred_best = best_model_xgb.predict(X_test_imputed)\n\nbest_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCurrent params:\n{'base_score': None,\n 'booster': None,\n 'callbacks': None,\n 'colsample_bylevel': None,\n 'colsample_bynode': None,\n 'colsample_bytree': None,\n 'device': None,\n 'early_stopping_rounds': None,\n 'enable_categorical': False,\n 'eval_metric': None,\n 'feature_types': None,\n 'gamma': None,\n 'grow_policy': None,\n 'importance_type': None,\n 'interaction_constraints': None,\n 'learning_rate': None,\n 'max_bin': None,\n 'max_cat_threshold': None,\n 'max_cat_to_onehot': None,\n 'max_delta_step': None,\n 'max_depth': None,\n 'max_leaves': None,\n 'min_child_weight': None,\n 'missing': nan,\n 'monotone_constraints': None,\n 'multi_strategy': None,\n 'n_estimators': None,\n 'n_jobs': None,\n 'num_parallel_tree': None,\n 'objective': 'binary:logistic',\n 'random_state': None,\n 'reg_alpha': None,\n 'reg_lambda': None,\n 'sampling_method': None,\n 'scale_pos_weight': None,\n 'subsample': None,\n 'tree_method': None,\n 'validate_parameters': None,\n 'verbosity': None}\nBest Hyperparameters:\nOrderedDict([('colsample_bytree', 0.16330485291293845),\n             ('gamma', 0.5998228910473469),\n             ('learning_rate', 0.31016606360093674),\n             ('max_depth', 8),\n             ('min_child_weight', 2),\n             ('n_estimators', 87),\n             ('subsample', 0.9281642866051433)])\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nXGB without hyperparameter tuning\nAccuracy: 0.7727272727272727\nF1 score: 0.6728971962616823\n\nXGB with hyperparameter tuning\nAccuracy: 0.7272727272727273\nF1 score: 0.625\n```\n:::\n:::\n\n\n::: {.cell execution_count=19}\n\n::: {.cell-output .cell-output-display}\n![](Classification_files/figure-html/cell-20-output-1.png){width=574 height=429}\n:::\n:::\n\n\nAnalyzing the results of all the chosen models, we get the table below:\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# tabulate their classification report\nevaluation_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\nplt.rcParams[\"figure.figsize\"] = [30, 7]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, axs = plt.subplots(1, 1)\naxs.axis('tight')\naxs.axis('off')\n\ntable1 = axs.table(cellText=best_preds,\n                      cellLoc = 'left',\n                      rowLabels = model_names,\n                      rowColours= [\"palegreen\"] * 10,\n                      colLabels=evaluation_metrics,\n                      colColours= [\"palegreen\"] * 10,\n                      loc='center')\n\n# Highlight cells with minimum value in each column\nfor col_idx, metric in enumerate(evaluation_metrics):\n    col_values = [row[col_idx] for row in best_preds]\n    max_value_idx = col_values.index(max(col_values))\n\n    # Highlight the cell with maximum value in coral color\n    table1[max_value_idx + 1, col_idx].set_facecolor(\"coral\")\n        \ntable1.auto_set_font_size(False)\ntable1.set_fontsize(14)\ntable1.scale(1, 4)\nfig.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Classification_files/figure-html/cell-21-output-1.png){width=2870 height=662}\n:::\n:::\n\n\nThis blog only discusses a few classification algorithms and model tuning parameters. By applying the right model, tuning, and regularizing the model, we can aim to improve the accuracy of the model.\n\n",
    "supporting": [
      "Classification_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}