{
  "hash": "f24c6dc4c012b227e509b15cf7cf0ade",
  "result": {
    "markdown": "---\ntitle: \"Anomaly Detection with DBSCAN\"\nauthor: \"Anushka S\"\ndate: \"2023-12-03\"\ncategories: [Clustering, Unsupervised Learning, Machine Learning, DBSCAN, Anomaly Detection]\n---\n\n[Unsupervised Learning](https://cloud.google.com/discover/what-is-unsupervised-learning#:~:text=Unsupervised%20learning%20in%20artificial%20intelligence,any%20explicit%20guidance%20or%20instruction.) in artificial intelligence is a type of machine learning that learns from data without human supervision. Unlike supervised learning, unsupervised machine learning models are given unlabeled data and allowed to discover patterns and insights without any explicit guidance or instruction. \n\nClustering is an unsupervised machine learning technique. Clustering is the process of building groups of data points in a way that the data in the same group are more similar to one another than to those in other groups. Clustering algorithms include agglomerative clustering, Gaussian mixtures for clustering, K-Means clustering, hierarchial clustering, DBSCAN, and much more. \n\nAnomaly Detection is a use case of the clustering algorithm to identify noise, exceptions, or outliers in the data which deviate significantly from standard behaviors or patterns. Density based clustering algorithms are especially useful in anomaly detection. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is one such algorithm which is popularly used in anomaly detection.\n\nUnlike methods that rely on distance thresholds (like in some distance-based clustering), DBSCAN automatically detects outliers without requiring a predefined distance threshold. It adapts to the local density of the data, making it robust to variations in the density of clusters. DBSCAN can scale to large datasets well and can handle clusters of arbitrary shapes, making it suitable for datasets where outliers might be located in regions with irregular shapes or non-uniform density.\n\nIn this blog, we analyze the ionosphere dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/52/ionosphere) to identify the \"Bad\" radars from the dataset.\n\n\"This radar data was collected by a system in Goose Bay, Labrador.  This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts.  See the paper for more details.  The targets were free electrons in the ionosphere. \"Good\" radar returns are those showing evidence of some type of structure in the ionosphere.  \"Bad\" returns are those that do not; their signals pass through the ionosphere.\"\nAs can be seen from the tabe, the data has already been normalized.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom ucimlrepo import fetch_ucirepo \nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# fetch dataset \nionosphere = fetch_ucirepo(id=52) \n\n\n# data (as pandas dataframes) \nX = ionosphere.data.features \ny = ionosphere.data.targets \n\n# metadata \nprint(ionosphere.metadata) \n  \n# variable information \nprint(ionosphere.variables) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'uci_id': 52, 'name': 'Ionosphere', 'repository_url': 'https://archive.ics.uci.edu/dataset/52/ionosphere', 'data_url': 'https://archive.ics.uci.edu/static/public/52/data.csv', 'abstract': 'Classification of radar returns from the ionosphere', 'area': 'Physics and Chemistry', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 351, 'num_features': 34, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['Class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1989, 'last_updated': 'Sun Jan 01 1989', 'dataset_doi': '10.24432/C5W01B', 'creators': ['V. Sigillito', 'S. Wing', 'L. Hutton', 'K. Baker'], 'intro_paper': None, 'additional_info': {'summary': 'This radar data was collected by a system in Goose Bay, Labrador.  This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts.  See the paper for more details.  The targets were free electrons in the ionosphere. \"Good\" radar returns are those showing evidence of some type of structure in the ionosphere.  \"Bad\" returns are those that do not; their signals pass through the ionosphere.  \\r\\n\\r\\nReceived signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number.  There were 17 pulse numbers for the Goose Bay system.  Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '-- All 34 are continuous\\r\\n-- The 35th attribute is either \"good\" or \"bad\" according to the definition summarized above.  This is a binary classification task.\\r\\n', 'citation': None}}\n           name     role         type demographic description units  \\\n0    Attribute1  Feature   Continuous        None        None  None   \n1    Attribute2  Feature   Continuous        None        None  None   \n2    Attribute3  Feature   Continuous        None        None  None   \n3    Attribute4  Feature   Continuous        None        None  None   \n4    Attribute5  Feature   Continuous        None        None  None   \n5    Attribute6  Feature   Continuous        None        None  None   \n6    Attribute7  Feature   Continuous        None        None  None   \n7    Attribute8  Feature   Continuous        None        None  None   \n8    Attribute9  Feature   Continuous        None        None  None   \n9   Attribute10  Feature   Continuous        None        None  None   \n10  Attribute11  Feature   Continuous        None        None  None   \n11  Attribute12  Feature   Continuous        None        None  None   \n12  Attribute13  Feature   Continuous        None        None  None   \n13  Attribute14  Feature   Continuous        None        None  None   \n14  Attribute15  Feature   Continuous        None        None  None   \n15  Attribute16  Feature   Continuous        None        None  None   \n16  Attribute17  Feature   Continuous        None        None  None   \n17  Attribute18  Feature   Continuous        None        None  None   \n18  Attribute19  Feature   Continuous        None        None  None   \n19  Attribute20  Feature   Continuous        None        None  None   \n20  Attribute21  Feature   Continuous        None        None  None   \n21  Attribute22  Feature   Continuous        None        None  None   \n22  Attribute23  Feature   Continuous        None        None  None   \n23  Attribute24  Feature   Continuous        None        None  None   \n24  Attribute25  Feature   Continuous        None        None  None   \n25  Attribute26  Feature   Continuous        None        None  None   \n26  Attribute27  Feature   Continuous        None        None  None   \n27  Attribute28  Feature   Continuous        None        None  None   \n28  Attribute29  Feature   Continuous        None        None  None   \n29  Attribute30  Feature   Continuous        None        None  None   \n30  Attribute31  Feature   Continuous        None        None  None   \n31  Attribute32  Feature   Continuous        None        None  None   \n32  Attribute33  Feature   Continuous        None        None  None   \n33  Attribute34  Feature   Continuous        None        None  None   \n34        Class   Target  Categorical        None        None  None   \n\n   missing_values  \n0              no  \n1              no  \n2              no  \n3              no  \n4              no  \n5              no  \n6              no  \n7              no  \n8              no  \n9              no  \n10             no  \n11             no  \n12             no  \n13             no  \n14             no  \n15             no  \n16             no  \n17             no  \n18             no  \n19             no  \n20             no  \n21             no  \n22             no  \n23             no  \n24             no  \n25             no  \n26             no  \n27             no  \n28             no  \n29             no  \n30             no  \n31             no  \n32             no  \n33             no  \n34             no  \n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Attribute1</th>\n      <th>Attribute2</th>\n      <th>Attribute3</th>\n      <th>Attribute4</th>\n      <th>Attribute5</th>\n      <th>Attribute6</th>\n      <th>Attribute7</th>\n      <th>Attribute8</th>\n      <th>Attribute9</th>\n      <th>Attribute10</th>\n      <th>...</th>\n      <th>Attribute25</th>\n      <th>Attribute26</th>\n      <th>Attribute27</th>\n      <th>Attribute28</th>\n      <th>Attribute29</th>\n      <th>Attribute30</th>\n      <th>Attribute31</th>\n      <th>Attribute32</th>\n      <th>Attribute33</th>\n      <th>Attribute34</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0.99539</td>\n      <td>-0.05889</td>\n      <td>0.85243</td>\n      <td>0.02306</td>\n      <td>0.83398</td>\n      <td>-0.37708</td>\n      <td>1.00000</td>\n      <td>0.03760</td>\n      <td>...</td>\n      <td>0.56811</td>\n      <td>-0.51171</td>\n      <td>0.41078</td>\n      <td>-0.46168</td>\n      <td>0.21266</td>\n      <td>-0.34090</td>\n      <td>0.42267</td>\n      <td>-0.54487</td>\n      <td>0.18641</td>\n      <td>-0.45300</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.00000</td>\n      <td>-0.18829</td>\n      <td>0.93035</td>\n      <td>-0.36156</td>\n      <td>-0.10868</td>\n      <td>-0.93597</td>\n      <td>1.00000</td>\n      <td>-0.04549</td>\n      <td>...</td>\n      <td>-0.20332</td>\n      <td>-0.26569</td>\n      <td>-0.20468</td>\n      <td>-0.18401</td>\n      <td>-0.19040</td>\n      <td>-0.11593</td>\n      <td>-0.16626</td>\n      <td>-0.06288</td>\n      <td>-0.13738</td>\n      <td>-0.02447</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.00000</td>\n      <td>-0.03365</td>\n      <td>1.00000</td>\n      <td>0.00485</td>\n      <td>1.00000</td>\n      <td>-0.12062</td>\n      <td>0.88965</td>\n      <td>0.01198</td>\n      <td>...</td>\n      <td>0.57528</td>\n      <td>-0.40220</td>\n      <td>0.58984</td>\n      <td>-0.22145</td>\n      <td>0.43100</td>\n      <td>-0.17365</td>\n      <td>0.60436</td>\n      <td>-0.24180</td>\n      <td>0.56045</td>\n      <td>-0.38238</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.00000</td>\n      <td>-0.45161</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>0.71216</td>\n      <td>-1.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>1.00000</td>\n      <td>0.90695</td>\n      <td>0.51613</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>-0.20099</td>\n      <td>0.25682</td>\n      <td>1.00000</td>\n      <td>-0.32382</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1.00000</td>\n      <td>-0.02401</td>\n      <td>0.94140</td>\n      <td>0.06531</td>\n      <td>0.92106</td>\n      <td>-0.23255</td>\n      <td>0.77152</td>\n      <td>-0.16399</td>\n      <td>...</td>\n      <td>0.03286</td>\n      <td>-0.65158</td>\n      <td>0.13290</td>\n      <td>-0.53206</td>\n      <td>0.02431</td>\n      <td>-0.62197</td>\n      <td>-0.05707</td>\n      <td>-0.59573</td>\n      <td>-0.04608</td>\n      <td>-0.65697</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 34 columns</p>\n</div>\n```\n:::\n:::\n\n\nSince the DBSCAN algorithm maps density based on a distance metric, the greater the number of dimensions, the harder it becomes for the algorithm to map the data points accurately. By applying Principal Component Analysis (PCA), we can reduce the number of dimensions. PCA transforms high-dimensional data into a lower-dimensional representation by identifying and emphasizing the principal components using statistical methods. By retaining only the most informative components, PCA simplifies data while preserving essential patterns.\n\nBy plotting a scree plot, we map the variance explained which helps determine the dimensions the final dataset can be reduced to without losing too much information. The \"elbow\" of the plot is usually considered the optimum value.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#define PCA model to use\npca = PCA(n_components=len(X.columns))\n\n#fit PCA model to data\npca_fit = pca.fit(X)\n\n# scree plot\nPC_values = np.arange(pca.n_components_) + 1\nplt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Variance Explained')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Anomaly_files/figure-html/cell-4-output-1.png){width=597 height=449}\n:::\n:::\n\n\nWe can see from the above plot that we can reduce the 34 column dimension data by projecting it into a 3D space.\n\n\nThe DBSCAN algorithm is mainly based on a metric function (normally the Euclidean distance) and a radius, ${epsilon}$. Given a sample point, its boundary is checked for other samples. If it is surrounded by at least $m$ minimum points, it becomes a core point. If the number of points are less than $m$, the point is classified as a boundary point, and if there are no other data points around within ${epsilon}$ radius, it is considered a noise point.\n\n![DBSCAN working](https://miro.medium.com/max/627/1*yT96veo7Zb5QeswV7Vr7YQ.png){width=80%}\n\nIt is important to understand the optimum epsilon ${epsilon}$ value for the best model performance to ensure that it does not classify data points with slight deviations from the normal to be considered noise (very low ${epsilon}$) and so that it does not include data points that are noise to be normal (very large ${epsilon}$).\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.neighbors import NearestNeighbors\nnbrs = NearestNeighbors(n_neighbors=2, metric='cosine').fit(X)\ndistances, indices = nbrs.kneighbors(X)\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\n\n# Plot the k-distance graph\nplt.plot(distances)\nplt.title('k-distance plot')\nplt.xlabel('Data Point Index')\nplt.ylabel('Distance to k-th nearest neighbor')\n\n\n# Find the optimal epsilon (knee point)\nknee_point_index = np.argmax(np.diff(distances))  # Find the index with the maximum difference in distances\nepsilon = distances[knee_point_index]\nplt.axvline(x=knee_point_index, color='r', linestyle='--', label=f'Optimal Epsilon = {epsilon:.2f}')\nplt.legend()\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Anomaly_files/figure-html/cell-5-output-1.png){width=589 height=449}\n:::\n:::\n\n\nWe can identify the optimum epsilon value from the 'knee point' of this graph. You can read more about this [here!](https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf)\n\nIn the below code, we structure the data into 3D using PCA and initialize and fit the DBSCAN model on the transformed data with the optimum chosen epsilon value. The DBSCAN implementation is imported from the [scikit-learn library](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.cluster import DBSCAN\nimport collections\n\nprincipalComponents = PCA(n_components=3).fit_transform(X)\n\nprincipalDf = pd.DataFrame(data = principalComponents)\n# initialize DBSCAN and fit data\ncluster = DBSCAN(eps=epsilon).fit(principalDf)\n\nprincipalDf.columns=['PCA1','PCA2','PCA3']\nprincipalDf\n\nprint(collections.Counter(cluster.labels_))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCounter({0: 252, -1: 49, 1: 32, 2: 18})\n```\n:::\n:::\n\n\nAfter fitting the data, the data points have been assigned clusters as can be seen in the above output. The datapoints assigned to cluster '-1' are considered to be the outlier points.\n\nAs can be seen from the 3D plot below, the outliers are low-density points.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport seaborn as sns\nimport numpy as np\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import ListedColormap\n\n\n# axes instance\nfig = plt.figure(figsize=(6,6))\nax = Axes3D(fig, auto_add_to_figure=False)\nfig.add_axes(ax)\n\n# get colormap from seaborn\ncmap = plt.cm.get_cmap('viridis', 2) \nx = principalDf['PCA1']\ny = principalDf['PCA2']\nz = principalDf['PCA3']\n\n# plot\nax.scatter(x, y, z, s=40, c=np.array(cluster.labels_)>-1, marker='o', cmap=cmap, alpha=1)\n\nax.set_xlabel('X Label')\nax.set_ylabel('Y Label')\nax.set_zlabel('Z Label')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Anomaly_files/figure-html/cell-7-output-1.png){width=598 height=595}\n:::\n:::\n\n\nWe can also use the [sns pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to visualize the distribution plots capturing relationship between the datapoints in each dimension . We can also visualize the representation of the outliers vs normal datapoints.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nprincipalDf['labels'] = cluster.labels_ > -1\nsns.pairplot(data=principalDf, hue='labels')\n```\n\n::: {.cell-output .cell-output-display}\n![](Anomaly_files/figure-html/cell-8-output-1.png){width=798 height=709}\n:::\n:::\n\n\nAs we can see, DBSCAN has proven to be effective in separating outliers from the data and is effective in applications of cleaning datasets, fraud detection, outlier detection, etc.\n\n",
    "supporting": [
      "Anomaly_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}