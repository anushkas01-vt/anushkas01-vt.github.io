{
  "hash": "e5b9df7f54808d1d141e435188d817ca",
  "result": {
    "markdown": "---\ntitle: \"Regression Analysis: Linear Regression and Non-Linear Regression\"\nauthor: \"Anushka S\"\ndate: \"2023-12-03\"\ncategories: [Regression, Machine Learning, Supervised Learning, SVM, Random Forest, Decision Tree, Gradient Boost, Linear, Non-Linear]\n---\n\n[Supervised learning](https://www.ibm.com/topics/supervised-learning#:~:text=Supervised%20learning%2C%20also%20known%20as,data%20or%20predict%20outcomes%20accurately.), also known as supervised machine learning, is a subcategory of machine learning and artificial intelligence. It is defined by its use of labeled datasets to train algorithms that to classify data or predict outcomes accurately.\n\nMachine Learning Regression is a technique for investigating the relationship between independent variables or features and a dependent variable or outcome. Itâ€™s used as a method for predictive modelling in machine learning, in which an algorithm is used to predict continuous outcomes. You can read more about it [here!](https://www.seldon.io/machine-learning-regression-explained#:~:text=Machine%20Learning%20Regression%20is%20a,used%20to%20predict%20continuous%20outcomes.)\n\nIn this blog, we will discuss two types of regression problems, Linear Regression, and Non-Linear Regression. For each, we will compare a handful of machine learning models (linear and non-linear models) and present their results on evaluation metrics.\n\n**Linear Regression**\nThis form of analysis estimates the coefficients of the linear equation, involving one or more independent variables that best predict the value of the dependent variable. Linear regression fits a straight line or surface that minimizes the discrepancies between predicted and actual output values.\n\nWe'll make use of the [Seoul Bike Sharing dataset](https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand) which contains count of public bicycles rented per hour in the Seoul Bike Sharing System, with corresponding weather data and holiday information. \nThe dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information. A sample of the dataset can be seen below. \nThe aim is to predict the bike count required at each hour for the stable supply of rental bikes. \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndf_bike = pd.read_csv(\"SeoulBikeData.csv\")\ndf_bike.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Rented Bike Count</th>\n      <th>Hour</th>\n      <th>Temperature(C)</th>\n      <th>Humidity(%)</th>\n      <th>Wind speed (m/s)</th>\n      <th>Visibility (10m)</th>\n      <th>Dew point temperature(C)</th>\n      <th>Solar Radiation (MJ/m2)</th>\n      <th>Rainfall(mm)</th>\n      <th>Snowfall (cm)</th>\n      <th>Seasons</th>\n      <th>Holiday</th>\n      <th>Functioning Day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>01/12/2017</td>\n      <td>254</td>\n      <td>0</td>\n      <td>-5.2</td>\n      <td>37</td>\n      <td>2.2</td>\n      <td>2000</td>\n      <td>-17.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Winter</td>\n      <td>No Holiday</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>01/12/2017</td>\n      <td>204</td>\n      <td>1</td>\n      <td>-5.5</td>\n      <td>38</td>\n      <td>0.8</td>\n      <td>2000</td>\n      <td>-17.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Winter</td>\n      <td>No Holiday</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>01/12/2017</td>\n      <td>173</td>\n      <td>2</td>\n      <td>-6.0</td>\n      <td>39</td>\n      <td>1.0</td>\n      <td>2000</td>\n      <td>-17.7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Winter</td>\n      <td>No Holiday</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01/12/2017</td>\n      <td>107</td>\n      <td>3</td>\n      <td>-6.2</td>\n      <td>40</td>\n      <td>0.9</td>\n      <td>2000</td>\n      <td>-17.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Winter</td>\n      <td>No Holiday</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>01/12/2017</td>\n      <td>78</td>\n      <td>4</td>\n      <td>-6.0</td>\n      <td>36</td>\n      <td>2.3</td>\n      <td>2000</td>\n      <td>-18.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Winter</td>\n      <td>No Holiday</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nIt is important to check the dataset for any missing values before it is used for model training and testing.\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 14 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Date                      8760 non-null   object \n 1   Rented Bike Count         8760 non-null   int64  \n 2   Hour                      8760 non-null   int64  \n 3   Temperature(C)            8760 non-null   float64\n 4   Humidity(%)               8760 non-null   int64  \n 5   Wind speed (m/s)          8760 non-null   float64\n 6   Visibility (10m)          8760 non-null   int64  \n 7   Dew point temperature(C)  8760 non-null   float64\n 8   Solar Radiation (MJ/m2)   8760 non-null   float64\n 9   Rainfall(mm)              8760 non-null   float64\n 10  Snowfall (cm)             8760 non-null   float64\n 11  Seasons                   8760 non-null   object \n 12  Holiday                   8760 non-null   object \n 13  Functioning Day           8760 non-null   object \ndtypes: float64(6), int64(4), object(4)\nmemory usage: 958.2+ KB\nNone\nDate                        0\nRented Bike Count           0\nHour                        0\nTemperature(C)              0\nHumidity(%)                 0\nWind speed (m/s)            0\nVisibility (10m)            0\nDew point temperature(C)    0\nSolar Radiation (MJ/m2)     0\nRainfall(mm)                0\nSnowfall (cm)               0\nSeasons                     0\nHoliday                     0\nFunctioning Day             0\ndtype: int64\nDate                        0\nRented Bike Count           0\nHour                        0\nTemperature(C)              0\nHumidity(%)                 0\nWind speed (m/s)            0\nVisibility (10m)            0\nDew point temperature(C)    0\nSolar Radiation (MJ/m2)     0\nRainfall(mm)                0\nSnowfall (cm)               0\nSeasons                     0\nHoliday                     0\nFunctioning Day             0\ndtype: int64\n```\n:::\n:::\n\n\nThis dataset seems to have no missing values so we're good!\n\nLet's format the dataset to ease data processing down the line. Beginning with breaking down the 'Date' into 'Day', 'Month', and 'Year' columns in the dataset.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\n# Can break the date into date, month, year columns and convert them into integers (from strings) for the purpose of correlation map\ndays = [int((df_bike['Date'].iloc[i])[0:2]) for i in range(len(df_bike))]\nmonth = [int((df_bike['Date'].iloc[i])[3:5]) for i in range(len(df_bike))]\nyear = [int((df_bike['Date'].iloc[i])[6:]) for i in range(len(df_bike))]\ndf_bike['Day'], df_bike['Month'], df_bike['Year'] = days, month, year\n\ndf_bike.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Rented Bike Count</th>\n      <th>Hour</th>\n      <th>Temperature(C)</th>\n      <th>Humidity(%)</th>\n      <th>Wind speed (m/s)</th>\n      <th>Visibility (10m)</th>\n      <th>Dew point temperature(C)</th>\n      <th>Solar Radiation (MJ/m2)</th>\n      <th>Rainfall(mm)</th>\n      <th>Snowfall (cm)</th>\n      <th>Seasons</th>\n      <th>Holiday</th>\n      <th>Functioning Day</th>\n      <th>Day</th>\n      <th>Month</th>\n      <th>Year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>01/12/2017</td>\n      <td>254</td>\n      <td>0</td>\n      <td>-5.2</td>\n      <td>37</td>\n      <td>2.2</td>\n      <td>2000</td>\n      <td>-17.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Winter</td>\n      <td>No Holiday</td>\n      <td>Yes</td>\n      <td>1</td>\n      <td>12</td>\n      <td>2017</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>01/12/2017</td>\n      <td>204</td>\n      <td>1</td>\n      <td>-5.5</td>\n      <td>38</td>\n      <td>0.8</td>\n      <td>2000</td>\n      <td>-17.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Winter</td>\n      <td>No Holiday</td>\n      <td>Yes</td>\n      <td>1</td>\n      <td>12</td>\n      <td>2017</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>01/12/2017</td>\n      <td>173</td>\n      <td>2</td>\n      <td>-6.0</td>\n      <td>39</td>\n      <td>1.0</td>\n      <td>2000</td>\n      <td>-17.7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Winter</td>\n      <td>No Holiday</td>\n      <td>Yes</td>\n      <td>1</td>\n      <td>12</td>\n      <td>2017</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01/12/2017</td>\n      <td>107</td>\n      <td>3</td>\n      <td>-6.2</td>\n      <td>40</td>\n      <td>0.9</td>\n      <td>2000</td>\n      <td>-17.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Winter</td>\n      <td>No Holiday</td>\n      <td>Yes</td>\n      <td>1</td>\n      <td>12</td>\n      <td>2017</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>01/12/2017</td>\n      <td>78</td>\n      <td>4</td>\n      <td>-6.0</td>\n      <td>36</td>\n      <td>2.3</td>\n      <td>2000</td>\n      <td>-18.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Winter</td>\n      <td>No Holiday</td>\n      <td>Yes</td>\n      <td>1</td>\n      <td>12</td>\n      <td>2017</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNext, we convert string values such as the values in the 'Seasons', 'Functioning Day', and 'Holiday' columns. We are able to do this by mapping the discrete set of string values to a discrete set of integer values.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\ndf1_bike = df_bike.drop(columns = ['Date'])\n# map unique season to numbers, map holiday to binary, and functioning day to binary\nseasons = {}\nfor idx, i in enumerate(df_bike['Seasons'].drop_duplicates()):\n    seasons[i] = idx\nholiday = {\"No Holiday\": 0, \"Holiday\": 1}\nfunctioning = {\"Yes\": 0, \"No\": 1}\ndf1_bike.Holiday = [holiday[item] for item in df_bike.Holiday]\ndf1_bike.Seasons = [seasons[item] for item in df_bike.Seasons]\ndf1_bike['Functioning Day'] = [functioning[item] for item in df1_bike['Functioning Day'] ]\n\ndf1_bike.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rented Bike Count</th>\n      <th>Hour</th>\n      <th>Temperature(C)</th>\n      <th>Humidity(%)</th>\n      <th>Wind speed (m/s)</th>\n      <th>Visibility (10m)</th>\n      <th>Dew point temperature(C)</th>\n      <th>Solar Radiation (MJ/m2)</th>\n      <th>Rainfall(mm)</th>\n      <th>Snowfall (cm)</th>\n      <th>Seasons</th>\n      <th>Holiday</th>\n      <th>Functioning Day</th>\n      <th>Day</th>\n      <th>Month</th>\n      <th>Year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>254</td>\n      <td>0</td>\n      <td>-5.2</td>\n      <td>37</td>\n      <td>2.2</td>\n      <td>2000</td>\n      <td>-17.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>12</td>\n      <td>2017</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>204</td>\n      <td>1</td>\n      <td>-5.5</td>\n      <td>38</td>\n      <td>0.8</td>\n      <td>2000</td>\n      <td>-17.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>12</td>\n      <td>2017</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>173</td>\n      <td>2</td>\n      <td>-6.0</td>\n      <td>39</td>\n      <td>1.0</td>\n      <td>2000</td>\n      <td>-17.7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>12</td>\n      <td>2017</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nPlotting the correlation matrix to identify the relationship, and the strength of relationship between the features(variables) in the dataset and also understand how strongly they are correlated with the target variable which is the rented bike count.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = df1_bike.corr()\nsns.heatmap(corr,\n    cmap=sns.diverging_palette(220, 10, as_cmap=True),\n    vmin=-1.0, vmax=1.0,\n    square=True, ax=ax)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-6-output-2.png){width=912 height=800}\n:::\n:::\n\n\nBelow, we can plot visualizations to see how each feature (variable) effects the target: Rented Bike Count.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nplt.rcParams[\"figure.autolayout\"] = True\nfig, ax = plt.subplots(2, 2, figsize=(10, 8));\n# hour vs bike count\nsns.barplot(data=df1_bike,x='Hour',y='Rented Bike Count',ax=ax[0][0], palette='viridis');\nax[0][0].set(title='Count of Rented bikes acording to Hour');\n\n# Functioning vs bike count\nsns.barplot(data=df1_bike,x='Functioning Day',y='Rented Bike Count',ax=ax[0][1], palette='inferno');\nax[0][1].set(title='Count of Rented bikes acording to Functioning Day');\nax[0][1].set_xticklabels(['Yes', 'No'])\n\n# season vs bike count\nsns.barplot(data=df1_bike,x='Seasons', y='Rented Bike Count',ax=ax[1][0], palette='plasma');\nax[1][0].set(title='Count of Rented bikes acording to Seasons');\nax[1][0].set_xticklabels(['Winter', 'Spring', 'Summer', 'Autumn'])\n\n# month vs bike count\nsns.barplot(data=df1_bike,x='Month',y='Rented Bike Count',ax=ax[1][1], palette='cividis');\nax[1][1].set(title='Count of Rented bikes acording to Month ');\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-7-output-1.png){width=955 height=758}\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfig,ax=plt.subplots(figsize=(10,8))\nsns.pointplot(data=df1_bike,x='Hour',y='Rented Bike Count',hue='Seasons',ax=ax);\nax.set(title='Count of Rented bikes acording to seasons and hour of the day');\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-8-output-1.png){width=950 height=758}\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(10, 8));\n# temperature vs bike count\n# Convert temperature in groups of 5C and average the rented bike counts for that range (rounding to 5s)\ntemp_min, temp_max = round(min(df1_bike['Temperature(C)'])/5)*5, round(max(df1_bike['Temperature(C)'])/5)*5\ndict_temp = {}\nfor i in range(temp_min, temp_max, 5):\n    # Filter rows based on the temperature interval\n    filtered_df = df1_bike[(df1_bike['Temperature(C)'] >= i) & (df1_bike['Temperature(C)'] < i+5)]\n    dict_temp[i] = filtered_df['Rented Bike Count'].mean()\n# print(dict_temp)\n# print(temp_max, temp_min)\nsns.barplot(data=dict_temp,ax=ax, palette='plasma');\nax.set(title='Count of Rented bikes acording to Temperature');\n\n# plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-9-output-1.png){width=950 height=758}\n:::\n:::\n\n\nPrinting the regression plot (Dependent Features vs Target Varibale).\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-fold=\"false\"}\nfig,ax=plt.subplots(4, 4, figsize=(10,10)) # since we know there are 16 features\nfor idx, col in enumerate(df1_bike.columns):\n  sns.regplot(x=df1_bike[col],y=df1_bike['Rented Bike Count'],scatter_kws={\"color\": 'blue'}, line_kws={\"color\": \"black\"}, ax=ax[idx//4][idx%4])\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-10-output-1.png){width=950 height=950}\n:::\n:::\n\n\nTo train and evaluate the machine learning models we need to split the dataset appropriately into the training and testing datasets.\nHence, we perform an 80-20 train-test split here.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\ny = df1_bike['Rented Bike Count']\nX = (df1_bike.drop(columns = ['Rented Bike Count'])).to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n:::\n\n\nThe following are the models used for estimating the number of bikes rented given other data.\nThe code implementations for each can be found in the scikit-learn library (linked for each model), and the model paramters used are default parameters.\n1. *Linear regression model*\n[Linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) is a simple yet powerful algorithm that models the relationship between the input features and the target variable by fitting a linear equation to the observed data. The main algorithm involves finding the coefficients that minimize the sum of squared differences between the predicted and actual values. This is typically achieved using the Ordinary Least Squares (OLS) method, aiming to optimize the line's parameters to best represent the data points.\n\n2. *Support Vector Machine (Regressor)*\nIn regression tasks, [Support Vector Machines](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR) (SVM) aim to find the hyperplane that best represents the relationship between the input features and the target variable. The primary algorithm involves identifying the support vectors and determining the optimal hyperplane to maximize the margin while minimizing the error. SVM uses a loss function that penalizes deviations from the regression line, and the algorithm seeks to find the coefficients that minimize this loss.\n\n3. *Ridge Regression*\n[Ridge Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) is an extension of linear regression that introduces a regularization term to prevent overfitting. The main algorithm involves adding a penalty term to the linear regression objective function, which is proportional to the square of the L2 norm of the coefficients. This regularization term helps stabilize the model by shrinking the coefficients, particularly useful when dealing with multicollinearity.\n\n4. *Lasso Regression*\n[Lasso Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso), similar to Ridge Regression, introduces regularization to linear regression. The main algorithm incorporates a penalty term, but in this case, it is proportional to the absolute value of the L1 norm of the coefficients. Lasso regression is effective for feature selection as it tends to produce sparse coefficient vectors, driving some coefficients to exactly zero.\n\n5. *Gradient Boosting Regressor*\n[Gradient Boosting Regressor](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html) is an ensemble learning method that builds a series of decision trees sequentially. The main algorithm involves fitting a weak learner (usually a shallow decision tree) to the residuals of the previous trees. The predictions of individual trees are combined to improve overall accuracy. The algorithm minimizes a loss function by adjusting the weights of the weak learners.\n\n6. *Random Forest Regressor*\n[Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) is an ensemble learning method that constructs multiple decision trees during training. The main algorithm involves training each tree on a random subset of the training data and features. The predictions of individual trees are then averaged or aggregated to reduce overfitting and improve generalization. Random Forest leverages the diversity among trees for robust predictions.\n\n7. *Decision Tree Regressor*\n[Decision Tree Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) models the relationship between input features and the target variable by recursively splitting the data based on feature thresholds. The main algorithm involves selecting the best split at each node to minimize the variance of the target variable. Decision trees are constructed until a stopping criterion is met, creating a tree structure that facilitates predictive modeling.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn import linear_model\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n\nmodel_names = ['Linear Regression Model',\n 'Support Vector Machine (Regression)',\n 'Ridge Regression',\n  'Lasso Regression',\n 'Gradient Boosting Regression',\n  'Random Forest',\n  'Decision Tree']\nmodels = [LinearRegression(),\n    SVR(), \n    linear_model.Ridge(), \n    linear_model.Lasso(),\n    GradientBoostingRegressor(),\n    RandomForestRegressor(),\n    DecisionTreeRegressor()]\n\nevaluation_metrics = ['Mean Squared Error (MSE)',\n 'Root MSE (RMSE)',\n  'Mean Absolute Error',\n  'R2 Score', \n  'Explained Variance Score']\n```\n:::\n\n\nThe model is fit on the training data, and predicted for the testing data. Regression models are commonly evaluated on the following metrics:\n1. *Mean Squared Error (MSE)*: MSE calculates the average squared difference between the predicted and actual values, providing a measure of the model's precision.\n$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n\n2. *Root Mean Squared Error (RMSE)*: RMSE is the square root of MSE and represents the average magnitude of the residuals in the same units as the target variable.\n$ \\text{RMSE} = \\sqrt{\\text{MSE}}$\n\n3. *Mean Absolute Error (MAE)*: MAE calculates the average absolute difference between the predicted and actual values, providing a measure of the model's accuracy.\n$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n\n4. *R2 Score*: R2 Score, or the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n$ R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}} where \\text{SSR} is the sum of squared residuals, and \\text{SST} is the total sum of squares.$\n\n5. *Explained Variance Score*: The Explained Variance Score measures the proportion by which the model's variance is reduced compared to a simple mean baseline.\n$ \\text{Explained Variance} = 1 - \\frac{\\text{Var}(y - \\hat{y})}{\\text{Var}(y)} where where y is the actual values and \\hat{y} is the predicted values$\n\n::: {.cell execution_count=12}\n``` {.python .cell-code code-fold=\"false\"}\ny_preds = [] # list of model predictions\nmodel_scores = [] # list of model scores based on the evaluation metrics defined\nfor model in models:\n    reg = model\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    y_preds.append(y_pred)\n\n    mse = mean_squared_error(y_test.values, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_test.values, y_pred)\n    r2 = r2_score(y_test.values, y_pred)\n    evs = explained_variance_score(y_test.values, y_pred)\n\n    model_scores.append([mse, rmse, mae, r2, evs])\n```\n:::\n\n\nLet us visualize the outputs of the linear models 'Linear Regression Model','Support Vector Machine (Regression)',\n'Ridge Regression', and 'Lasso Regression'. and evaluate the results.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nplt.rcParams[\"figure.figsize\"] = [10,7]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, axs = plt.subplots(1, 1)\naxs.axis('tight')\naxs.axis('off')\n\ntable1 = axs.table(cellText=model_scores[0:4],\n                      cellLoc = 'left',\n                      rowLabels = model_names[0:4],\n                      rowColours= [\"palegreen\"] * 10,\n                      colLabels=evaluation_metrics,\n                      colColours= [\"palegreen\"] * 10,\n                      loc='center')\n\n# Highlight cells with minimum value in each column\nfor col_idx, metric in enumerate(evaluation_metrics):\n    col_values = [row[col_idx] for row in model_scores[0:4]]\n    min_value_idx = col_values.index(min(col_values))\n\n    # Highlight the cell with minimum value in coral color\n    table1[min_value_idx + 1, col_idx].set_facecolor(\"coral\")\n        \ntable1.auto_set_font_size(False)\ntable1.set_fontsize(14)\ntable1.scale(1, 4)\nfig.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-14-output-1.png){width=950 height=662}\n:::\n:::\n\n\nNow, let us compare the performance of the linear models against non linear models on Linear Regression.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nplt.rcParams[\"figure.figsize\"] = [10, 7]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, axs = plt.subplots(1, 1)\naxs.axis('tight')\naxs.axis('off')\ntable2 = axs.table(cellText=model_scores,\n                      cellLoc = 'left',\n                      rowLabels = model_names,\n                      rowColours= [\"palegreen\"] * 10,\n                      colLabels=evaluation_metrics,\n                      colColours= [\"palegreen\"] * 10,\n                      loc='center')\n\n# Highlight cells with minimum value in each column\nfor col_idx, metric in enumerate(evaluation_metrics):\n    col_values = [row[col_idx] for row in model_scores]\n    min_value_idx = col_values.index(min(col_values))\n\n    # Highlight the cell with minimum value in coral color\n    table2[min_value_idx + 1, col_idx].set_facecolor(\"coral\")\n        \ntable2.auto_set_font_size(False)\ntable2.set_fontsize(14)\ntable2.scale(1, 4)\nfig.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-15-output-1.png){width=950 height=662}\n:::\n:::\n\n\nAs we can see, non-linear models, due to their complex structure and abilty to map and analyze/learn from complex data, perform better on this task. However, it is not always that non-linear models are better than linear models since we must keep in mind the computational expense and efficiency of a model for a task, as well as the [bias-variance](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html) tradeoff which again, is dependent not only on the model but also on the dataset/application at hand.\n\nAnalysing the difference between the actual and predicted values for the regression task by each model on 3 randomly chosen data points.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# printing how far the predicted value is to the actual value for a random row in X\nimport random\nfig, ax = plt.subplots(figsize=(10, 5));\n\nlength = len(model_names)\n\nfor i in range(3):\n    idx = random.randint(0,len(y_test)-1)\n    plt.plot(range(length), [(y_test.values)[idx]]*length, label='True Value');\n    plt.scatter(range(length), [y_preds[q][idx] for q in range(length)], label='Predicted Values');\n    for j in range(length):\n        plt.plot([j, j], [(y_test.values)[idx], y_preds[j][idx]], color='gray', linestyle='--', linewidth=0.8)\n    plt.xticks(range(length), model_names)\n    plt.tight_layout()\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-16-output-1.png){width=943 height=469}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-16-output-2.png){width=940 height=661}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-16-output-3.png){width=938 height=661}\n:::\n:::\n\n\n**Non Linear Regression**\n[Nonlinear regression](https://www.mathworks.com/discovery/nonlinear-regression.html) is a statistical technique that helps describe nonlinear relationships in experimental data. Nonlinear regression models are generally assumed to be parametric, where the model is described as a nonlinear equation. Typically machine learning methods are used for non-parametric nonlinear regression.\nParametric nonlinear regression models the dependent variable (also called the response) as a function of a combination of nonlinear parameters and one or more independent variables (called predictors). The model can be univariate (single response variable) or multivariate (multiple response variables).The parameters can take the form of an exponential, trigonometric, power, or any other nonlinear function. \n\nThe non-linear [dataset](https://github.com/Lawrence-Krukrubo) models China's GDP value for each year from 1960-2014. A sample of the dataset and the dataset visualization can be seen below.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndf_gdp = pd.read_csv(\"China_GDP.csv\")\nprint(df_gdp.info())\nprint(df_gdp.isna().sum())\ndf_gdp.head(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 55 entries, 0 to 54\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Year    55 non-null     int64  \n 1   Value   55 non-null     float64\ndtypes: float64(1), int64(1)\nmemory usage: 1008.0 bytes\nNone\nYear     0\nValue    0\ndtype: int64\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1960</td>\n      <td>5.918412e+10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1961</td>\n      <td>4.955705e+10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1962</td>\n      <td>4.668518e+10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1963</td>\n      <td>5.009730e+10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1964</td>\n      <td>5.906225e+10</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code code-fold=\"false\"}\n# plot Year vs GDP_value\nsns.scatterplot(data=df_gdp, x = 'Value', y = 'Year');\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-18-output-1.png){width=950 height=662}\n:::\n:::\n\n\nWe try to apply a regression line plot for the data. We see that the regression line is not able to accurately capture a linear relationship due to the non-linear relationship between the variables.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code code-fold=\"false\"}\n# Regression line plot\nsns.regplot(x=df_gdp['Value'],y=df_gdp['Year'],scatter_kws={\"color\": 'blue'}, line_kws={\"color\": \"black\"})\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-19-output-1.png){width=950 height=662}\n:::\n:::\n\n\nSplitting the dataset into 80-20 training-testing set to train and evaluate the aforementioned models. \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\ny = df_gdp['Year']\nX = (df_gdp.drop(columns = ['Year'])).to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n```\n:::\n\n\nWe can plot the training set points as well as the true test set points and predicted test set points for each model (Linear and Non-Linear) to visualize model accuracy and performance. Again, we see that the linear models struggle to accurately predict the target value for a non-linear dataset.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nfig, ax = plt.subplots(2,2, figsize=(10, 10));\nfor idx, model in enumerate(models[0:4]):\n    reg = model\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    # Plot the data points for training set\n    ax[idx//2][idx%2].scatter(X_train, y_train, marker='o', color='black', label='Train');\n    # Plot the data points for testing set (true)\n    ax[idx//2][idx%2].scatter(X_test, y_test, color='purple', marker='o', label='True');\n    # Plot the data points for testing set (predicted)\n    ax[idx//2][idx%2].scatter(X_test, y_pred, color='blue', marker='o', label='Predicted');\n    ax[idx//2][idx%2].set_title(model_names[idx])\n    ax[idx//2][idx%2].set_xlabel(\"GDP\")\n    ax[idx//2][idx%2].set_xlabel(\"Year\")\n    ax[idx//2][idx%2].legend()\nplt.title(\"True vs Predicted Performance of Linear Regression Models\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-21-output-1.png){width=979 height=950}\n:::\n:::\n\n\nHowever, the complex, non-linear models are able to capture and analyze the non-linearity and predict the target variable value more accurately.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\ny_preds = [] # list of model predictions\nmodel_scores = [] # list of model scores based on the evaluation metrics defined\n\nfig, ax = plt.subplots(3, 1, figsize=(10, 10));\nfor idx, model in enumerate(models[4:]):\n    reg = model\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    # Plot the data points for training set\n    ax[idx].scatter(X_train, y_train, marker='o', color='black', label='Train');\n    # Plot the data points for testing set (true)\n    ax[idx].scatter(X_test, y_test, color='purple', marker='o', label='True');\n    # Plot the data points for testing set (predicted)\n    ax[idx].scatter(X_test, y_pred, color='blue', marker='o', label='Predicted');\n    ax[idx].set_title(model_names[4+idx])\n    ax[idx].set_xlabel(\"GDP\")\n    ax[idx].set_xlabel(\"Year\")\n    ax[idx].legend()\n    \n    y_preds.append(y_pred)\n\n    mse = mean_squared_error(y_test.values, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_test.values, y_pred)\n    r2 = r2_score(y_test.values, y_pred)\n    evs = explained_variance_score(y_test.values, y_pred)\n\n    model_scores.append([mse, rmse, mae, r2, evs])\n    \nplt.title(\"True vs Predicted Performance of Non-Linear Regression Models\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-22-output-1.png){width=950 height=950}\n:::\n:::\n\n\nWe chart the model performance for the non-linear models. The models used for non-linear regression are *Random Forest Regressor*, *Decision Tree Regressor*, and *Gradient Boost Regressor*.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nplt.rcParams[\"figure.figsize\"] = [10, 7]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, axs = plt.subplots(1, 1)\naxs.axis('tight')\naxs.axis('off')\n\ntable1 = axs.table(cellText=model_scores,\n                      cellLoc = 'left',\n                      rowLabels = model_names[4:],\n                      rowColours= [\"palegreen\"] * 10,\n                      colLabels=evaluation_metrics,\n                      colColours= [\"palegreen\"] * 10,\n                      loc='center')\n\n# Highlight cells with minimum value in each column\nfor col_idx, metric in enumerate(evaluation_metrics):\n    col_values = [row[col_idx] for row in model_scores]\n    min_value_idx = col_values.index(min(col_values))\n\n    # Highlight the cell with minimum value in coral color\n    table1[min_value_idx + 1, col_idx].set_facecolor(\"coral\")\n        \ntable1.auto_set_font_size(False)\ntable1.set_fontsize(14)\ntable1.scale(1, 4)\nfig.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-23-output-1.png){width=950 height=662}\n:::\n:::\n\n\nOnce again, we analyze the difference between the actual and predicted values for the regression task by each model on 3 randomly chosen data points.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\n# printing how far the predicted value is to the actual value for a random row in X\nimport random\nfig, ax = plt.subplots(figsize=(10, 5));\n\nlength = len(model_names[4:])\n\nfor i in range(3):\n    idx = random.randint(0,len(y_test)-1)\n    plt.plot(range(length), [(y_test.values)[idx]]*length, label='True Value');\n    plt.scatter(range(length), [y_preds[q][idx] for q in range(length)], label='Predicted Values');\n    for j in range(length):\n        plt.plot([j, j], [(y_test.values)[idx], y_preds[j][idx]], color='gray', linestyle='--', linewidth=0.8)\n    plt.xticks(range(length), model_names[4:])\n    plt.tight_layout()\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-24-output-1.png){width=940 height=469}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-24-output-2.png){width=940 height=661}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Regression_files/figure-html/cell-24-output-3.png){width=940 height=661}\n:::\n:::\n\n\nFrom this blog, we get a glimpse into the performance and approach to applying the appropriate based on the type of dataset at hand.\n\n",
    "supporting": [
      "Regression_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}