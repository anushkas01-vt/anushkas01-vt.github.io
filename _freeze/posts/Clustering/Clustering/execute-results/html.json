{
  "hash": "9b89509fb3ae08e27da400ffbd363148",
  "result": {
    "markdown": "---\ntitle: \"Clustering with K-Means\"\nauthor: \"Anushka S\"\ndate: \"2023-12-03\"\ncategories: [Clustering, Unsupervised Learning, Machine Learning, K-Means]\n---\n\nIn this blog, we explore the application of identifying the most dominant colours in an image using K-Means Clustering. \n\n\n[Unsupervised Learning](https://cloud.google.com/discover/what-is-unsupervised-learning#:~:text=Unsupervised%20learning%20in%20artificial%20intelligence,any%20explicit%20guidance%20or%20instruction.) in artificial intelligence is a type of machine learning that learns from data without human supervision. Unlike supervised learning, unsupervised machine learning models are given unlabeled data and allowed to discover patterns and insights without any explicit guidance or instruction. \n\nClustering is an unsupervised machine learning technique. Clustering is the process of building groups of data points in a way that the data in the same group are more similar to one another than to those in other groups.\n\nClustering algorithms include agglomerative clustering, Gaussian mixtures for clustering, K-Means clustering, hierarchial clustering, DBSCAN, and much more. For the application of extracting dominant colours from an image we make use of the K-Means algorithm (distance-based) over some of the other popular density based clustering algorithms like DBSCAN (popularly used for anomaly detection) Since the similarity between the colours can be simply represented by the location in the 3D space. Many distance-based clustering algorithms, including K-means, are computationally efficient and can handle large datasets, making them suitable for processing images with a large number of pixels. The efficiency of these algorithms allows for faster clustering and analysis of image data. \n\nThe K-means algorithms is one of the most popular clustering algorithms. It can cluster groups of data given the number of clusters to form. It begins by selecting n cluster centroids (randomly or using the kmeans++ initialization algorithm) and assigning the data points to each cluster based on its Euclidean Distance from the cluster centroid. A point is considered to be in a particular cluster if it is closer to that cluster's centroid than any other centroid. \nK-Means finds the best centroids by alternating between (1) assigning data points to clusters based on the current centroids (2) chosing centroids (points which are the center of a cluster) based on the current assignment of data points to clusters. It continues this process for a number of iterations while trying to minimize the root mean square error each iteration (till the centroids dont change) and improve clustering. \n\nA great example of this can be seen in the figure below taken from this [KMeans article](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html).\n\n![K-Means Algorithm Visualization Sample](K-Means-example.JPG){width=80%}\n\nComing to extracting the most dominant colours in an image, let's take the example of the following image.\n![Sample Beach Image](Beach.jpg){width=80%}\n\nWe begin by analyzing the image by extracting the RGB (Red, Green, Blue) values that together make up the colour of the pixel. By making use of the [cv2](https://opencv.org/) library from OpenCV, we can essentially \"read\" the image, i.e. get it's pixel value, and store the data in a [Pandas](https://pandas.pydata.org/docs/index.html#) dataframe for future use. An example of the RGB values for the sample image can be seen below.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport cv2\nimport numpy as np\n\nRGB_values = pd.DataFrame(columns=['R', 'G', 'B'])\nimage = cv2.imread(\"beach.jpg\")\n\n# get image shape\nnumPixels = image.shape\nprint(numPixels)\n\ny, x = numPixels[0], numPixels[1]\nfor i, j in zip(range(y), range(x)):\n    BGR_values = image[i, j]\n    RGB_values.loc[len(RGB_values)] = np.flip(BGR_values) # to get the (B, G, R) values in (R, G, B) format\n\nRGB_values.head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1200, 1920, 3)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>R</th>\n      <th>G</th>\n      <th>B</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>161</td>\n      <td>195</td>\n      <td>240</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>161</td>\n      <td>195</td>\n      <td>240</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>160</td>\n      <td>194</td>\n      <td>239</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>159</td>\n      <td>193</td>\n      <td>238</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>159</td>\n      <td>193</td>\n      <td>238</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>158</td>\n      <td>192</td>\n      <td>237</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>157</td>\n      <td>191</td>\n      <td>237</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>157</td>\n      <td>191</td>\n      <td>237</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>156</td>\n      <td>190</td>\n      <td>236</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>156</td>\n      <td>190</td>\n      <td>236</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nA lot of processing libraries, and machine learning algorithms require the data to be scaled or normalized. The colorsys library requires the RGB values to be normalized before being able to convert it into HSL values to generate the plot displaying the colours in the image in a list form! Additionally, since the K-Means clustering algorithm is a distance based algorithm, it is beneficial to rescale each feature dimension of the observation set by its standard deviation so that higher range of some features do not influence the algorithm by acting 'weighted'. Each feature is normalized across all observations to give it unit variance.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# plot the RGB values on a graph\nfrom mpl_toolkits import mplot3d\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport colorsys\n\n# normalize the RGB values\nRGB_values = RGB_values/255\n\nRGB_values.head(5)\n\nRGB_unique = RGB_values.drop_duplicates()\nRGB_unique = list(RGB_unique.to_numpy())\nRGB_unique.sort(key=lambda rgb: colorsys.rgb_to_hls(*rgb))\ncmap_RGB = matplotlib.colors.ListedColormap(RGB_unique, \"Colours in the image\")\ncmap_RGB\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div style=\"vertical-align: middle;\"><strong>Colours in the image</strong> </div><div class=\"cmap\"><img alt=\"Colours in the image colormap\" title=\"Colours in the image\" style=\"border: 1px solid #555;\" src=\"data:image/png;base64,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\"></div><div style=\"vertical-align: middle; max-width: 514px; display: flex; justify-content: space-between;\"><div style=\"float: left;\"><div title=\"#e7c3c3ff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #e7c3c3ff;\"></div> under</div><div style=\"margin: 0 auto; display: inline-block;\">bad <div title=\"#00000000\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #00000000;\"></div></div><div style=\"float: right;\">over <div title=\"#e9c5c7ff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #e9c5c7ff;\"></div></div>\n```\n:::\n:::\n\n\nWe can see below the (R, G, B) values plotted in a 3D space. This visualization is especially important to better analyze the K-Means clustering algorithm.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(111, projection='3d')\n# Data for three-dimensional scattered points\nzdata = RGB_values['B']\nxdata = RGB_values['R']\nydata = RGB_values['G']\nax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='hsv');\n\nax.set_xlabel('R')\nax.set_ylabel('G')\nax.set_zlabel('B')\nax.set_title('RGB value of input image');\n```\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-4-output-1.png){width=618 height=631}\n:::\n:::\n\n\nSince K-Means clustering requires a defined number of clusters, for accurate clustering, it is important to have an optimum number of clusters, not too less, not too many. We make use of the KElbowVisualizer to find the optimal value for the number of clusters. From the documentation: \"The KElbowVisualizer implements the “elbow” method to help data scientists select the optimal number of clusters by fitting the model with a range of values for \n. If the line chart resembles an arm, then the “elbow” (the point of inflection on the curve) is a good indication that the underlying model fits best at that point. In the visualizer “elbow” will be annotated with a dashed line. The elbow method runs k-means clustering on the dataset for a range of values for k (say from 1-10) and then for each value of k computes an average score for all clusters.\"  [Read More!](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html)\n\nWe can see from the graph that for our data, 4 is the optimum number of clusters. So our final result would be the 4 most dominant colours in the image.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nimport sklearn.cluster\nfrom yellowbrick.cluster import KElbowVisualizer\nmodel = KElbowVisualizer(sklearn.cluster.KMeans(), k=10)\nmodel.fit(RGB_values.to_numpy());\nmodel.show();\nelbow_value = model.elbow_value_\n```\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-5-output-1.png){width=705 height=486}\n:::\n:::\n\n\nBelow is the algorithm for K-Means clustering with k-means++ initialization from scratch. The K-Means clustering algorithm can also be implemented by using the [scikit learn library](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n\nWe begin by initializing values such as the dataset in the [numpy](https://numpy.org/doc/stable/index.html) format, setting the number of clusters, setting the maximum number of iterations, as well as choosing the first centroid at random.\n\nThen, we run the k-means++ algorithm to select the initial centroids. The k-means++ algorithm selects initial cluster centroids using sampling based on an empirical probability distribution of the points' contribution to the overall inertia. This technique speeds up convergence.\n\n* Take one centroid $c_1$, chosen uniformly at random from the dataset.\n* Take a new center $c_i$, choosing an instance $\\mathbf{x}_i$ with probability: $D(\\mathbf{x}_i)^2$ / $\\sum\\limits_{j=1}^{m}{D(\\mathbf{x}_j)}^2$ where $D(\\mathbf{x}_i)$ is the distance between the instance $\\mathbf{x}_i$ and the closest centroid that was already chosen. This probability distribution ensures that instances that are further away from already chosen centroids are much more likely be selected as centroids.\n* Repeat the previous step until all $k$ centroids have been chosen.\n\nAfter the centroids have been chosen, the K-means algorithm begins running continuously mapping each datapoint to each cluster and improving chosen centroid values until the maximum number of iterations or until there is no change in the centroids chosen.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\" code-line-numbers=\"true\"}\nimport math\nimport random\n\nx = RGB_values.to_numpy()\ncentroid1 = x[random.randint(0,len(x))]\ncentroids = []\ncentroids.append([centroid1])\nnumClusters = elbow_value\niteration = 0\nmax_iter = 100 # can be passed in as a parameter\n\n'''\nk-means++ \n'''\n\n# pick cluster centroid with probability proportional to the centroid1\ndistance = [math.dist(centroid1, x[i])**2 for i in range(len(x))]\n\nfor i in range(1, numClusters):\n    # so above has just chosen the highest dist ones, but we still want random choice where the probability is depending on the distance\n    # also, normalize dists\n    # calculates probabilities\n    prob = distance/np.sum(distance)\n    # choose next centroid with probability proportional to distance squared\n    new_centroid = x[np.random.choice(range(len(x)), size=1, p=prob)]\n    centroids.append(new_centroid)\n    # update distances between newly chosen centroid and other points now\n    distance = [math.dist(new_centroid[0], x[i])**2 for i in range(len(x))]\n'''\nK-means algorithm\n'''\ncentroids = np.array(centroids)\nprev_centroids = np.zeros(centroids.shape)\nwhile np.not_equal(centroids, prev_centroids).any() and iteration < max_iter:\n            # Sort each datapoint, assigning to nearest centroid\n            # sorted points is of size num_clusters, num_points in each cluster\n            sorted_points = [[] for _ in range(numClusters)]\n            for point in x:\n                dists = [math.dist(point, np.squeeze(i)) for i in centroids]\n                centroid_idx = np.argmin(dists)\n                sorted_points[centroid_idx].append(point)\n            # Push current centroids to previous, reassign centroids as mean of the points belonging to them\n            # new centroid is mean of the points in each cluster\n            prev_centroids = centroids[:]\n            centroids = [np.mean(cluster, axis=0) for cluster in sorted_points]\n            # make sure that none of the centroid values are nan\n            for points in range(len(centroids)):\n                if np.isnan(centroids[points]).any():\n                    centroids[points] = prev_centroids[points]\n            iteration += 1\n```\n:::\n\n\nPlotting the clusters formed after K-Means clustering algorithm as shown below.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ncolors = []\n\nfor i in centroids:\n    r, g, b = i\n    colors.append((\n    r,\n    g,\n    b\n    ))\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(111, projection='3d')\n\n\n# plot the sorted clusters\n\nfor i in range(len(sorted_points)):\n    a, b, c = zip(*sorted_points[i])\n    ax.scatter(a, b, c, s = 40 , color = colors[i], marker = 'o', label = \"cluster \"+str(i))\n    label = \"centroid of cluster\" if i == len(sorted_points)-1 else \"\"\n    ax.scatter(colors[i][0], colors[i][1], colors[i][2], s = 100 , marker = 'x', color = [0,0,0], label = label)\n\nax.set_xlabel('R')\nax.set_ylabel('G')\nax.set_zlabel('B')\nax.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-7-output-1.png){width=625 height=611}\n:::\n:::\n\n\nWe can visualize the following to be the most dominant colours in the given image!\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nplt.grid(False)\nplt.imshow([colors])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-8-output-1.png){width=659 height=189}\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ntotal_points = len(RGB_values.to_numpy())\nlabels = ['Colour' + str(i+1) for i in range(len(colors))]\nsizes = [len(sorted_points[i])/total_points for i in range(len(sorted_points))]\nfig, ax = plt.subplots()\nax.pie(sizes,\n       colors=colors, autopct='%1.1f%%', pctdistance=1.15);\n```\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-9-output-1.png){width=426 height=426}\n:::\n:::\n\n\nAnd we're done!\n\n",
    "supporting": [
      "Clustering_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}