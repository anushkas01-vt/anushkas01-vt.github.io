{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding Baum-Welch Algorithm from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(states, sequence, a, b, pi, key):\n",
    "    N = len(states)\n",
    "    T = len(sequence)\n",
    "    pi = pi[key] # prob of state i, since 2 states, let's half it be 0.5, 0.5 initially\n",
    "    i = key # holds the first state\n",
    "\n",
    "    # Pseudocount to handle zeros\n",
    "    pseudocount = 1e-100\n",
    "    # for all possible states, and the first actual state (alpha)\n",
    "    # i.e. alpha i for all i has been caluclated given yt\n",
    "    alpha = np.zeros((N, T))\n",
    "    alpha[:,0] = pi * b[:,int(sequence[0])] + pseudocount\n",
    "\n",
    "\n",
    "    # next, we have to do iterations to calculate alpha at different times t\n",
    "    # we need all alpha values since it is going to be summed up to calculate gamma\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        for j in range(N):\n",
    "            alpha[j][t] = sum(alpha[i][t-1]*a[i][j]*b[j][int(sequence[t])] for i in range(N)) + pseudocount\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(states, sequence, a, b):\n",
    "    N = len(states)\n",
    "    T = len(sequence)\n",
    "    beta = np.zeros((N, T))\n",
    "\n",
    "    # Pseudocount to handle zeros\n",
    "    pseudocount = 1e-100\n",
    "\n",
    "    # Initialization\n",
    "    beta[:, -1] = 1  # Set the last column to 1\n",
    "\n",
    "    # Recursion\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        for i in range(N):\n",
    "            beta[i, t] = sum(a[i, j] * b[j, int(sequence[t + 1])] * beta[j, t + 1] for j in range(N)) + pseudocount\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(a, b, pi, sequence, states, key, n_iterations = 100, tol=1e-6):\n",
    "    #Baum-Welch algorithm for HMM\n",
    "    # calculate gamma, xi, and then update a and b parameters\n",
    "    N = len(states)\n",
    "    T = len(sequence)\n",
    "    \n",
    "    # M is the number of possible observations i.e. number of columns\n",
    "    M = b.shape[1]\n",
    "\n",
    "    prev_log_likelihood = 0\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        alpha = forward(states, sequence, a, b, pi, key)\n",
    "        beta = backward(states, sequence, a, b)\n",
    "\n",
    "        print(f\"Alpha: {alpha}\")\n",
    "        print(f\"Beta:{beta}\")\n",
    "\n",
    "        # Pseudocount to handle zeros\n",
    "        pseudocount = 1e-100\n",
    "        gamma = alpha * beta\n",
    "        # print(gamma)\n",
    "        denominator = np.sum(gamma, axis=0, keepdims=True) # same for all i\n",
    "        gamma = gamma/denominator + pseudocount\n",
    "\n",
    "        print(f\"gamma:{gamma}\") \n",
    "\n",
    "        xi = np.zeros((N, N, T - 1))\n",
    "\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                for t in range(T - 1):\n",
    "                    numerator = alpha[i, t] * a[i, j] * b[j, int(sequence[t + 1])] * beta[j, t + 1]\n",
    "                    denominator = np.sum(alpha[k, t] * a[k, l] * b[l, int(sequence[t + 1])] * beta[l, t + 1] for k in range(N) for l in range(N))\n",
    "                    xi[i, j, t] = (numerator / denominator) + pseudocount\n",
    "\n",
    "        print(f\"Xi: {xi}\")\n",
    "\n",
    "\n",
    "        # update a and b\n",
    "        # M-step\n",
    "        '''\n",
    "        sequence == k creates a boolean array of the same length as sequence, where each element is True if the corresponding element in sequence is equal to k, and False otherwise.\n",
    "    mask = (sequence == k) assigns this boolean array to the variable mask.\n",
    "    In the context of the Baum-Welch algorithm or similar algorithms for Hidden Markov Models (HMMs), this kind of mask is often used to select specific observations in the computation of probabilities. For example, \n",
    "    it might be used to sum over only the observations that match a particular value, which is relevant when updating the emission matrix b.\n",
    "        '''\n",
    "        # a = (np.sum(xi, axis=2) + pseudocount)/ np.sum(gamma[:, :-1], axis=1, keepdims=True) \n",
    "        for i in range(N):  # N is the number of states\n",
    "            for j in range(N):  # N is the number of states\n",
    "                numerator = np.sum(xi[i, j, :])\n",
    "                denominator = np.sum(gamma[i, :])\n",
    "                a[i, j] = (numerator+pseudocount) / (denominator+pseudocount) \n",
    "\n",
    "\n",
    "        b = np.zeros((N, M))\n",
    "        # print(gamma.shape)\n",
    "        gamma_sum = np.sum(gamma, axis=1)\n",
    "        \n",
    "        obs = []\n",
    "        for i in sequence:\n",
    "            obs.append(int(i))\n",
    "        obs = np.array(obs)\n",
    "\n",
    "        for j in range(N):\n",
    "            for k in range(M):\n",
    "                mask = (obs==k) # for indicative function i.e. 1 if observed = yt, else 0\n",
    "                b[j, k] = (np.sum(gamma[j]*mask)+ pseudocount) / (np.sum(gamma[j]) + pseudocount) \n",
    "        \n",
    "\n",
    "        # Normalize rows to ensure each row sums to 1.0\n",
    "        a = a / np.sum(a, axis=1)[:, np.newaxis]\n",
    "        b = b / np.sum(b, axis=1)[:, np.newaxis]\n",
    "\n",
    "        print(f\"a = {a}, b = {b}\")\n",
    "\n",
    "        # Log Likelihood Calculation\n",
    "        log_likelihood = np.sum(np.log(np.sum(alpha, axis=0)))\n",
    "\n",
    "        # Convergence Check\n",
    "        if np.abs(log_likelihood - prev_log_likelihood) < tol:\n",
    "            print(f\"Converged after {iteration + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "        prev_log_likelihood = log_likelihood\n",
    "\n",
    "    return a, b, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sequence, states, a, b, pi):\n",
    "    # Makes use of the viterbi algorithm to predict best path\n",
    "    # Initialize Variables\n",
    "    T = len(sequence)\n",
    "    N = len(states)\n",
    "\n",
    "    # Pseudocount to handle zeros\n",
    "    pseudocount = 1e-100\n",
    "\n",
    "    viterbi_table = np.zeros((N, T)) # delta\n",
    "    backpointer = np.zeros((N, T)) # psi\n",
    "\n",
    "    # Initialization step, for t = 0\n",
    "    print(int(sequence[0]))\n",
    "    viterbi_table[:, 0] = pi * b[:, int(sequence[0])] + pseudocount\n",
    "\n",
    "    # Calculate Probabilities\n",
    "    for t in range(1, T):\n",
    "        for s in range(N):\n",
    "            \n",
    "            max_prob = max(viterbi_table[prev_s][t-1] * a[prev_s][s] for prev_s in range(N)) * b[s][int(sequence[t])] \n",
    "            viterbi_table[s][t] = max_prob + pseudocount\n",
    "            backpointer[s][t] = np.argmax([viterbi_table[prev_s][t-1] * a[prev_s][s]for prev_s in range(N)])\n",
    "\n",
    "    #Traceback and Find Best Path\n",
    "    best_path = []\n",
    "    last_state = np.argmax(viterbi_table[:, -1])\n",
    "\n",
    "    best_path.append(last_state)\n",
    "    best_prob = 1.0\n",
    "    for t in range(T-2, -1, -1):\n",
    "        last_state = last_state = np.argmax(viterbi_table[:, t])\n",
    "        best_prob *= (viterbi_table[last_state, t] + pseudocount)\n",
    "        best_path.append(last_state) # i.e. add to start of list\n",
    "\n",
    "        \n",
    "    return best_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain this, prepare some more datasets, read them from csv files etc at least shows data prep etc\n",
    "Talk about Baum-Welch algorithm and viterbi and show code for it (even though it does not work. Can add a small note towards the end of the blog and say you are still debugging its implementation that would be cute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hmmlearn import hmm\n",
    "\n",
    "gen_model = hmm.CategoricalHMM(n_components=2, random_state=99)\n",
    "\n",
    "\n",
    "gen_model.startprob_ = np.array([1.0, 0.0])\n",
    "\n",
    "gen_model.transmat_ = np.array([[0.95, 0.05],\n",
    "                                [0.1, 0.9]])\n",
    "\n",
    "gen_model.emissionprob_ = \\\n",
    "    np.array([[1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6],\n",
    "              [1 / 10, 1 / 10, 1 / 10, 1 / 10, 1 / 10, 1 / 2]])\n",
    "\n",
    "# simulate the loaded dice rolls\n",
    "rolls, gen_states = gen_model.sample(30000)\n",
    "\n",
    "print(\"ROLLS\",rolls)\n",
    "\n",
    "# plot states over time, let's just look at the first rolls for clarity\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(gen_states[:500])\n",
    "ax.set_title('States over time')\n",
    "ax.set_xlabel('Time (# of rolls)')\n",
    "ax.set_ylabel('State')\n",
    "fig.show()\n",
    "\n",
    "# plot rolls for the fair and loaded states\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(rolls[gen_states == 0], label='fair', alpha=0.5,\n",
    "        bins=np.arange(7) - 0.5, density=True)\n",
    "ax.hist(rolls[gen_states == 1], label='loaded', alpha=0.5,\n",
    "        bins=np.arange(7) - 0.5, density=True)\n",
    "ax.set_title('Roll probabilities by state')\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_ylabel('Roll')\n",
    "ax.legend()\n",
    "fig.show()\n",
    "\n",
    "# %%\n",
    "# Now, let's see if we can recover our hidden states, transmission matrix\n",
    "# and emission probabilities.\n",
    "\n",
    "# split our data into training and validation sets (50/50 split)\n",
    "X_train = rolls[:rolls.shape[0] // 2]\n",
    "X_validate = rolls[rolls.shape[0] // 2:]\n",
    "\n",
    "# check optimal score\n",
    "gen_score = gen_model.score(X_validate)\n",
    "\n",
    "best_score = best_model = None\n",
    "n_fits = 50\n",
    "np.random.seed(13)\n",
    "for idx in range(n_fits):\n",
    "    model = hmm.CategoricalHMM(\n",
    "        n_components=2, random_state=idx,\n",
    "        init_params='se')  # don't init transition, set it below\n",
    "    # we need to initialize with random transition matrix probabilities\n",
    "    # because the default is an even likelihood transition\n",
    "    # we know transitions are rare (otherwise the casino would get caught!)\n",
    "    # so let's have an Dirichlet random prior with an alpha value of\n",
    "    # (0.1, 0.9) to enforce our assumption transitions happen roughly 10%\n",
    "    # of the time\n",
    "    model.transmat_ = np.array([np.random.dirichlet([0.9, 0.1]),\n",
    "                                np.random.dirichlet([0.1, 0.9])])\n",
    "    model.fit(X_train)\n",
    "    score = model.score(X_validate)\n",
    "    print(f'Model #{idx}\\tScore: {score}')\n",
    "    if best_score is None or score > best_score:\n",
    "        best_model = model\n",
    "        best_score = score\n",
    "\n",
    "print(f'Generated score: {gen_score}\\nBest score:      {best_score}')\n",
    "\n",
    "# use the Viterbi algorithm to predict the most likely sequence of states\n",
    "# given the model\n",
    "states = best_model.predict(rolls)\n",
    "\n",
    "# plot our recovered states compared to generated (aim 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(gen_states[:500], label='generated')\n",
    "ax.plot(states[:500] + 1.5, label='recovered')\n",
    "ax.set_yticks([])\n",
    "ax.set_title('States compared to generated')\n",
    "ax.set_xlabel('Time (# rolls)')\n",
    "ax.set_xlabel('State')\n",
    "ax.legend()\n",
    "fig.show()\n",
    "\n",
    "# %%\n",
    "# Let's check our learned transition probabilities and see if they match.\n",
    "\n",
    "print(f'Transmission Matrix Generated:\\n{gen_model.transmat_.round(3)}\\n\\n'\n",
    "      f'Transmission Matrix Recovered:\\n{best_model.transmat_.round(3)}\\n\\n')\n",
    "\n",
    "# %%\n",
    "# Finally, let's see if we can tell how the die is loaded.\n",
    "\n",
    "print(f'Emission Matrix Generated:\\n{gen_model.emissionprob_.round(3)}\\n\\n'\n",
    "      f'Emission Matrix Recovered:\\n{best_model.emissionprob_.round(3)}\\n\\n')\n",
    "\n",
    "# %%\n",
    "# In this case, we were able to get very good estimates of the transition and\n",
    "# emission matrices, but decoding the states was imperfect. That's because\n",
    "# the decoding algorithm is greedy and picks the most likely series of states\n",
    "# which isn't necessarily what happens in real life. Even so, our model could\n",
    "# tell us when to watch for the loaded die and we'd have a better chance at\n",
    "# catching them red-handed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally, do tests with comparison so you can do confusion matrix, roc curve or something, but i feel like hmm model would be nice and confusion matrix\n",
    "# cri time"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
