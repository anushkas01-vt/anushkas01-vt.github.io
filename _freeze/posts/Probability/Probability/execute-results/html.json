{
  "hash": "9e8f77d19c218a554ef551f778a26a3e",
  "result": {
    "markdown": "---\ntitle: \"Probability Theory\"\nauthor: \"Anushka S\"\ndate: \"2023-12-03\"\ncategories: [Probability Theory, Machine Learning, Random Variables, Hidden Markov Models, Viterbi algorithm, Baum Welch]\n---\n\nHidden Markov Models (HMMs) are a powerful statistical tool used in various fields, including speech recognition, bioinformatics, and natural language processing. Grounded in probability theory, HMMs are a type of stochastic model that represents a system evolving over time with hidden states. The model assumes that the observed data result from a probabilistic process involving these hidden states, making it particularly effective in situations where the underlying dynamics are not directly observable but can be inferred through observed data and the probabilities governing state transitions and emissions. Probability theory forms the backbone of HMMs, allowing them to make predictions and decisions based on the likelihood of sequences of observations given the model's parameters.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n:::\n\n\nThe [hmmlearn](https://hmmlearn.readthedocs.io/en/latest/) library in Python performs Unsupervised learning and inference of Hidden Markov Models.  \n\nLet us take the common Hidden Markov Model example of the ocassionally dishonest casino. In a casino, they use a fair die most of the time, but switch to the loaded die once in a while. The purpose of making use of the Hidden Markov Model is to identify instances when the dice roll is probabilistically from the fair die or the loaded die. \n\nThe probabilities for the various outcome variables have been adapted from [this textbook](https://doi.org/10.1017/CBO9780511790492).\n\n![Markov Model for dishonest casino](dice_prob.JPG){width=30%}\n\nThe emission probabilities are the probability of each outcome (die roll) at its current state. The transition probabilities define the probability of transitioning to a state (fair, loaded), and the start probabilities define the starting probability of being in each state.\n\nFrom the hmmlearn library, the CategoricalHMM model (derived from the MultinomialHMM model), uses the Baum-Welch algorithm for training hidden Markov models (HMMs). The Baum-Welch algorithm, also known as the Forward-Backward algorithm or the Expectation-Maximization (EM) algorithm for HMMs, is an iterative procedure for estimating the parameters of an HMM given a set of observed data.\n\nYou can read more on the training of the Hidden Markov Model and the probability theory behind identifying the sequence state part at the end of this blog.\n\nIn the block of code below, we initialize the probabilities, initialize the CategoricalHMM method which takes in n_components as the number of possible states, the number of iterations for training. The other parameters (such as init_param, algorithm, etc.) details can be found [here](https://hmmlearn.readthedocs.io/en/latest/api.html#categoricalhmm).\n\nWe then use the sample() method to generate die roll and corresponding state samples.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\" code-line-numbers=\"true\"}\nfrom hmmlearn import hmm\n\ngen_model = hmm.CategoricalHMM(n_components=2, n_iter=100, init_params = 'se')\n\ngen_model.startprob_ = np.array([1.0, 0.0])\n\ngen_model.transmat_ = np.array([[0.95, 0.05],\n                                [0.1, 0.9]])\n\ngen_model.emissionprob_ = \\\n    np.array([[1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6],\n              [1 / 10, 1 / 10, 1 / 10, 1 / 10, 1 / 10, 1 / 2]])\n\nrolls, gen_states = gen_model.sample(30000)\n```\n:::\n\n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\nTransition Model:\nEmission Matrix:\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Probability_files/figure-html/cell-4-output-2.png){width=466 height=463}\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fair</th>\n      <th>Unfair</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.166667</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.166667</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.166667</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.166667</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.166667</td>\n      <td>0.1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\nSample of Dice Rolls generated\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Roll</th>\n      <th>Coin_State</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nPlotting the states of the first 500 generated coin flips:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfig, ax = plt.subplots()\nax.plot(gen_states[:500])\nax.set_title('States over time')\nax.set_xlabel('Time (# of rolls)')\nax.set_ylabel('State')\nfig.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Probability_files/figure-html/cell-6-output-1.png){width=589 height=449}\n:::\n:::\n\n\nPlotting the rolls for the fair and loaded states\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfig, ax = plt.subplots()\nax.hist(rolls[gen_states == 0], label='fair', alpha=0.5,\n        bins=np.arange(7) - 0.5, density=True)\nax.hist(rolls[gen_states == 1], label='loaded', alpha=0.5,\n        bins=np.arange(7) - 0.5, density=True)\nax.set_title('Roll probabilities by state')\nax.set_xlabel('Count')\nax.set_ylabel('Roll')\nax.legend()\nfig.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Probability_files/figure-html/cell-7-output-1.png){width=589 height=449}\n:::\n:::\n\n\nIn the code below, we are performing a 50%-50% train-test dataset split. \nWe are then fitting the model on our train set and obtaining the score for the model which is simply the log probability under the model.\nThen, we make use of the predict method which implements the Viterbi algorithm to predict the best sequence of states for the given observations (dice rolls).\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"false\" code-line-numbers=\"true\"}\n# split our data into training and validation sets (50/50 split)\nX_train = rolls[:rolls.shape[0] // 2]\nX_test = rolls[rolls.shape[0] // 2:]\ny_test = np.array(gen_states[gen_states.shape[0] // 2:])\ngen_model = gen_model.fit(X_train)\n\n# check base score (non-tuned model)\ngen_score = gen_model.score(X_test)\n\nprint(f'Generated score: {gen_score}')\n\n# use the Viterbi algorithm to predict the most likely sequence of states\n# given the model\nstates = gen_model.predict(X_test)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nEven though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\nEven though the 'emissionprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'e'\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nGenerated score: -26111.908215770163\n```\n:::\n:::\n\n\nRecovered states vs Generated states:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfig, ax = plt.subplots()\nax.plot(gen_states[:500], label='generated')\nax.plot(states[:500] + 1.5, label='recovered')\nax.set_yticks([])\nax.set_title('States compared to generated')\nax.set_xlabel('Time (# rolls)')\nax.set_xlabel('State')\nax.legend()\nfig.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Probability_files/figure-html/cell-9-output-1.png){width=540 height=449}\n:::\n:::\n\n\nUpdated Markov Model probabilities after training the HMM on the dataset with the Baum-Welch algorithm.\n\n::: {.cell execution_count=9}\n\n::: {.cell-output .cell-output-stdout}\n```\nTransition Model:\nEmission Matrix:\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Probability_files/figure-html/cell-10-output-2.png){width=466 height=463}\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fair</th>\n      <th>Unfair</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.170</td>\n      <td>0.100</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.168</td>\n      <td>0.096</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.165</td>\n      <td>0.111</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.164</td>\n      <td>0.104</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.171</td>\n      <td>0.094</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nResults of the model in the form of a confusion matrix to identify how many times the model predicted 'Fair' and 'Loaded' coin correctly given the dice roll.\n\nAs we can see from the results below, the accuracy of the model is not considered to be extremely good. This is because we are dealing with a truly probabilistic model, the results are based on the 'likelihood' parameter. Also, the model has been trained on sample data which may not mimic true data to the fullest. \n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\nfrom sklearn.metrics import RocCurveDisplay\n\n# True states (hidden states)\ntrue_states = y_test\npredicted_states = states\n\n# Evaluate confusion matrix\nconf_matrix = confusion_matrix(true_states, predicted_states)\n# Display confusion matrix\nprint(\"Confusion Matrix:\")\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Fair', 'Loaded'])\ndisp.plot()\nplt.show()\n\n# Evaluate classification report\nclass_report = classification_report(true_states, predicted_states)\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(class_report)\n\nRocCurveDisplay.from_predictions(true_states, predicted_states)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix:\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.82      0.90      0.86     10061\n           1       0.75      0.60      0.66      4939\n\n    accuracy                           0.80     15000\n   macro avg       0.78      0.75      0.76     15000\nweighted avg       0.80      0.80      0.79     15000\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Probability_files/figure-html/cell-11-output-2.png){width=553 height=431}\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x2911d9aad70>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Probability_files/figure-html/cell-11-output-4.png){width=589 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndef convert_to_numpy(string_val, type):\n    str_int = []\n    if type == \"rolls\":\n        for i in string_val:\n            str_int.append(np.array([int(i)-1]))\n    else:\n        dice = {'F':0, 'L':1}\n        for i in string_val:\n            str_int.append(np.array(dice[i]))\n    return str_int\n\ndef convert_to_string(string_die):\n    dice = {0: 'F', 1:'L'}\n    string = \"\"\n    for i in string_die:\n        string+=dice[i]\n    return string\n```\n:::\n\n\nTesting the model on an example taken from the textbook:\n![HMM example](posts/Probability/HMM_test_sets.JPG){width = 60%}\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ntest_rolls1 = \"315116246446644245311321631164152133625144543631656626566666\"\n\ny_true1 = \"FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFLLLLLLLLLLLL\"\n\ntest_rolls2 = \"222555441666566563564324364131513465146353411126414626253356\"\ny_true2 = \"FFFFFFFFLLLLLLLLLLLLLFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFL\"\n\nX_test_1 = convert_to_numpy(test_rolls1, \"rolls\")\ny_test_1 = gen_model.predict(X_test_1)\nb = convert_to_string(y_test_1)\nprint(f\"Output:{test_rolls1} \\nDie:{y_true1} \\nViterbi:{b}\")\n\nX_test_2 = convert_to_numpy(test_rolls2, \"rolls\")\ny_test_2 = gen_model.predict(X_test_2)\nb = convert_to_string(y_test_2)\nprint(f\"Output:{test_rolls2} \\nDie:{y_true2} \\nViterbi:{b}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOutput:315116246446644245311321631164152133625144543631656626566666 \nDie:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFLLLLLLLLLLLL \nViterbi:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFLLLLLLLLLLLLLLL\nOutput:222555441666566563564324364131513465146353411126414626253356 \nDie:FFFFFFFFLLLLLLLLLLLLLFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFL \nViterbi:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\n```\n:::\n:::\n\n\n\\huge Detailing the probability theory behind the Hidden Markov Model\nThe Baum-Welch algorithm, also known as the Forward-Backward algorithm, is a parameter estimation technique for Hidden Markov Models (HMMs). Named after Leonard Baum and Lloyd Welch, this algorithm is a form of the Expectation-Maximization (EM) algorithm. Its primary goal is to iteratively refine the parameters of an HMM based on observed data, making it a powerful tool for model training.\n\nThe algorithm consists of two main steps, the expectation step and the maximization step.\n\nThe parameters of a HMM are given by $\\theta=(A,B,\\pi)$, where:\n\n* $A$ is the state transition matrix, which defines the probability of transitioning from one state to another.\n* $B$ is the emission matrix, which defines the probability of emitting a given observation from a given state.\n* $\\pi$ is the initial state distribution, which defines the probability of being in each state at the beginning of the sequence.\n\n$A=\\{a_{ij}\\}=P(X_{t}=j|X_{t-1}=i)$ is the state transition matrix\n\n$\\pi=\\{\\pi_{i}\\}=P(X_{1}=i)$ is the initial state distribution\n\n$B=\\{b_{j}(y_{t})\\}=P(Y_{t}=y_{t}|X_{t}=j)$ is the emission matrix\n\nGiven observation sequences $(Y=(Y_{1}=y_{1},Y_{2}=y_{2},...,Y_{T}=y_{T}))$ the\nalgorithm tries to find the parameters $(\\theta)$ that maximise the probability of the\nobservation.\n\nThe algorithm starts by choosing some initial values for the HMM parameters $\\theta = (A, B, \\pi)$. Then, it repeats the following steps until convergence:\n\n1. Determine probable state paths. This involves calculating the probability of each possible state path, given the observed sequence of emissions.\n2. Count the expected number of transitions and emissions. This involves counting the number of times each state transition is taken and each emission is made, weighted by the probability of each state path.\n3. Re-estimate the HMM parameters. This involves using the expected number of transitions and emissions to update the HMM parameters $\\theta$.\n\nThe forward-backward algorithm is used for finding probable paths.\n\n\nForward Procedure\n$(\\alpha_{i}(t)=P(Y_{1}=y_{1},...,Y_{t}=y_{t},X_{t}=i|\\theta))$ be the probability of seeing $(y_{1},...,y_{t})$ and being in state $i$ at time $t$. Found recursively using:\n\n$\\(\\alpha_{i}(1)=\\pi_{i}b_{i}(y_{1})\\)$\n\n$\\(\\alpha_{j}(t+1)=b_{j}(y_{t+1})\\sum_{i=1}^{N}\\alpha_{i}(t)a_{ij}\\)$\n\n\nBackward Procedure\n$(\\beta_{i}(t)=P(Y_{t+1}=y_{t+1},...,Y_{T}=y_{T}|X_{t}=i,\\theta))$ be the probability of ending partial sequence $(y_{t+1},...,y_{T})$ given starting state $i$ at time $t$. \n\n$(\\beta_{i}(t))$ is computed recursively as:\n\n$\\(\\beta_{i}(T)=1\\)$\n$\\(\\beta_{i}(t)=\\sum_{j=1}^{N}\\beta_{j}(t+1)a_{ij}b_{j}(y_{t+1}))$\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndef forward(states, sequence, a, b, pi, key):\n    N = len(states)\n    T = len(sequence)\n    pi = pi[key] # prob of state i, since 2 states, let's half it be 0.5, 0.5 initially\n    i = key # holds the first state\n\n    # Pseudocount to handle zeros\n    pseudocount = 1e-100\n    # for all possible states, and the first actual state (alpha)\n    # i.e. alpha i for all i has been caluclated given yt\n    alpha = np.zeros((N, T))\n    alpha[:,0] = pi * b[:,int(sequence[0])] + pseudocount\n\n\n    # next, we have to do iterations to calculate alpha at different times t\n    # we need all alpha values since it is going to be summed up to calculate gamma\n    \n    for t in range(1, T):\n        for j in range(N):\n            alpha[j][t] = sum(alpha[i][t-1]*a[i][j]*b[j][int(sequence[t])] for i in range(N)) + pseudocount\n\n    return alpha\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ndef backward(states, sequence, a, b):\n    N = len(states)\n    T = len(sequence)\n    beta = np.zeros((N, T))\n\n    # Pseudocount to handle zeros\n    pseudocount = 1e-100\n\n    # Initialization\n    beta[:, -1] = 1  # Set the last column to 1\n\n    # Recursion\n    for t in range(T - 2, -1, -1):\n        for i in range(N):\n            beta[i, t] = sum(a[i, j] * b[j, int(sequence[t + 1])] * beta[j, t + 1] for j in range(N)) + pseudocount\n\n    return beta\n```\n:::\n\n\nThe Expectation step:\nCalculate the probabilities of being in each state at each time step given the observed sequence using the Forward-Backward algorithm. These probabilities represent the likelihood of the system being in state \n$i$ at time $t$ given the entire observed sequence.\nCalculate the joint probabilities of transitioning from state $i$ to state $j$ at consecutive time steps given the observed sequence. \n\n$\\gamma_t(i) = \\frac{\\alpha_t(i) \\cdot \\beta_t(i)}{\\sum_{j=1}^{N} \\alpha_t(j) \\cdot \\beta_t(j)}$\n\n$\\xi_t(i, j) = \\frac{\\alpha_t(i) \\cdot a_{ij} \\cdot b_j(o_{t+1}) \\cdot \\beta_{t+1}(j)}{\\sum_{k=1}^{N} \\sum_{l=1}^{N} \\alpha_t(k) \\cdot a_{kl} \\cdot b_l(o_{t+1}) \\cdot \\beta_{t+1}(l)}$\n\n\nThe Maximization step:\nUpdate the model parameters, including the initial state probabilities, transition probabilities, and emission probabilities.\nThe updated parameters are computed by normalizing the expected counts derived from the E-step.\n\n$\\hat{\\pi}_i = \\gamma_1(i)$\n\n$\\hat{a}_{ij} = \\frac{\\sum_{t=1}^{T-1} \\xi_t(i, j)}{\\sum_{t=1}^{T-1} \\gamma_t(i)}$\n\n$\\hat{b}_i(k) = \\frac{\\sum_{t=1}^{T} \\gamma_t(i) \\cdot \\delta_{o_t, k}}{\\sum_{t=1}^{T} \\gamma_t(i)}$\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndef train(a, b, pi, sequence, states, key, n_iterations = 100, tol=1e-6):\n    #Baum-Welch algorithm for HMM\n    # calculate gamma, xi, and then update a and b parameters\n    N = len(states)\n    T = len(sequence)\n    \n    # M is the number of possible observations i.e. number of columns\n    M = b.shape[1]\n\n    prev_log_likelihood = 0\n\n    for iteration in range(n_iterations):\n        alpha = forward(states, sequence, a, b, pi, key)\n        beta = backward(states, sequence, a, b)\n\n        print(f\"Alpha: {alpha}\")\n        print(f\"Beta:{beta}\")\n\n        # Pseudocount to handle zeros\n        pseudocount = 1e-100\n        gamma = alpha * beta\n        # print(gamma)\n        denominator = np.sum(gamma, axis=0, keepdims=True) # same for all i\n        gamma = gamma/denominator + pseudocount\n\n        print(f\"gamma:{gamma}\") \n\n        xi = np.zeros((N, N, T - 1))\n\n        for i in range(N):\n            for j in range(N):\n                for t in range(T - 1):\n                    numerator = alpha[i, t] * a[i, j] * b[j, int(sequence[t + 1])] * beta[j, t + 1]\n                    denominator = np.sum(alpha[k, t] * a[k, l] * b[l, int(sequence[t + 1])] * beta[l, t + 1] for k in range(N) for l in range(N))\n                    xi[i, j, t] = (numerator / denominator) + pseudocount\n\n        print(f\"Xi: {xi}\")\n\n\n        # update a and b\n        # M-step\n        '''\n        sequence == k creates a boolean array of the same length as sequence, where each element is True if the corresponding element in sequence is equal to k, and False otherwise.\n    mask = (sequence == k) assigns this boolean array to the variable mask.\n    In the context of the Baum-Welch algorithm or similar algorithms for Hidden Markov Models (HMMs), this kind of mask is often used to select specific observations in the computation of probabilities. For example, \n    it might be used to sum over only the observations that match a particular value, which is relevant when updating the emission matrix b.\n        '''\n        # a = (np.sum(xi, axis=2) + pseudocount)/ np.sum(gamma[:, :-1], axis=1, keepdims=True) \n        for i in range(N):  # N is the number of states\n            for j in range(N):  # N is the number of states\n                numerator = np.sum(xi[i, j, :])\n                denominator = np.sum(gamma[i, :])\n                a[i, j] = (numerator+pseudocount) / (denominator+pseudocount) \n\n\n        b = np.zeros((N, M))\n        # print(gamma.shape)\n        gamma_sum = np.sum(gamma, axis=1)\n        \n        obs = []\n        for i in sequence:\n            obs.append(int(i))\n        obs = np.array(obs)\n\n        for j in range(N):\n            for k in range(M):\n                mask = (obs==k) # for indicative function i.e. 1 if observed = yt, else 0\n                b[j, k] = (np.sum(gamma[j]*mask)+ pseudocount) / (np.sum(gamma[j]) + pseudocount) \n        \n\n        # Normalize rows to ensure each row sums to 1.0\n        a = a / np.sum(a, axis=1)[:, np.newaxis]\n        b = b / np.sum(b, axis=1)[:, np.newaxis]\n\n        print(f\"a = {a}, b = {b}\")\n\n        # Log Likelihood Calculation\n        log_likelihood = np.sum(np.log(np.sum(alpha, axis=0)))\n\n        # Convergence Check\n        if np.abs(log_likelihood - prev_log_likelihood) < tol:\n            print(f\"Converged after {iteration + 1} iterations.\")\n            break\n\n        prev_log_likelihood = log_likelihood\n\n    return a, b, pi\n```\n:::\n\n\nThe Viterbi algorithm is a dynamic programming algorithm used for decoding Hidden Markov Models (HMMs) and finding the most likely sequence of hidden states given an observed sequence. \nThe algorithm efficiently determines the optimal state sequence by considering the probabilities of transitions and emissions.\n\nThe core idea behind the Viterbi algorithm is to iteratively compute the most likely path to each state at each time step, incorporating both the current observation and the previously calculated probabilities.\n\n$$\nδ_i(t) = max_j δ_j(t - 1) a_ji b_i(Y_t)\n$$\n\n$$\nψ_i(t) = argmax_j δ_j(t - 1) a_ji\n$$\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndef predict(sequence, states, a, b, pi):\n    # Makes use of the viterbi algorithm to predict best path\n    # Initialize Variables\n    T = len(sequence)\n    N = len(states)\n\n    # Pseudocount to handle zeros\n    pseudocount = 1e-100\n\n    viterbi_table = np.zeros((N, T)) # delta\n    backpointer = np.zeros((N, T)) # psi\n\n    # Initialization step, for t = 0\n    print(int(sequence[0]))\n    viterbi_table[:, 0] = pi * b[:, int(sequence[0])] + pseudocount\n\n    # Calculate Probabilities\n    for t in range(1, T):\n        for s in range(N):\n            \n            max_prob = max(viterbi_table[prev_s][t-1] * a[prev_s][s] for prev_s in range(N)) * b[s][int(sequence[t])] \n            viterbi_table[s][t] = max_prob + pseudocount\n            backpointer[s][t] = np.argmax([viterbi_table[prev_s][t-1] * a[prev_s][s]for prev_s in range(N)])\n\n    #Traceback and Find Best Path\n    best_path = []\n    last_state = np.argmax(viterbi_table[:, -1])\n\n    best_path.append(last_state)\n    best_prob = 1.0\n    for t in range(T-2, -1, -1):\n        last_state = last_state = np.argmax(viterbi_table[:, t])\n        best_prob *= (viterbi_table[last_state, t] + pseudocount)\n        best_path.append(last_state) # i.e. add to start of list\n\n        \n    return best_path\n```\n:::\n\n\nHidden Markov Models are also used in many applications, one such interesting one is to identify cpgIslandsgiven a genomic sequence. You can read more about it [here!](https://doi.org/10.1017/CBO9780511790492).\n\n",
    "supporting": [
      "Probability_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}