<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Anushka S">
<meta name="dcterms.date" content="2023-12-03">

<title>CS5805 - Classification: Predicting the onset of diabetes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CS5805</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Classification: Predicting the onset of diabetes</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">Classification</div>
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">Supervised Learning</div>
                <div class="quarto-category">SVM</div>
                <div class="quarto-category">Random Forest</div>
                <div class="quarto-category">Decision Tree</div>
                <div class="quarto-category">XGBoost</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Anushka S </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 3, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><a href="https://www.ibm.com/topics/supervised-learning#:~:text=Supervised%20learning%2C%20also%20known%20as,data%20or%20predict%20outcomes%20accurately.">Supervised learning</a>, also known as supervised machine learning, is a subcategory of machine learning and artificial intelligence. It is defined by its use of labeled datasets to train algorithms that to classify data or predict outcomes accurately.</p>
<p>Classification uses machine learning algorithms that learn how to assign a class label to examples from the problem domain. The class labels are a set of discrete values. A model will use the training dataset and will calculate how to best map examples of input data to specific class labels. As such, the training dataset must be sufficiently representative of the problem and have many examples of each class label. Based on the set of class labels, classification can be binary classification (2 class labels) or multi-class classification (&gt;2 class labels). You can read more on classification <a href="https://machinelearningmastery.com/types-of-classification-in-machine-learning/">here!</a>.</p>
<p>In this blog, we will be dealing with binary classification on the Pima Indian Diabetes dataet from the <a href="https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database">UCI Machine Learning Repository</a>. This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.</p>
<p>Below, we can see a sample of the dataset chosen.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>df_diabetes <span class="op">=</span> pd.read_csv(<span class="st">"diabetes.csv"</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>df_diabetes.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Pregnancies</th>
<th data-quarto-table-cell-role="th">Glucose</th>
<th data-quarto-table-cell-role="th">BloodPressure</th>
<th data-quarto-table-cell-role="th">SkinThickness</th>
<th data-quarto-table-cell-role="th">Insulin</th>
<th data-quarto-table-cell-role="th">BMI</th>
<th data-quarto-table-cell-role="th">DiabetesPedigreeFunction</th>
<th data-quarto-table-cell-role="th">Age</th>
<th data-quarto-table-cell-role="th">Outcome</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>6</td>
<td>148</td>
<td>72</td>
<td>35</td>
<td>0</td>
<td>33.6</td>
<td>0.627</td>
<td>50</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1</td>
<td>85</td>
<td>66</td>
<td>29</td>
<td>0</td>
<td>26.6</td>
<td>0.351</td>
<td>31</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>8</td>
<td>183</td>
<td>64</td>
<td>0</td>
<td>0</td>
<td>23.3</td>
<td>0.672</td>
<td>32</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1</td>
<td>89</td>
<td>66</td>
<td>23</td>
<td>94</td>
<td>28.1</td>
<td>0.167</td>
<td>21</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>0</td>
<td>137</td>
<td>40</td>
<td>35</td>
<td>168</td>
<td>43.1</td>
<td>2.288</td>
<td>33</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>5</td>
<td>116</td>
<td>74</td>
<td>0</td>
<td>0</td>
<td>25.6</td>
<td>0.201</td>
<td>30</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>3</td>
<td>78</td>
<td>50</td>
<td>32</td>
<td>88</td>
<td>31.0</td>
<td>0.248</td>
<td>26</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>10</td>
<td>115</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>35.3</td>
<td>0.134</td>
<td>29</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>2</td>
<td>197</td>
<td>70</td>
<td>45</td>
<td>543</td>
<td>30.5</td>
<td>0.158</td>
<td>53</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>8</td>
<td>125</td>
<td>96</td>
<td>0</td>
<td>0</td>
<td>0.0</td>
<td>0.232</td>
<td>54</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>We can plot the correlation between the features(columns) and in the chart below, we can see which features have a higher correlation with the tagret variable.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>f, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>corr <span class="op">=</span> df_diabetes.corr()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    cmap<span class="op">=</span>sns.diverging_palette(<span class="dv">220</span>, <span class="dv">10</span>, as_cmap<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    vmin<span class="op">=-</span><span class="fl">1.0</span>, vmax<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    annot <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    square<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>ax)<span class="op">;</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Classification_files/figure-html/cell-3-output-1.png" width="788" height="741"></p>
</div>
</div>
<p>We can view the info about the data. We see that there are no null values but there are columns having 0 values which are missing values. It is important to handle missing data and prepare it well before it is processed through the classification model.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_diabetes.info())</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_diabetes.drop(columns<span class="op">=</span>[<span class="st">'Pregnancies'</span>, <span class="st">'Outcome'</span>]).isin([<span class="dv">0</span>, <span class="fl">0.0</span>]).<span class="bu">sum</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 768 entries, 0 to 767
Data columns (total 9 columns):
 #   Column                    Non-Null Count  Dtype  
---  ------                    --------------  -----  
 0   Pregnancies               768 non-null    int64  
 1   Glucose                   768 non-null    int64  
 2   BloodPressure             768 non-null    int64  
 3   SkinThickness             768 non-null    int64  
 4   Insulin                   768 non-null    int64  
 5   BMI                       768 non-null    float64
 6   DiabetesPedigreeFunction  768 non-null    float64
 7   Age                       768 non-null    int64  
 8   Outcome                   768 non-null    int64  
dtypes: float64(2), int64(7)
memory usage: 54.1 KB
None
Glucose                       5
BloodPressure                35
SkinThickness               227
Insulin                     374
BMI                          11
DiabetesPedigreeFunction      0
Age                           0
dtype: int64</code></pre>
</div>
</div>
<p>We convert the missing values into Nan values so that we can apply the K nearest neighbours imputation algorithm.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># storing outcomes in dataframe y, and storing pregnancies in a separate list temporarily</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># instead of creating a copy of another dataframe</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>pregnancies <span class="op">=</span> df_diabetes[<span class="st">'Pregnancies'</span>]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_diabetes[<span class="st">'Outcome'</span>]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>df_diabetes <span class="op">=</span> df_diabetes.drop(columns<span class="op">=</span>[<span class="st">'Pregnancies'</span>, <span class="st">'Outcome'</span>])</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># making the 0 missing values into Nan values for imputing</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>df_diabetes.replace(<span class="dv">0</span>, np.nan, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of missing values = </span><span class="sc">{</span>np<span class="sc">.</span>isnan(df_diabetes.to_numpy())<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>df_diabetes[<span class="st">'Pregnancies'</span>] <span class="op">=</span> pregnancies</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>columns <span class="op">=</span> df_diabetes.columns</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>df_diabetes.head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Number of missing values = 652</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Glucose</th>
<th data-quarto-table-cell-role="th">BloodPressure</th>
<th data-quarto-table-cell-role="th">SkinThickness</th>
<th data-quarto-table-cell-role="th">Insulin</th>
<th data-quarto-table-cell-role="th">BMI</th>
<th data-quarto-table-cell-role="th">DiabetesPedigreeFunction</th>
<th data-quarto-table-cell-role="th">Age</th>
<th data-quarto-table-cell-role="th">Pregnancies</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>148.0</td>
<td>72.0</td>
<td>35.0</td>
<td>NaN</td>
<td>33.6</td>
<td>0.627</td>
<td>50</td>
<td>6</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>85.0</td>
<td>66.0</td>
<td>29.0</td>
<td>NaN</td>
<td>26.6</td>
<td>0.351</td>
<td>31</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>183.0</td>
<td>64.0</td>
<td>NaN</td>
<td>NaN</td>
<td>23.3</td>
<td>0.672</td>
<td>32</td>
<td>8</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>89.0</td>
<td>66.0</td>
<td>23.0</td>
<td>94.0</td>
<td>28.1</td>
<td>0.167</td>
<td>21</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>137.0</td>
<td>40.0</td>
<td>35.0</td>
<td>168.0</td>
<td>43.1</td>
<td>2.288</td>
<td>33</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Before imputing the data, we: 1. Split the data into train-test split 2. Scale the training data and the testing data separately.</p>
<p>The reason for splitting the data and then scaling it and then applying imputation is so that there is no data leakage between the train-test datasets. Since data leakage can make our model biased leading to incorrect results and inaccurate evaluation metric scores.</p>
<p>The training set and the test set are then imputed separately with the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html">KNNImputer</a> with 5 neighbours. Imputation for completing missing values using k-Nearest Neighbors. Each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> (df_diabetes).to_numpy()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 80-20 Train-Test split</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>scaling_x<span class="op">=</span>StandardScaler()</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>X_train<span class="op">=</span>scaling_x.fit_transform(X_train)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>X_test<span class="op">=</span>scaling_x.transform(X_test)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Imputing missing values using knn</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># knn imputation transform for the dataset</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.impute <span class="im">import</span> KNNImputer</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># print total missing</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Missing: </span><span class="sc">%d</span><span class="st">'</span> <span class="op">%</span> <span class="bu">sum</span>(np.isnan(X).flatten()))</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co"># define imputer</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>imputer <span class="op">=</span> KNNImputer(n_neighbors<span class="op">=</span><span class="dv">5</span>) <span class="co"># taking 5 neighbours</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="co"># fit transform on the dataset for training and testing set</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>X_train_imputed <span class="op">=</span> imputer.fit_transform(X_train)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>X_test_imputed <span class="op">=</span> imputer.transform(X_test)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co"># print total missing</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>X_trans <span class="op">=</span> np.concatenate((X_train_imputed, X_test_imputed), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Missing: </span><span class="sc">%d</span><span class="st">'</span> <span class="op">%</span> <span class="bu">sum</span>(np.isnan(X_trans).flatten()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Missing: 652
Missing: 0</code></pre>
</div>
</div>
<p>We can see, all values have been normalized and there are no missing values in the dataset.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>df_diabetes_cleaned <span class="op">=</span> pd.DataFrame(X_trans, columns <span class="op">=</span> columns)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>df_diabetes_cleaned.head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Glucose</th>
<th data-quarto-table-cell-role="th">BloodPressure</th>
<th data-quarto-table-cell-role="th">SkinThickness</th>
<th data-quarto-table-cell-role="th">Insulin</th>
<th data-quarto-table-cell-role="th">BMI</th>
<th data-quarto-table-cell-role="th">DiabetesPedigreeFunction</th>
<th data-quarto-table-cell-role="th">Age</th>
<th data-quarto-table-cell-role="th">Pregnancies</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.757757</td>
<td>0.607781</td>
<td>1.665478</td>
<td>-0.227452</td>
<td>0.831683</td>
<td>0.529526</td>
<td>0.567932</td>
<td>1.516591</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.233976</td>
<td>-0.808447</td>
<td>0.698824</td>
<td>0.513708</td>
<td>1.313142</td>
<td>-0.069689</td>
<td>0.398450</td>
<td>1.812018</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>-0.649904</td>
<td>0.135705</td>
<td>1.085486</td>
<td>-0.433789</td>
<td>0.729555</td>
<td>-0.794249</td>
<td>0.991638</td>
<td>0.925736</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>-0.060651</td>
<td>0.450422</td>
<td>-0.905821</td>
<td>1.088148</td>
<td>-1.050384</td>
<td>-0.167519</td>
<td>2.601722</td>
<td>1.221164</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>-0.060651</td>
<td>0.293064</td>
<td>0.795490</td>
<td>-0.433789</td>
<td>1.094297</td>
<td>-0.760619</td>
<td>-0.364222</td>
<td>-0.551400</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>We can now begin classification and compare various popular classification models such as Support Vector Machines (SVM), Decision Trees (DT), Random Forest (RF), and XGBoost (XGB). We also explore some hyperparameter tuning parameters and compare all the models on their performance on this dataset. Hyperparameter tuning relies more on experimental results than theory, and thus the best method to determine the optimal settings is to try many different combinations evaluate the performance of each model.</p>
<p>The evaluation metrics are: 1. Accuracy: Accuracy is a measure of overall correctness and is calculated as the ratio of correctly predicted instances to the total number of instances in the dataset. <span class="math inline">\(Accuracy = \frac{True Positive + True Negative}{True Positive + True Negative + False Positive + False Negative}\)</span></p>
<ol start="2" type="1">
<li><p>Precision: Precision is the ratio of correctly predicted positive instances to the total predicted positive instances. It measures the accuracy of positive predictions. <span class="math inline">\(Precision = \frac{True Positive}{ True Positive + False Positive}\)</span></p></li>
<li><p>Recall: Recall is the ratio of correctly predicted positive instances to the total actual positive instances. It measures the model’s ability to capture all positive instances. <span class="math inline">\(Recall (Sensitivity) = \frac{True Positive}{ True Positive + False Negative}\)</span></p></li>
<li><p>F1 score: The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall, especially useful when the class distribution is imbalanced. <span class="math inline">\(F1 = \frac{2*Precision*Recall}{ Precision + Recall}\)</span></p></li>
</ol>
<p>where TP and TN define the samples labeled correctly as the positive class or negative class, FP define the samples falsely labeled as positive and FN define the samples falsely labeled as negative.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pprint <span class="im">import</span> pprint</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>best_preds <span class="op">=</span> []</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>model_names <span class="op">=</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Support Vector Machine Classification</strong></p>
<p>Support Vector Machines (SVM) is a supervised machine learning algorithm used for classification and regression tasks. SVM works by finding a hyperplane in a high-dimensional space that best separates the data points of different classes. The “support vectors” are the data points closest to the decision boundary, and the margin is the distance between the support vectors and the decision boundary. SVM aims to maximize this margin, providing robust generalization to unseen data. SVM is considered a linear classifier but it can handle non-linear relationships through the use of kernel functions, allowing it to map input data into a higher-dimensional space. SVM is particularly effective in high-dimensional spaces and is widely used in various applications, including image classification and text categorization. In the code, we make use of the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC">scikit-learn</a> library for the SVM implementation. The parameter information can be found in the implementation page.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>model_names.append(<span class="st">'Support Vector Machine'</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an SVM model</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>svm_model <span class="op">=</span> SVC()</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Current params:"</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>pprint(svm_model.get_params())</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>svm_model.fit(X_train_imputed, y_train)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>y_pred_best <span class="op">=</span> svm_model.predict(X_test_imputed)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>best_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Current params:
{'C': 1.0,
 'break_ties': False,
 'cache_size': 200,
 'class_weight': None,
 'coef0': 0.0,
 'decision_function_shape': 'ovr',
 'degree': 3,
 'gamma': 'scale',
 'kernel': 'rbf',
 'max_iter': -1,
 'probability': False,
 'random_state': None,
 'shrinking': True,
 'tol': 0.001,
 'verbose': False}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="cell-output cell-output-stdout">
<pre><code>
SVM classifier
Accuracy: 0.7987012987012987
F1 score: 0.6666666666666666</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="cell-output cell-output-display">
<p><img src="Classification_files/figure-html/cell-11-output-1.png" width="574" height="429"></p>
</div>
</div>
<p><strong>Decision Tree Classification</strong></p>
<p>Decision Trees are a non-linear, hierarchical model that partitions the input space into regions and assigns a class label or regression value to each region. The tree structure is built by recursively splitting the data based on the most informative features at each node. A common splitting technique is using the <a href="https://medium.com/@viswatejaster/measure-of-impurity-62bda86d8760">impurity measure</a> to decide whether the branch must be split or not. Decision Trees are interpretable, easy to visualize, and capable of handling both categorical and numerical features. However, they are prone to overfitting, especially when deep trees are constructed. Techniques like pruning and limiting tree depth help mitigate overfitting and improve generalization. In the code, we make use of the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier">scikit-learn</a> library for the Decision Tree implementation. The parameter information can be found in the implementation page.</p>
<p><em>Grid-Search Hyperparameter tuning</em></p>
<p>GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations of parameters we define. In the code, we make use of the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">scikit-learn</a> library for the GridSearchCV implementation. The parameter information can be found in the implementation page.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>model_names.append(<span class="st">'Decision Tree'</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Current params:"</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>pprint(dt.get_params())</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>dt.fit(X_train_imputed, y_train)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="va">None</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>],</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">5</span>),</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: [<span class="st">'auto'</span>, <span class="st">'sqrt'</span>, <span class="st">'log2'</span>, <span class="va">None</span>],</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'criterion'</span>: [<span class="st">'gini'</span>, <span class="st">'entropy'</span>],</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>grid_search_dt <span class="op">=</span> GridSearchCV(dt, params, cv<span class="op">=</span><span class="dv">3</span>, scoring<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model to the data and perform hyperparameter tuning</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>grid_search_dt.fit(X_train_imputed, y_train)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the best hyperparameters</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Hyperparameters:"</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>pprint(grid_search_dt.best_params_)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best model</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>best_model_dt <span class="op">=</span> grid_search_dt.best_estimator_</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> dt.predict(X_test_imputed)</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>y_pred_best <span class="op">=</span> best_model_dt.predict(X_test_imputed)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>best_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Current params:
{'ccp_alpha': 0.0,
 'class_weight': None,
 'criterion': 'gini',
 'max_depth': None,
 'max_features': None,
 'max_leaf_nodes': None,
 'min_impurity_decrease': 0.0,
 'min_samples_leaf': 1,
 'min_samples_split': 2,
 'min_weight_fraction_leaf': 0.0,
 'random_state': None,
 'splitter': 'best'}
Best Hyperparameters:
{'criterion': 'entropy',
 'max_depth': 15,
 'max_features': 'log2',
 'min_samples_leaf': 4,
 'min_samples_split': 10}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="cell-output cell-output-stdout">
<pre><code>
DT without hyperparameter tuning
Accuracy: 0.6883116883116883
F1 score: 0.5636363636363636

DT with hyperparameter tuning
Accuracy: 0.7337662337662337
F1 score: 0.5684210526315789</code></pre>
</div>
</div>
<div class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<p><img src="Classification_files/figure-html/cell-14-output-1.png" width="574" height="429"></p>
</div>
</div>
<p><strong>Random Forest Classification</strong></p>
<p>Random Forest is an ensemble learning method that builds a multitude of decision trees during training and outputs the mode of the classes for classification tasks or the average prediction for regression tasks. Each tree in the forest is constructed using a random subset of the training data and a random subset of features. The randomness and diversity among trees help mitigate overfitting and improve overall model accuracy. Random Forest is known for its robustness, versatility, and effectiveness in handling high-dimensional data. In the code, we make use of the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">scikit-learn</a> library for the Random Forest implementation. The parameter information can be found in the implementation page.</p>
<p><em>RandomSearch Hyperparameter tuning</em></p>
<p>RandomizedSearchCV, a method that, sample randomly from a distribution and evaluates the randomly chosen of parameters we define. In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter. In the code, we make use of the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV">scikit-learn</a> library for the RandomizedSearchCV implementation. The parameter information can be found in the implementation page.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> RandomizedSearchCV</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>model_names.append(<span class="st">'Random Forest'</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier()</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Current params:"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>pprint(rf.get_params())</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>rf.fit(X_train_imputed, y_train)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>max_depth <span class="op">=</span> [<span class="bu">int</span>(x) <span class="cf">for</span> x <span class="kw">in</span> np.linspace(<span class="dv">10</span>, <span class="dv">110</span>, num <span class="op">=</span> <span class="dv">11</span>)]</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>max_depth.append(<span class="va">None</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the random grid</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>random_grid <span class="op">=</span> {<span class="st">'n_estimators'</span>: [<span class="bu">int</span>(x) <span class="cf">for</span> x <span class="kw">in</span> np.linspace(start <span class="op">=</span> <span class="dv">200</span>, stop <span class="op">=</span> <span class="dv">2000</span>, num <span class="op">=</span> <span class="dv">10</span>)],</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>               <span class="st">'max_features'</span>: [<span class="st">'auto'</span>, <span class="st">'sqrt'</span>],</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>               <span class="st">'max_depth'</span>: max_depth,</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>               <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>               <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>],</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>               <span class="st">'bootstrap'</span>: [<span class="va">True</span>, <span class="va">False</span>]}</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>rf_random <span class="op">=</span> RandomizedSearchCV(estimator <span class="op">=</span> rf, param_distributions <span class="op">=</span> random_grid, n_iter <span class="op">=</span> <span class="dv">100</span>, cv <span class="op">=</span> <span class="dv">3</span>, verbose<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_jobs <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the random search model</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>rf_random.fit(X_train_imputed, y_train)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the best hyperparameters</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Hyperparameters:"</span>)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>pprint(rf_random.best_params_)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best model</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>best_model_rf <span class="op">=</span> rf_random.best_estimator_</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> rf.predict(X_test_imputed)</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>y_pred_best <span class="op">=</span> best_model_rf.predict(X_test_imputed)</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>best_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Current params:
{'bootstrap': True,
 'ccp_alpha': 0.0,
 'class_weight': None,
 'criterion': 'gini',
 'max_depth': None,
 'max_features': 'sqrt',
 'max_leaf_nodes': None,
 'max_samples': None,
 'min_impurity_decrease': 0.0,
 'min_samples_leaf': 1,
 'min_samples_split': 2,
 'min_weight_fraction_leaf': 0.0,
 'n_estimators': 100,
 'n_jobs': None,
 'oob_score': False,
 'random_state': None,
 'verbose': 0,
 'warm_start': False}
Fitting 3 folds for each of 100 candidates, totalling 300 fits
Best Hyperparameters:
{'bootstrap': True,
 'max_depth': 80,
 'max_features': 'sqrt',
 'min_samples_leaf': 2,
 'min_samples_split': 10,
 'n_estimators': 1400}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="cell-output cell-output-stdout">
<pre><code>
RF without hyperparameter tuning
Accuracy: 0.8051948051948052
F1 score: 0.7058823529411765

RF with hyperparameter tuning
Accuracy: 0.8051948051948052
F1 score: 0.7000000000000001</code></pre>
</div>
</div>
<div class="cell" data-execution_count="16">
<div class="cell-output cell-output-display">
<p><img src="Classification_files/figure-html/cell-17-output-1.png" width="574" height="429"></p>
</div>
</div>
<p><strong>XGBoost</strong></p>
<p>XGBoost, or Extreme Gradient Boosting, is a machine learning algorithm renowned for its efficiency and performance in predictive modeling tasks. It belongs to the ensemble learning family and is an extension of traditional gradient boosting methods. The core idea behind XGBoost is the sequential addition of weak learners, often decision trees, which are trained to correct errors made by preceding models. XGBoost introduces several key innovations, including regularization techniques to prevent overfitting, parallelized tree construction for faster training, and gradient-based optimization for rapid convergence. At its core, XGBoost structures its ensemble as a collection of decision trees, each contributing to the final prediction. So, gradient boosting is in the form of ensemble of weak prediction models. The algorithm assigns weights to the misclassified instances in each iteration, adjusting subsequent trees to focus on the previously misclassified samples. During training, XGBoost uses gradient-based optimization to efficiently navigate the solution space and arrive at an ensemble of trees that collectively delivers a robust and accurate prediction.</p>
<p>In the code, we make use of the <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">XGBoost</a> library for the XGBoost implementation. The parameter information can be found in the implementation page.</p>
<p><em>BayesSearch Hyperparameter tuning</em></p>
<p>Bayesian optimization is a more sophisticated technique that uses Bayesian methods to model the underlying function that maps hyperparameters to the model performance. It tries to find the optimal set of hyperparameters by making smart guesses based on the previous results. Bayesian optimization is more efficient than grid or random search because it attempts to balance exploration and exploitation of the search space. It can also deal with the cases of large number of hyperparameters and large search space. However, it can be more difficult to implement than grid search or random search and may require more computational resources.</p>
<p>In the code, we make use of the <a href="https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html">skopt</a> library for the BayesSearchCV implementation. The parameter information can be found in the implementation page.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBClassifier</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt <span class="im">import</span> BayesSearchCV</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>model_names.append(<span class="st">'XGBoost'</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an XGBoost classifier</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>xgb <span class="op">=</span> XGBClassifier()</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Current params:"</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>pprint(xgb.get_params())</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>xgb.fit(X_train_imputed, y_train)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameter search space</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>param_space <span class="op">=</span> {</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: (<span class="dv">3</span>, <span class="dv">10</span>),</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'learning_rate'</span>: (<span class="fl">0.01</span>, <span class="fl">1.0</span>, <span class="st">'log-uniform'</span>),</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: (<span class="dv">50</span>, <span class="dv">200</span>),</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_child_weight'</span>: (<span class="dv">1</span>, <span class="dv">10</span>),</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'subsample'</span>: (<span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="st">'uniform'</span>),</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'gamma'</span>: (<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="st">'uniform'</span>),</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">'colsample_bytree'</span>: (<span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="st">'uniform'</span>),</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate BayesSearchCV</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>bayes_search_xgb <span class="op">=</span> BayesSearchCV(</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    xgb,</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    param_space,</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">3</span>,  <span class="co"># Number of cross-validation folds</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">int</span> <span class="op">=</span> np.int_</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model to the training data and perform hyperparameter tuning</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>bayes_search_xgb.fit(X_train_imputed, y_train)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the best hyperparameters</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Hyperparameters:"</span>)</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>pprint(bayes_search_xgb.best_params_)</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best model</span></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>best_model_xgb <span class="op">=</span> bayes_search_xgb.best_estimator_</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> xgb.predict(X_test_imputed)</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>y_pred_best <span class="op">=</span> best_model_xgb.predict(X_test_imputed)</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>best_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Current params:
{'base_score': None,
 'booster': None,
 'callbacks': None,
 'colsample_bylevel': None,
 'colsample_bynode': None,
 'colsample_bytree': None,
 'device': None,
 'early_stopping_rounds': None,
 'enable_categorical': False,
 'eval_metric': None,
 'feature_types': None,
 'gamma': None,
 'grow_policy': None,
 'importance_type': None,
 'interaction_constraints': None,
 'learning_rate': None,
 'max_bin': None,
 'max_cat_threshold': None,
 'max_cat_to_onehot': None,
 'max_delta_step': None,
 'max_depth': None,
 'max_leaves': None,
 'min_child_weight': None,
 'missing': nan,
 'monotone_constraints': None,
 'multi_strategy': None,
 'n_estimators': None,
 'n_jobs': None,
 'num_parallel_tree': None,
 'objective': 'binary:logistic',
 'random_state': None,
 'reg_alpha': None,
 'reg_lambda': None,
 'sampling_method': None,
 'scale_pos_weight': None,
 'subsample': None,
 'tree_method': None,
 'validate_parameters': None,
 'verbosity': None}
Best Hyperparameters:
OrderedDict([('colsample_bytree', 0.6428543790514372),
             ('gamma', 0.28683956560688645),
             ('learning_rate', 0.01),
             ('max_depth', 6),
             ('min_child_weight', 10),
             ('n_estimators', 162),
             ('subsample', 0.8413378311689648)])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="cell-output cell-output-stdout">
<pre><code>
XGB without hyperparameter tuning
Accuracy: 0.7727272727272727
F1 score: 0.6728971962616823

XGB with hyperparameter tuning
Accuracy: 0.7857142857142857
F1 score: 0.6451612903225806</code></pre>
</div>
</div>
<div class="cell" data-execution_count="19">
<div class="cell-output cell-output-display">
<p><img src="Classification_files/figure-html/cell-20-output-1.png" width="574" height="430"></p>
</div>
</div>
<p>Analyzing the results of all the chosen models, we get the table below:</p>
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tabulate their classification report</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>evaluation_metrics <span class="op">=</span> [<span class="st">'Accuracy'</span>, <span class="st">'Precision'</span>, <span class="st">'Recall'</span>, <span class="st">'F1-score'</span>]</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> [<span class="dv">30</span>, <span class="dv">7</span>]</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.autolayout"</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>axs.axis(<span class="st">'tight'</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>axs.axis(<span class="st">'off'</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>table1 <span class="op">=</span> axs.table(cellText<span class="op">=</span>best_preds,</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>                      cellLoc <span class="op">=</span> <span class="st">'left'</span>,</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>                      rowLabels <span class="op">=</span> model_names,</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>                      rowColours<span class="op">=</span> [<span class="st">"palegreen"</span>] <span class="op">*</span> <span class="dv">10</span>,</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>                      colLabels<span class="op">=</span>evaluation_metrics,</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>                      colColours<span class="op">=</span> [<span class="st">"palegreen"</span>] <span class="op">*</span> <span class="dv">10</span>,</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>                      loc<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight cells with minimum value in each column</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col_idx, metric <span class="kw">in</span> <span class="bu">enumerate</span>(evaluation_metrics):</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    col_values <span class="op">=</span> [row[col_idx] <span class="cf">for</span> row <span class="kw">in</span> best_preds]</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    max_value_idx <span class="op">=</span> col_values.index(<span class="bu">max</span>(col_values))</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Highlight the cell with maximum value in coral color</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>    table1[max_value_idx <span class="op">+</span> <span class="dv">1</span>, col_idx].set_facecolor(<span class="st">"coral"</span>)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>table1.auto_set_font_size(<span class="va">False</span>)</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>table1.set_fontsize(<span class="dv">14</span>)</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>table1.scale(<span class="dv">1</span>, <span class="dv">4</span>)</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Classification_files/figure-html/cell-21-output-1.png" width="2870" height="662"></p>
</div>
</div>
<p>This blog only discusses a few classification algorithms and model tuning parameters. By applying the right model, tuning, and regularizing the model, we can aim to improve the accuracy of the model.</p>


<!-- -->


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb24" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Classification: Predicting the onset of diabetes"</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Anushka S"</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-12-03"</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Classification, Machine Learning, Supervised Learning, SVM, Random Forest, Decision Tree, XGBoost]</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Supervised learning</span><span class="co">](https://www.ibm.com/topics/supervised-learning#:~:text=Supervised%20learning%2C%20also%20known%20as,data%20or%20predict%20outcomes%20accurately.)</span>, also known as supervised machine learning, is a subcategory of machine learning and artificial intelligence. It is defined by its use of labeled datasets to train algorithms that to classify data or predict outcomes accurately.</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>Classification uses machine learning algorithms that learn how to assign a class label to examples from the problem domain. The class labels are a set of discrete values. A model will use the training dataset and will calculate how to best map examples of input data to specific class labels. As such, the training dataset must be sufficiently representative of the problem and have many examples of each class label. Based on the set of class labels, classification can be binary classification (2 class labels) or multi-class classification (&gt;2 class labels). You can read more on classification <span class="co">[</span><span class="ot">here!</span><span class="co">](https://machinelearningmastery.com/types-of-classification-in-machine-learning/)</span>.</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>In this blog, we will be dealing with binary classification on the Pima Indian Diabetes dataet from the <span class="co">[</span><span class="ot">UCI Machine Learning Repository</span><span class="co">](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)</span>. This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>Below, we can see a sample of the dataset chosen.</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>df_diabetes <span class="op">=</span> pd.read_csv(<span class="st">"diabetes.csv"</span>)</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>df_diabetes.head(<span class="dv">10</span>)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>We can plot the correlation between the features(columns) and in the chart below, we can see which features have a higher correlation with the tagret variable.</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>f, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>corr <span class="op">=</span> df_diabetes.corr()</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr,</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>    cmap<span class="op">=</span>sns.diverging_palette(<span class="dv">220</span>, <span class="dv">10</span>, as_cmap<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>    vmin<span class="op">=-</span><span class="fl">1.0</span>, vmax<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>    annot <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>    square<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>ax)<span class="op">;</span></span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>We can view the info about the data. We see that there are no null values but there are columns having 0 values which are missing values. It is important to handle missing data and prepare it well before it is processed through the classification model.</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_diabetes.info())</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_diabetes.drop(columns<span class="op">=</span>[<span class="st">'Pregnancies'</span>, <span class="st">'Outcome'</span>]).isin([<span class="dv">0</span>, <span class="fl">0.0</span>]).<span class="bu">sum</span>())</span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>We convert the missing values into Nan values so that we can apply the K nearest neighbours imputation algorithm.</span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a><span class="co"># storing outcomes in dataframe y, and storing pregnancies in a separate list temporarily</span></span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a><span class="co"># instead of creating a copy of another dataframe</span></span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a>pregnancies <span class="op">=</span> df_diabetes[<span class="st">'Pregnancies'</span>]</span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_diabetes[<span class="st">'Outcome'</span>]</span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a>df_diabetes <span class="op">=</span> df_diabetes.drop(columns<span class="op">=</span>[<span class="st">'Pregnancies'</span>, <span class="st">'Outcome'</span>])</span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a><span class="co"># making the 0 missing values into Nan values for imputing</span></span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>df_diabetes.replace(<span class="dv">0</span>, np.nan, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of missing values = </span><span class="sc">{</span>np<span class="sc">.</span>isnan(df_diabetes.to_numpy())<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a>df_diabetes[<span class="st">'Pregnancies'</span>] <span class="op">=</span> pregnancies</span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a>columns <span class="op">=</span> df_diabetes.columns</span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a>df_diabetes.head(<span class="dv">5</span>)</span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a>Before imputing the data, we:</span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Split the data into train-test split</span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Scale the training data and the testing data separately.</span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a>The reason for splitting the data and then scaling it and then applying imputation is so that there is no data leakage between the train-test datasets. Since data leakage can make our model biased leading to incorrect results and inaccurate evaluation metric scores.</span>
<span id="cb24-78"><a href="#cb24-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-79"><a href="#cb24-79" aria-hidden="true" tabindex="-1"></a>The training set and the test set are then imputed separately with the <span class="co">[</span><span class="ot">KNNImputer</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)</span> with 5 neighbours.</span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a>Imputation for completing missing values using k-Nearest Neighbors. Each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.</span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-84"><a href="#cb24-84" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-85"><a href="#cb24-85" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb24-86"><a href="#cb24-86" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb24-87"><a href="#cb24-87" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb24-88"><a href="#cb24-88" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y</span>
<span id="cb24-89"><a href="#cb24-89" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> (df_diabetes).to_numpy()</span>
<span id="cb24-90"><a href="#cb24-90" aria-hidden="true" tabindex="-1"></a><span class="co"># 80-20 Train-Test split</span></span>
<span id="cb24-91"><a href="#cb24-91" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb24-92"><a href="#cb24-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-93"><a href="#cb24-93" aria-hidden="true" tabindex="-1"></a>scaling_x<span class="op">=</span>StandardScaler()</span>
<span id="cb24-94"><a href="#cb24-94" aria-hidden="true" tabindex="-1"></a>X_train<span class="op">=</span>scaling_x.fit_transform(X_train)</span>
<span id="cb24-95"><a href="#cb24-95" aria-hidden="true" tabindex="-1"></a>X_test<span class="op">=</span>scaling_x.transform(X_test)</span>
<span id="cb24-96"><a href="#cb24-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-97"><a href="#cb24-97" aria-hidden="true" tabindex="-1"></a><span class="co"># Imputing missing values using knn</span></span>
<span id="cb24-98"><a href="#cb24-98" aria-hidden="true" tabindex="-1"></a><span class="co"># knn imputation transform for the dataset</span></span>
<span id="cb24-99"><a href="#cb24-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-100"><a href="#cb24-100" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.impute <span class="im">import</span> KNNImputer</span>
<span id="cb24-101"><a href="#cb24-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-102"><a href="#cb24-102" aria-hidden="true" tabindex="-1"></a><span class="co"># print total missing</span></span>
<span id="cb24-103"><a href="#cb24-103" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Missing: </span><span class="sc">%d</span><span class="st">'</span> <span class="op">%</span> <span class="bu">sum</span>(np.isnan(X).flatten()))</span>
<span id="cb24-104"><a href="#cb24-104" aria-hidden="true" tabindex="-1"></a><span class="co"># define imputer</span></span>
<span id="cb24-105"><a href="#cb24-105" aria-hidden="true" tabindex="-1"></a>imputer <span class="op">=</span> KNNImputer(n_neighbors<span class="op">=</span><span class="dv">5</span>) <span class="co"># taking 5 neighbours</span></span>
<span id="cb24-106"><a href="#cb24-106" aria-hidden="true" tabindex="-1"></a><span class="co"># fit transform on the dataset for training and testing set</span></span>
<span id="cb24-107"><a href="#cb24-107" aria-hidden="true" tabindex="-1"></a>X_train_imputed <span class="op">=</span> imputer.fit_transform(X_train)</span>
<span id="cb24-108"><a href="#cb24-108" aria-hidden="true" tabindex="-1"></a>X_test_imputed <span class="op">=</span> imputer.transform(X_test)</span>
<span id="cb24-109"><a href="#cb24-109" aria-hidden="true" tabindex="-1"></a><span class="co"># print total missing</span></span>
<span id="cb24-110"><a href="#cb24-110" aria-hidden="true" tabindex="-1"></a>X_trans <span class="op">=</span> np.concatenate((X_train_imputed, X_test_imputed), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb24-111"><a href="#cb24-111" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Missing: </span><span class="sc">%d</span><span class="st">'</span> <span class="op">%</span> <span class="bu">sum</span>(np.isnan(X_trans).flatten()))</span>
<span id="cb24-112"><a href="#cb24-112" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-113"><a href="#cb24-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-114"><a href="#cb24-114" aria-hidden="true" tabindex="-1"></a>We can see, all values have been normalized and there are no missing values in the dataset.</span>
<span id="cb24-117"><a href="#cb24-117" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-118"><a href="#cb24-118" aria-hidden="true" tabindex="-1"></a>df_diabetes_cleaned <span class="op">=</span> pd.DataFrame(X_trans, columns <span class="op">=</span> columns)</span>
<span id="cb24-119"><a href="#cb24-119" aria-hidden="true" tabindex="-1"></a>df_diabetes_cleaned.head(<span class="dv">5</span>)</span>
<span id="cb24-120"><a href="#cb24-120" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-121"><a href="#cb24-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-122"><a href="#cb24-122" aria-hidden="true" tabindex="-1"></a>We can now begin classification and compare various popular classification models such as Support Vector Machines (SVM), Decision Trees (DT), Random Forest (RF), and XGBoost (XGB). We also explore some hyperparameter tuning parameters and compare all the models on their performance on this dataset. Hyperparameter tuning relies more on experimental results than theory, and thus the best method to determine the optimal settings is to try many different combinations evaluate the performance of each model. </span>
<span id="cb24-123"><a href="#cb24-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-124"><a href="#cb24-124" aria-hidden="true" tabindex="-1"></a>The evaluation metrics are:</span>
<span id="cb24-125"><a href="#cb24-125" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Accuracy: Accuracy is a measure of overall correctness and is calculated as the ratio of correctly predicted instances to the total number of instances in the dataset.</span>
<span id="cb24-126"><a href="#cb24-126" aria-hidden="true" tabindex="-1"></a>$Accuracy = \frac{True Positive + True Negative}{True Positive + True Negative + False Positive + False Negative}$</span>
<span id="cb24-127"><a href="#cb24-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-128"><a href="#cb24-128" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Precision: Precision is the ratio of correctly predicted positive instances to the total predicted positive instances. It measures the accuracy of positive predictions.</span>
<span id="cb24-129"><a href="#cb24-129" aria-hidden="true" tabindex="-1"></a>$Precision = \frac{True Positive}{ True Positive + False Positive}$</span>
<span id="cb24-130"><a href="#cb24-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-131"><a href="#cb24-131" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Recall:  Recall is the ratio of correctly predicted positive instances to the total actual positive instances. It measures the model's ability to capture all positive instances.</span>
<span id="cb24-132"><a href="#cb24-132" aria-hidden="true" tabindex="-1"></a>$Recall (Sensitivity) = \frac{True Positive}{ True Positive + False Negative}$</span>
<span id="cb24-133"><a href="#cb24-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-134"><a href="#cb24-134" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>F1 score: The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall, especially useful when the class distribution is imbalanced.</span>
<span id="cb24-135"><a href="#cb24-135" aria-hidden="true" tabindex="-1"></a>$F1 = \frac{2*Precision*Recall}{ Precision + Recall}$</span>
<span id="cb24-136"><a href="#cb24-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-137"><a href="#cb24-137" aria-hidden="true" tabindex="-1"></a>where TP and TN define the samples labeled correctly as the positive class or negative class, FP define the samples falsely labeled as positive and FN define the samples falsely labeled as negative.</span>
<span id="cb24-138"><a href="#cb24-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-141"><a href="#cb24-141" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-142"><a href="#cb24-142" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score</span>
<span id="cb24-143"><a href="#cb24-143" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb24-144"><a href="#cb24-144" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pprint <span class="im">import</span> pprint</span>
<span id="cb24-145"><a href="#cb24-145" aria-hidden="true" tabindex="-1"></a>best_preds <span class="op">=</span> []</span>
<span id="cb24-146"><a href="#cb24-146" aria-hidden="true" tabindex="-1"></a>model_names <span class="op">=</span> []</span>
<span id="cb24-147"><a href="#cb24-147" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-148"><a href="#cb24-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-149"><a href="#cb24-149" aria-hidden="true" tabindex="-1"></a>**Support Vector Machine Classification**</span>
<span id="cb24-150"><a href="#cb24-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-151"><a href="#cb24-151" aria-hidden="true" tabindex="-1"></a>Support Vector Machines (SVM) is a supervised machine learning algorithm used for classification and regression tasks. SVM works by finding a hyperplane in a high-dimensional space that best separates the data points of different classes. The "support vectors" are the data points closest to the decision boundary, and the margin is the distance between the support vectors and the decision boundary. SVM aims to maximize this margin, providing robust generalization to unseen data. SVM is considered a linear classifier but it can handle non-linear relationships through the use of kernel functions, allowing it to map input data into a higher-dimensional space. SVM is particularly effective in high-dimensional spaces and is widely used in various applications, including image classification and text categorization. </span>
<span id="cb24-152"><a href="#cb24-152" aria-hidden="true" tabindex="-1"></a>In the code, we make use of the <span class="co">[</span><span class="ot">scikit-learn</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)</span> library for the SVM implementation. The parameter information can be found in the implementation page.</span>
<span id="cb24-153"><a href="#cb24-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-156"><a href="#cb24-156" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-157"><a href="#cb24-157" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb24-158"><a href="#cb24-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-159"><a href="#cb24-159" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb24-160"><a href="#cb24-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-161"><a href="#cb24-161" aria-hidden="true" tabindex="-1"></a>model_names.append(<span class="st">'Support Vector Machine'</span>)</span>
<span id="cb24-162"><a href="#cb24-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-163"><a href="#cb24-163" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an SVM model</span></span>
<span id="cb24-164"><a href="#cb24-164" aria-hidden="true" tabindex="-1"></a>svm_model <span class="op">=</span> SVC()</span>
<span id="cb24-165"><a href="#cb24-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-166"><a href="#cb24-166" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Current params:"</span>)</span>
<span id="cb24-167"><a href="#cb24-167" aria-hidden="true" tabindex="-1"></a>pprint(svm_model.get_params())</span>
<span id="cb24-168"><a href="#cb24-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-169"><a href="#cb24-169" aria-hidden="true" tabindex="-1"></a>svm_model.fit(X_train_imputed, y_train)</span>
<span id="cb24-170"><a href="#cb24-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-171"><a href="#cb24-171" aria-hidden="true" tabindex="-1"></a>y_pred_best <span class="op">=</span> svm_model.predict(X_test_imputed)</span>
<span id="cb24-172"><a href="#cb24-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-173"><a href="#cb24-173" aria-hidden="true" tabindex="-1"></a>best_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])</span>
<span id="cb24-174"><a href="#cb24-174" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-175"><a href="#cb24-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-178"><a href="#cb24-178" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-179"><a href="#cb24-179" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-180"><a href="#cb24-180" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb24-181"><a href="#cb24-181" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"SVM classifier"</span>)</span>
<span id="cb24-182"><a href="#cb24-182" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy_score(y_test, y_pred_best)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-183"><a href="#cb24-183" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 score: </span><span class="sc">{</span>f1_score(y_test, y_pred_best)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-184"><a href="#cb24-184" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb24-185"><a href="#cb24-185" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-186"><a href="#cb24-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-189"><a href="#cb24-189" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-190"><a href="#cb24-190" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-191"><a href="#cb24-191" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_test, y_pred_best, labels<span class="op">=</span>svm_model.classes_)</span>
<span id="cb24-192"><a href="#cb24-192" aria-hidden="true" tabindex="-1"></a>disp <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm, display_labels<span class="op">=</span>[<span class="st">'Non-Diabetic'</span>, <span class="st">'Diabetic'</span>])</span>
<span id="cb24-193"><a href="#cb24-193" aria-hidden="true" tabindex="-1"></a>disp.plot()</span>
<span id="cb24-194"><a href="#cb24-194" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-195"><a href="#cb24-195" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-196"><a href="#cb24-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-197"><a href="#cb24-197" aria-hidden="true" tabindex="-1"></a>**Decision Tree Classification**</span>
<span id="cb24-198"><a href="#cb24-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-199"><a href="#cb24-199" aria-hidden="true" tabindex="-1"></a>Decision Trees are a non-linear, hierarchical model that partitions the input space into regions and assigns a class label or regression value to each region. The tree structure is built by recursively splitting the data based on the most informative features at each node. A common splitting technique is using the <span class="co">[</span><span class="ot">impurity measure</span><span class="co">](https://medium.com/@viswatejaster/measure-of-impurity-62bda86d8760)</span> to decide whether the branch must be split or not. Decision Trees are interpretable, easy to visualize, and capable of handling both categorical and numerical features. However, they are prone to overfitting, especially when deep trees are constructed. Techniques like pruning and limiting tree depth help mitigate overfitting and improve generalization.</span>
<span id="cb24-200"><a href="#cb24-200" aria-hidden="true" tabindex="-1"></a>In the code, we make use of the <span class="co">[</span><span class="ot">scikit-learn</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)</span> library for the Decision Tree implementation. The parameter information can be found in the implementation page.</span>
<span id="cb24-201"><a href="#cb24-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-202"><a href="#cb24-202" aria-hidden="true" tabindex="-1"></a>*Grid-Search Hyperparameter tuning*</span>
<span id="cb24-203"><a href="#cb24-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-204"><a href="#cb24-204" aria-hidden="true" tabindex="-1"></a>GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations of parameters we define.</span>
<span id="cb24-205"><a href="#cb24-205" aria-hidden="true" tabindex="-1"></a>In the code, we make use of the <span class="co">[</span><span class="ot">scikit-learn</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)</span> library for the GridSearchCV implementation. The parameter information can be found in the implementation page.</span>
<span id="cb24-206"><a href="#cb24-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-209"><a href="#cb24-209" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-210"><a href="#cb24-210" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb24-211"><a href="#cb24-211" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb24-212"><a href="#cb24-212" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb24-213"><a href="#cb24-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-214"><a href="#cb24-214" aria-hidden="true" tabindex="-1"></a>model_names.append(<span class="st">'Decision Tree'</span>)</span>
<span id="cb24-215"><a href="#cb24-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-216"><a href="#cb24-216" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="cb24-217"><a href="#cb24-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-218"><a href="#cb24-218" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Current params:"</span>)</span>
<span id="cb24-219"><a href="#cb24-219" aria-hidden="true" tabindex="-1"></a>pprint(dt.get_params())</span>
<span id="cb24-220"><a href="#cb24-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-221"><a href="#cb24-221" aria-hidden="true" tabindex="-1"></a>dt.fit(X_train_imputed, y_train)</span>
<span id="cb24-222"><a href="#cb24-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-223"><a href="#cb24-223" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb24-224"><a href="#cb24-224" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="va">None</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>],</span>
<span id="cb24-225"><a href="#cb24-225" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb24-226"><a href="#cb24-226" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">5</span>),</span>
<span id="cb24-227"><a href="#cb24-227" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: [<span class="st">'auto'</span>, <span class="st">'sqrt'</span>, <span class="st">'log2'</span>, <span class="va">None</span>],</span>
<span id="cb24-228"><a href="#cb24-228" aria-hidden="true" tabindex="-1"></a>    <span class="st">'criterion'</span>: [<span class="st">'gini'</span>, <span class="st">'entropy'</span>],</span>
<span id="cb24-229"><a href="#cb24-229" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-230"><a href="#cb24-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-231"><a href="#cb24-231" aria-hidden="true" tabindex="-1"></a>grid_search_dt <span class="op">=</span> GridSearchCV(dt, params, cv<span class="op">=</span><span class="dv">3</span>, scoring<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb24-232"><a href="#cb24-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-233"><a href="#cb24-233" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model to the data and perform hyperparameter tuning</span></span>
<span id="cb24-234"><a href="#cb24-234" aria-hidden="true" tabindex="-1"></a>grid_search_dt.fit(X_train_imputed, y_train)</span>
<span id="cb24-235"><a href="#cb24-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-236"><a href="#cb24-236" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the best hyperparameters</span></span>
<span id="cb24-237"><a href="#cb24-237" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Hyperparameters:"</span>)</span>
<span id="cb24-238"><a href="#cb24-238" aria-hidden="true" tabindex="-1"></a>pprint(grid_search_dt.best_params_)</span>
<span id="cb24-239"><a href="#cb24-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-240"><a href="#cb24-240" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best model</span></span>
<span id="cb24-241"><a href="#cb24-241" aria-hidden="true" tabindex="-1"></a>best_model_dt <span class="op">=</span> grid_search_dt.best_estimator_</span>
<span id="cb24-242"><a href="#cb24-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-243"><a href="#cb24-243" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> dt.predict(X_test_imputed)</span>
<span id="cb24-244"><a href="#cb24-244" aria-hidden="true" tabindex="-1"></a>y_pred_best <span class="op">=</span> best_model_dt.predict(X_test_imputed)</span>
<span id="cb24-245"><a href="#cb24-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-246"><a href="#cb24-246" aria-hidden="true" tabindex="-1"></a>best_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])</span>
<span id="cb24-247"><a href="#cb24-247" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-248"><a href="#cb24-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-251"><a href="#cb24-251" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-252"><a href="#cb24-252" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-253"><a href="#cb24-253" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb24-254"><a href="#cb24-254" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DT without hyperparameter tuning"</span>)</span>
<span id="cb24-255"><a href="#cb24-255" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy_score(y_test, y_pred)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-256"><a href="#cb24-256" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 score: </span><span class="sc">{</span>f1_score(y_test, y_pred)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-257"><a href="#cb24-257" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb24-258"><a href="#cb24-258" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DT with hyperparameter tuning"</span>)</span>
<span id="cb24-259"><a href="#cb24-259" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy_score(y_test, y_pred_best)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-260"><a href="#cb24-260" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 score: </span><span class="sc">{</span>f1_score(y_test, y_pred_best)<span class="sc">}</span><span class="ss">"</span>)  </span>
<span id="cb24-261"><a href="#cb24-261" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-264"><a href="#cb24-264" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-265"><a href="#cb24-265" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-266"><a href="#cb24-266" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_test, y_pred_best, labels<span class="op">=</span>best_model_dt.classes_)</span>
<span id="cb24-267"><a href="#cb24-267" aria-hidden="true" tabindex="-1"></a>disp <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm, display_labels<span class="op">=</span>[<span class="st">'Non-Diabetic'</span>, <span class="st">'Diabetic'</span>])</span>
<span id="cb24-268"><a href="#cb24-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-269"><a href="#cb24-269" aria-hidden="true" tabindex="-1"></a>disp.plot()</span>
<span id="cb24-270"><a href="#cb24-270" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-271"><a href="#cb24-271" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-272"><a href="#cb24-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-273"><a href="#cb24-273" aria-hidden="true" tabindex="-1"></a>**Random Forest Classification**</span>
<span id="cb24-274"><a href="#cb24-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-275"><a href="#cb24-275" aria-hidden="true" tabindex="-1"></a>Random Forest is an ensemble learning method that builds a multitude of decision trees during training and outputs the mode of the classes for classification tasks or the average prediction for regression tasks. Each tree in the forest is constructed using a random subset of the training data and a random subset of features. The randomness and diversity among trees help mitigate overfitting and improve overall model accuracy. Random Forest is known for its robustness, versatility, and effectiveness in handling high-dimensional data.</span>
<span id="cb24-276"><a href="#cb24-276" aria-hidden="true" tabindex="-1"></a>In the code, we make use of the <span class="co">[</span><span class="ot">scikit-learn</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)</span> library for the Random Forest implementation. The parameter information can be found in the implementation page.</span>
<span id="cb24-277"><a href="#cb24-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-278"><a href="#cb24-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-279"><a href="#cb24-279" aria-hidden="true" tabindex="-1"></a>*RandomSearch Hyperparameter tuning*</span>
<span id="cb24-280"><a href="#cb24-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-281"><a href="#cb24-281" aria-hidden="true" tabindex="-1"></a>RandomizedSearchCV, a method that, sample randomly from a distribution and evaluates the randomly chosen of parameters we define. In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.</span>
<span id="cb24-282"><a href="#cb24-282" aria-hidden="true" tabindex="-1"></a>In the code, we make use of the <span class="co">[</span><span class="ot">scikit-learn</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)</span> library for the RandomizedSearchCV implementation. The parameter information can be found in the implementation page.</span>
<span id="cb24-283"><a href="#cb24-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-286"><a href="#cb24-286" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-287"><a href="#cb24-287" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb24-288"><a href="#cb24-288" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb24-289"><a href="#cb24-289" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> RandomizedSearchCV</span>
<span id="cb24-290"><a href="#cb24-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-291"><a href="#cb24-291" aria-hidden="true" tabindex="-1"></a>model_names.append(<span class="st">'Random Forest'</span>)</span>
<span id="cb24-292"><a href="#cb24-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-293"><a href="#cb24-293" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier()</span>
<span id="cb24-294"><a href="#cb24-294" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Current params:"</span>)</span>
<span id="cb24-295"><a href="#cb24-295" aria-hidden="true" tabindex="-1"></a>pprint(rf.get_params())</span>
<span id="cb24-296"><a href="#cb24-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-297"><a href="#cb24-297" aria-hidden="true" tabindex="-1"></a>rf.fit(X_train_imputed, y_train)</span>
<span id="cb24-298"><a href="#cb24-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-299"><a href="#cb24-299" aria-hidden="true" tabindex="-1"></a>max_depth <span class="op">=</span> [<span class="bu">int</span>(x) <span class="cf">for</span> x <span class="kw">in</span> np.linspace(<span class="dv">10</span>, <span class="dv">110</span>, num <span class="op">=</span> <span class="dv">11</span>)]</span>
<span id="cb24-300"><a href="#cb24-300" aria-hidden="true" tabindex="-1"></a>max_depth.append(<span class="va">None</span>)</span>
<span id="cb24-301"><a href="#cb24-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-302"><a href="#cb24-302" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the random grid</span></span>
<span id="cb24-303"><a href="#cb24-303" aria-hidden="true" tabindex="-1"></a>random_grid <span class="op">=</span> {<span class="st">'n_estimators'</span>: [<span class="bu">int</span>(x) <span class="cf">for</span> x <span class="kw">in</span> np.linspace(start <span class="op">=</span> <span class="dv">200</span>, stop <span class="op">=</span> <span class="dv">2000</span>, num <span class="op">=</span> <span class="dv">10</span>)],</span>
<span id="cb24-304"><a href="#cb24-304" aria-hidden="true" tabindex="-1"></a>               <span class="st">'max_features'</span>: [<span class="st">'auto'</span>, <span class="st">'sqrt'</span>],</span>
<span id="cb24-305"><a href="#cb24-305" aria-hidden="true" tabindex="-1"></a>               <span class="st">'max_depth'</span>: max_depth,</span>
<span id="cb24-306"><a href="#cb24-306" aria-hidden="true" tabindex="-1"></a>               <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb24-307"><a href="#cb24-307" aria-hidden="true" tabindex="-1"></a>               <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>],</span>
<span id="cb24-308"><a href="#cb24-308" aria-hidden="true" tabindex="-1"></a>               <span class="st">'bootstrap'</span>: [<span class="va">True</span>, <span class="va">False</span>]}</span>
<span id="cb24-309"><a href="#cb24-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-310"><a href="#cb24-310" aria-hidden="true" tabindex="-1"></a>rf_random <span class="op">=</span> RandomizedSearchCV(estimator <span class="op">=</span> rf, param_distributions <span class="op">=</span> random_grid, n_iter <span class="op">=</span> <span class="dv">100</span>, cv <span class="op">=</span> <span class="dv">3</span>, verbose<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_jobs <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb24-311"><a href="#cb24-311" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the random search model</span></span>
<span id="cb24-312"><a href="#cb24-312" aria-hidden="true" tabindex="-1"></a>rf_random.fit(X_train_imputed, y_train)</span>
<span id="cb24-313"><a href="#cb24-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-314"><a href="#cb24-314" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the best hyperparameters</span></span>
<span id="cb24-315"><a href="#cb24-315" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Hyperparameters:"</span>)</span>
<span id="cb24-316"><a href="#cb24-316" aria-hidden="true" tabindex="-1"></a>pprint(rf_random.best_params_)</span>
<span id="cb24-317"><a href="#cb24-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-318"><a href="#cb24-318" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best model</span></span>
<span id="cb24-319"><a href="#cb24-319" aria-hidden="true" tabindex="-1"></a>best_model_rf <span class="op">=</span> rf_random.best_estimator_</span>
<span id="cb24-320"><a href="#cb24-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-321"><a href="#cb24-321" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> rf.predict(X_test_imputed)</span>
<span id="cb24-322"><a href="#cb24-322" aria-hidden="true" tabindex="-1"></a>y_pred_best <span class="op">=</span> best_model_rf.predict(X_test_imputed)</span>
<span id="cb24-323"><a href="#cb24-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-324"><a href="#cb24-324" aria-hidden="true" tabindex="-1"></a>best_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])</span>
<span id="cb24-325"><a href="#cb24-325" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-328"><a href="#cb24-328" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-329"><a href="#cb24-329" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-330"><a href="#cb24-330" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb24-331"><a href="#cb24-331" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RF without hyperparameter tuning"</span>)</span>
<span id="cb24-332"><a href="#cb24-332" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy_score(y_test, y_pred)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-333"><a href="#cb24-333" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 score: </span><span class="sc">{</span>f1_score(y_test, y_pred)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-334"><a href="#cb24-334" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb24-335"><a href="#cb24-335" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RF with hyperparameter tuning"</span>)</span>
<span id="cb24-336"><a href="#cb24-336" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy_score(y_test, y_pred_best)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-337"><a href="#cb24-337" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 score: </span><span class="sc">{</span>f1_score(y_test, y_pred_best)<span class="sc">}</span><span class="ss">"</span>)  </span>
<span id="cb24-338"><a href="#cb24-338" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-341"><a href="#cb24-341" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-342"><a href="#cb24-342" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-343"><a href="#cb24-343" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_test, y_pred_best, labels<span class="op">=</span>best_model_rf.classes_)</span>
<span id="cb24-344"><a href="#cb24-344" aria-hidden="true" tabindex="-1"></a>disp <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm, display_labels<span class="op">=</span>[<span class="st">'Non-Diabetic'</span>, <span class="st">'Diabetic'</span>])</span>
<span id="cb24-345"><a href="#cb24-345" aria-hidden="true" tabindex="-1"></a>disp.plot()</span>
<span id="cb24-346"><a href="#cb24-346" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-347"><a href="#cb24-347" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-348"><a href="#cb24-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-349"><a href="#cb24-349" aria-hidden="true" tabindex="-1"></a>**XGBoost**</span>
<span id="cb24-350"><a href="#cb24-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-351"><a href="#cb24-351" aria-hidden="true" tabindex="-1"></a>XGBoost, or Extreme Gradient Boosting, is a machine learning algorithm renowned for its efficiency and performance in predictive modeling tasks. It belongs to the ensemble learning family and is an extension of traditional gradient boosting methods. The core idea behind XGBoost is the sequential addition of weak learners, often decision trees, which are trained to correct errors made by preceding models. XGBoost introduces several key innovations, including regularization techniques to prevent overfitting, parallelized tree construction for faster training, and gradient-based optimization for rapid convergence.</span>
<span id="cb24-352"><a href="#cb24-352" aria-hidden="true" tabindex="-1"></a>At its core, XGBoost structures its ensemble as a collection of decision trees, each contributing to the final prediction. So, gradient boosting is in the form of ensemble of weak prediction models. The algorithm assigns weights to the misclassified instances in each iteration, adjusting subsequent trees to focus on the previously misclassified samples. During training, XGBoost uses gradient-based optimization to efficiently navigate the solution space and arrive at an ensemble of trees that collectively delivers a robust and accurate prediction. </span>
<span id="cb24-353"><a href="#cb24-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-354"><a href="#cb24-354" aria-hidden="true" tabindex="-1"></a>In the code, we make use of the <span class="co">[</span><span class="ot">XGBoost</span><span class="co">](https://xgboost.readthedocs.io/en/stable/parameter.html)</span> library for the XGBoost implementation. The parameter information can be found in the implementation page.</span>
<span id="cb24-355"><a href="#cb24-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-356"><a href="#cb24-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-357"><a href="#cb24-357" aria-hidden="true" tabindex="-1"></a>*BayesSearch Hyperparameter tuning*</span>
<span id="cb24-358"><a href="#cb24-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-359"><a href="#cb24-359" aria-hidden="true" tabindex="-1"></a>Bayesian optimization is a more sophisticated technique that uses Bayesian methods to model the underlying function that maps hyperparameters to the model performance. It tries to find the optimal set of hyperparameters by making smart guesses based on the previous results. Bayesian optimization is more efficient than grid or random search because it attempts to balance exploration and exploitation of the search space. It can also deal with the cases of large number of hyperparameters and large search space. However, it can be more difficult to implement than grid search or random search and may require more computational resources.</span>
<span id="cb24-360"><a href="#cb24-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-361"><a href="#cb24-361" aria-hidden="true" tabindex="-1"></a>In the code, we make use of the <span class="co">[</span><span class="ot">skopt</span><span class="co">](https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html)</span> library for the BayesSearchCV implementation. The parameter information can be found in the implementation page.</span>
<span id="cb24-362"><a href="#cb24-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-365"><a href="#cb24-365" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-366"><a href="#cb24-366" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb24-367"><a href="#cb24-367" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBClassifier</span>
<span id="cb24-368"><a href="#cb24-368" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt <span class="im">import</span> BayesSearchCV</span>
<span id="cb24-369"><a href="#cb24-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-370"><a href="#cb24-370" aria-hidden="true" tabindex="-1"></a>model_names.append(<span class="st">'XGBoost'</span>)</span>
<span id="cb24-371"><a href="#cb24-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-372"><a href="#cb24-372" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an XGBoost classifier</span></span>
<span id="cb24-373"><a href="#cb24-373" aria-hidden="true" tabindex="-1"></a>xgb <span class="op">=</span> XGBClassifier()</span>
<span id="cb24-374"><a href="#cb24-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-375"><a href="#cb24-375" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Current params:"</span>)</span>
<span id="cb24-376"><a href="#cb24-376" aria-hidden="true" tabindex="-1"></a>pprint(xgb.get_params())</span>
<span id="cb24-377"><a href="#cb24-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-378"><a href="#cb24-378" aria-hidden="true" tabindex="-1"></a>xgb.fit(X_train_imputed, y_train)</span>
<span id="cb24-379"><a href="#cb24-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-380"><a href="#cb24-380" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameter search space</span></span>
<span id="cb24-381"><a href="#cb24-381" aria-hidden="true" tabindex="-1"></a>param_space <span class="op">=</span> {</span>
<span id="cb24-382"><a href="#cb24-382" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: (<span class="dv">3</span>, <span class="dv">10</span>),</span>
<span id="cb24-383"><a href="#cb24-383" aria-hidden="true" tabindex="-1"></a>    <span class="st">'learning_rate'</span>: (<span class="fl">0.01</span>, <span class="fl">1.0</span>, <span class="st">'log-uniform'</span>),</span>
<span id="cb24-384"><a href="#cb24-384" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: (<span class="dv">50</span>, <span class="dv">200</span>),</span>
<span id="cb24-385"><a href="#cb24-385" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_child_weight'</span>: (<span class="dv">1</span>, <span class="dv">10</span>),</span>
<span id="cb24-386"><a href="#cb24-386" aria-hidden="true" tabindex="-1"></a>    <span class="st">'subsample'</span>: (<span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="st">'uniform'</span>),</span>
<span id="cb24-387"><a href="#cb24-387" aria-hidden="true" tabindex="-1"></a>    <span class="st">'gamma'</span>: (<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="st">'uniform'</span>),</span>
<span id="cb24-388"><a href="#cb24-388" aria-hidden="true" tabindex="-1"></a>    <span class="st">'colsample_bytree'</span>: (<span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="st">'uniform'</span>),</span>
<span id="cb24-389"><a href="#cb24-389" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-390"><a href="#cb24-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-391"><a href="#cb24-391" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate BayesSearchCV</span></span>
<span id="cb24-392"><a href="#cb24-392" aria-hidden="true" tabindex="-1"></a>bayes_search_xgb <span class="op">=</span> BayesSearchCV(</span>
<span id="cb24-393"><a href="#cb24-393" aria-hidden="true" tabindex="-1"></a>    xgb,</span>
<span id="cb24-394"><a href="#cb24-394" aria-hidden="true" tabindex="-1"></a>    param_space,</span>
<span id="cb24-395"><a href="#cb24-395" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">3</span>,  <span class="co"># Number of cross-validation folds</span></span>
<span id="cb24-396"><a href="#cb24-396" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-397"><a href="#cb24-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-398"><a href="#cb24-398" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">int</span> <span class="op">=</span> np.int_</span>
<span id="cb24-399"><a href="#cb24-399" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model to the training data and perform hyperparameter tuning</span></span>
<span id="cb24-400"><a href="#cb24-400" aria-hidden="true" tabindex="-1"></a>bayes_search_xgb.fit(X_train_imputed, y_train)</span>
<span id="cb24-401"><a href="#cb24-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-402"><a href="#cb24-402" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the best hyperparameters</span></span>
<span id="cb24-403"><a href="#cb24-403" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Hyperparameters:"</span>)</span>
<span id="cb24-404"><a href="#cb24-404" aria-hidden="true" tabindex="-1"></a>pprint(bayes_search_xgb.best_params_)</span>
<span id="cb24-405"><a href="#cb24-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-406"><a href="#cb24-406" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best model</span></span>
<span id="cb24-407"><a href="#cb24-407" aria-hidden="true" tabindex="-1"></a>best_model_xgb <span class="op">=</span> bayes_search_xgb.best_estimator_</span>
<span id="cb24-408"><a href="#cb24-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-409"><a href="#cb24-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-410"><a href="#cb24-410" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> xgb.predict(X_test_imputed)</span>
<span id="cb24-411"><a href="#cb24-411" aria-hidden="true" tabindex="-1"></a>y_pred_best <span class="op">=</span> best_model_xgb.predict(X_test_imputed)</span>
<span id="cb24-412"><a href="#cb24-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-413"><a href="#cb24-413" aria-hidden="true" tabindex="-1"></a>best_preds.append([accuracy_score(y_test, y_pred_best), precision_score(y_test, y_pred_best), recall_score(y_test, y_pred_best), f1_score(y_test, y_pred_best)])</span>
<span id="cb24-414"><a href="#cb24-414" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-415"><a href="#cb24-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-418"><a href="#cb24-418" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-419"><a href="#cb24-419" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-420"><a href="#cb24-420" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb24-421"><a href="#cb24-421" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"XGB without hyperparameter tuning"</span>)</span>
<span id="cb24-422"><a href="#cb24-422" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy_score(y_test, y_pred)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-423"><a href="#cb24-423" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 score: </span><span class="sc">{</span>f1_score(y_test, y_pred)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-424"><a href="#cb24-424" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb24-425"><a href="#cb24-425" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"XGB with hyperparameter tuning"</span>)</span>
<span id="cb24-426"><a href="#cb24-426" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy_score(y_test, y_pred_best)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-427"><a href="#cb24-427" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 score: </span><span class="sc">{</span>f1_score(y_test, y_pred_best)<span class="sc">}</span><span class="ss">"</span>)  </span>
<span id="cb24-428"><a href="#cb24-428" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-429"><a href="#cb24-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-432"><a href="#cb24-432" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-433"><a href="#cb24-433" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-434"><a href="#cb24-434" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_test, y_pred_best, labels<span class="op">=</span>best_model_xgb.classes_)</span>
<span id="cb24-435"><a href="#cb24-435" aria-hidden="true" tabindex="-1"></a>disp <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm, display_labels<span class="op">=</span>[<span class="st">'Non-Diabetic'</span>, <span class="st">'Diabetic'</span>])</span>
<span id="cb24-436"><a href="#cb24-436" aria-hidden="true" tabindex="-1"></a>disp.plot()</span>
<span id="cb24-437"><a href="#cb24-437" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-438"><a href="#cb24-438" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-439"><a href="#cb24-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-440"><a href="#cb24-440" aria-hidden="true" tabindex="-1"></a>Analyzing the results of all the chosen models, we get the table below:</span>
<span id="cb24-441"><a href="#cb24-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-444"><a href="#cb24-444" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-445"><a href="#cb24-445" aria-hidden="true" tabindex="-1"></a><span class="co"># tabulate their classification report</span></span>
<span id="cb24-446"><a href="#cb24-446" aria-hidden="true" tabindex="-1"></a>evaluation_metrics <span class="op">=</span> [<span class="st">'Accuracy'</span>, <span class="st">'Precision'</span>, <span class="st">'Recall'</span>, <span class="st">'F1-score'</span>]</span>
<span id="cb24-447"><a href="#cb24-447" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> [<span class="dv">30</span>, <span class="dv">7</span>]</span>
<span id="cb24-448"><a href="#cb24-448" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.autolayout"</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb24-449"><a href="#cb24-449" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb24-450"><a href="#cb24-450" aria-hidden="true" tabindex="-1"></a>axs.axis(<span class="st">'tight'</span>)</span>
<span id="cb24-451"><a href="#cb24-451" aria-hidden="true" tabindex="-1"></a>axs.axis(<span class="st">'off'</span>)</span>
<span id="cb24-452"><a href="#cb24-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-453"><a href="#cb24-453" aria-hidden="true" tabindex="-1"></a>table1 <span class="op">=</span> axs.table(cellText<span class="op">=</span>best_preds,</span>
<span id="cb24-454"><a href="#cb24-454" aria-hidden="true" tabindex="-1"></a>                      cellLoc <span class="op">=</span> <span class="st">'left'</span>,</span>
<span id="cb24-455"><a href="#cb24-455" aria-hidden="true" tabindex="-1"></a>                      rowLabels <span class="op">=</span> model_names,</span>
<span id="cb24-456"><a href="#cb24-456" aria-hidden="true" tabindex="-1"></a>                      rowColours<span class="op">=</span> [<span class="st">"palegreen"</span>] <span class="op">*</span> <span class="dv">10</span>,</span>
<span id="cb24-457"><a href="#cb24-457" aria-hidden="true" tabindex="-1"></a>                      colLabels<span class="op">=</span>evaluation_metrics,</span>
<span id="cb24-458"><a href="#cb24-458" aria-hidden="true" tabindex="-1"></a>                      colColours<span class="op">=</span> [<span class="st">"palegreen"</span>] <span class="op">*</span> <span class="dv">10</span>,</span>
<span id="cb24-459"><a href="#cb24-459" aria-hidden="true" tabindex="-1"></a>                      loc<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb24-460"><a href="#cb24-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-461"><a href="#cb24-461" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight cells with minimum value in each column</span></span>
<span id="cb24-462"><a href="#cb24-462" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col_idx, metric <span class="kw">in</span> <span class="bu">enumerate</span>(evaluation_metrics):</span>
<span id="cb24-463"><a href="#cb24-463" aria-hidden="true" tabindex="-1"></a>    col_values <span class="op">=</span> [row[col_idx] <span class="cf">for</span> row <span class="kw">in</span> best_preds]</span>
<span id="cb24-464"><a href="#cb24-464" aria-hidden="true" tabindex="-1"></a>    max_value_idx <span class="op">=</span> col_values.index(<span class="bu">max</span>(col_values))</span>
<span id="cb24-465"><a href="#cb24-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-466"><a href="#cb24-466" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Highlight the cell with maximum value in coral color</span></span>
<span id="cb24-467"><a href="#cb24-467" aria-hidden="true" tabindex="-1"></a>    table1[max_value_idx <span class="op">+</span> <span class="dv">1</span>, col_idx].set_facecolor(<span class="st">"coral"</span>)</span>
<span id="cb24-468"><a href="#cb24-468" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-469"><a href="#cb24-469" aria-hidden="true" tabindex="-1"></a>table1.auto_set_font_size(<span class="va">False</span>)</span>
<span id="cb24-470"><a href="#cb24-470" aria-hidden="true" tabindex="-1"></a>table1.set_fontsize(<span class="dv">14</span>)</span>
<span id="cb24-471"><a href="#cb24-471" aria-hidden="true" tabindex="-1"></a>table1.scale(<span class="dv">1</span>, <span class="dv">4</span>)</span>
<span id="cb24-472"><a href="#cb24-472" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb24-473"><a href="#cb24-473" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-474"><a href="#cb24-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-475"><a href="#cb24-475" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-476"><a href="#cb24-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-477"><a href="#cb24-477" aria-hidden="true" tabindex="-1"></a>This blog only discusses a few classification algorithms and model tuning parameters. By applying the right model, tuning, and regularizing the model, we can aim to improve the accuracy of the model.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>